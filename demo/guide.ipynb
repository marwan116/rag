{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augment Generation\n",
    "\n",
    "### Usecase: Building docsrag: A query-engine to help developers quickly find information in open-source documentation\n",
    "- More specifically we will be building **raybot** with our docsrag library: A retrieval-augmented question answering system using ray's documentation\n",
    "\n",
    "### Techstack:\n",
    "- `llama_index`\n",
    "   - `llama_hub` for document loading\n",
    "   - `openai` and `huggingface` for LLM models\n",
    "   - `langchain` for language chaining\n",
    "   - `nltk` for text processing\n",
    "- `ray`\n",
    "\n",
    "### Building a retrieval-augmented question answering system using ray documentation\n",
    "Retrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data. It generally consists of two stages:\n",
    "1. indexing stage: preparing a knowledge base\n",
    "2. querying stage: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n",
    "\n",
    "[<img src=\"rag.jpeg\" height=\"500\"/>](rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Stage\n",
    "Given a dataset of documents, we first need to index them. This is done by:\n",
    "- Load the documents\n",
    "- Parse the documents into passages which are called nodes\n",
    "- Use an embedding model to encode the nodes into embedding vectors\n",
    "- Index the embeddings using a vector similarity search database\n",
    "<!-- ![index](index_build.jpeg) -->\n",
    "[<img src=\"index_build.jpeg\" height=\"500\"/>](index_build.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader\n",
    "\n",
    "We will go over how a sample markdown document is loaded into a document object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the llama-index markdown-reader does not support introducing document relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocumentLoader implementation in docsrag\n",
    "\n",
    "We showcase the docsrag `GithubDocumentLoader` which simply an adapter for `llama_hub.github_repo.GithubRepositoryReader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, the `GithubDocumentLoader`:\n",
    "- consider only markdown (`.md`) and restructured-text (`.rst`) files inside the ray repo doc/source folder.\n",
    "- read the documents as raw text given the default `llama_index` readers have their flaws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.docs_loader import GithubDocumentLoader\n",
    "\n",
    "document_loader = GithubDocumentLoader(\n",
    "    owner=\"ray-project\",\n",
    "    repo=\"ray\",\n",
    "    version_tag=\"releases/2.6.3\",\n",
    "    paths_to_include=[\"doc/source/\"],\n",
    "    file_extensions_to_include=[\".md\", \".rst\"],\n",
    "    paths_to_exclude=[\n",
    "        \"doc/source/_ext/\",\n",
    "        \"doc/source/_includes/\",\n",
    "        \"doc/source/_static/\",\n",
    "        \"doc/source/_templates/\",\n",
    "    ],\n",
    "    filenames_to_exclude=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run this command to fetch the documents\n",
    "# docsrag fetch-documents --config-path ./data/config.yaml --data-path ./data --overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"./data/docs/{hash(document_loader)}.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of documents: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_mkdown_doc = next(\n",
    "    doc for doc in docs if doc.metadata[\"file_path\"].endswith(\".md\")\n",
    ")\n",
    "\n",
    "print(sample_mkdown_doc.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Parser\n",
    "A node parser chunks a document into nodes\n",
    "\n",
    "The parser will:\n",
    "- run a text chunker\n",
    "- inject additional node metadata\n",
    "- construct node relationships\n",
    "\n",
    "A node is:\n",
    "- the chunk text plus metadata (e.g. node text hash, node relationships to other nodes)\n",
    "\n",
    "We showcase the docsrag `NodeParser` which simply an adapter for `llama_hub.github_repo.GithubRepositoryReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.node_parser import NodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = NodeParser.parse_obj(\n",
    "    {\n",
    "        \"inherit_metadata_from_doc\": True,\n",
    "        \"construct_prev_next_relations\": True,\n",
    "        \"text_chunker\": {\n",
    "            \"chunk_size\": 1024,\n",
    "            \"chunk_overlap\": 20,\n",
    "            \"paragraph_separator\": \"\\n\\n\\n\",\n",
    "            \"sentence_tokenizer\": {\"type\": \"tokenizers/punkt\"},\n",
    "            \"secondary_chunking_regex\": \"[^,.;。]+[,.;。]?\",\n",
    "            \"tokenizer\": {\"encoding\": \"gpt2\"},\n",
    "            \"word_seperator\": \" \",\n",
    "        },\n",
    "        \"metadata_pipeline\": {\n",
    "            \"extractors\": [\n",
    "                \"file_path_extractor\",\n",
    "                \"text_hash_extractor\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run this command to parse the nodes\n",
    "# docsrag parse-nodes --config-path ./data/config.yaml --data-path ./data --overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psource node_parser.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"./data/nodes/{hash(node_parser)}.pkl\", \"rb\") as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import gradio as gr\n",
    "from docsrag.node_parser import NodeParser\n",
    "import pickle\n",
    "\n",
    "with open(\"tutorial_docs.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "config = {\n",
    "    \"inherit_metadata_from_doc\": True,\n",
    "    \"construct_prev_next_relations\": True,\n",
    "    \"text_chunker\": {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"chunk_overlap\": 20,\n",
    "        \"paragraph_separator\": \"\\n\\n\\n\",\n",
    "        \"sentence_tokenizer\": {\"type\": \"tokenizers/punkt\"},\n",
    "        \"secondary_chunking_regex\": \"[^,.;。]+[,.;。]?\",\n",
    "        \"tokenizer\": {\"encoding\": \"gpt2\"},\n",
    "        \"word_seperator\": \" \",\n",
    "    },\n",
    "    \"metadata_pipeline\": {\n",
    "        \"extractors\": [\n",
    "            \"file_path_extractor\",\n",
    "            \"text_hash_extractor\",\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def parse_nodes(\n",
    "    text,\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    paragraph_separator=\"\\n\\n\\n\",\n",
    "    sentence_tokenizer=\"tokenizers/punkt\",\n",
    "    secondary_chunking_regex=\"[^,.;。]+[,.;。]?\",\n",
    "    tokenizer=\"gpt2\",\n",
    "    word_seperator=\" \",\n",
    "    extractors=[\"file_path_extractor\", \"text_hash_extractor\"],\n",
    "):\n",
    "    config_dict = config\n",
    "    config_dict[\"text_chunker\"][\"chunk_size\"] = chunk_size\n",
    "    config_dict[\"text_chunker\"][\"chunk_overlap\"] = chunk_overlap\n",
    "    config_dict[\"text_chunker\"][\"paragraph_separator\"] = paragraph_separator\n",
    "    config_dict[\"text_chunker\"][\"sentence_tokenizer\"][\"type\"] = sentence_tokenizer\n",
    "    config_dict[\"text_chunker\"][\"secondary_chunking_regex\"] = secondary_chunking_regex\n",
    "    config_dict[\"text_chunker\"][\"tokenizer\"][\"encoding\"] = tokenizer\n",
    "    config_dict[\"text_chunker\"][\"word_seperator\"] = word_seperator\n",
    "    config_dict[\"metadata_pipeline\"][\"extractors\"] = extractors\n",
    "\n",
    "    node_parser = NodeParser.parse_obj(config_dict)\n",
    "    doc = docs[0]\n",
    "    doc.text = text\n",
    "    nodes = node_parser.run([doc], use_ray=False)\n",
    "    return (\n",
    "        nodes[0].text,\n",
    "        yaml.dump(nodes[0].metadata),\n",
    "        yaml.dump([str(rel) for rel in nodes[0].relationships]),\n",
    "        nodes[1].text,\n",
    "        yaml.dump(nodes[1].metadata),\n",
    "        yaml.dump([str(rel) for rel in nodes[1].relationships]),\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Node Parser Demo\n",
    "                Shows how configuration options affect the output of the node parser.\n",
    "                \"\"\"\n",
    "            )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3, min_width=100):\n",
    "            text1 = gr.Textbox(label=\"Document\", value=docs[0].text)\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text2 = gr.Textbox(label=\"NodeParser chunksize\", value=1024)\n",
    "            text3 = gr.Textbox(label=\"NodeParser chunk_overlap\", value=20)\n",
    "            text4 = gr.Textbox(label=\"NodeParser paragraph_separator\", value='\"\\n\\n\\n\"')\n",
    "            text5 = gr.Textbox(\n",
    "                label=\"NodeParser sentence_tokenizer\", value=\"tokenizers/punkt\"\n",
    "            )\n",
    "            text6 = gr.Textbox(\n",
    "                label=\"NodeParser secondary_chunking_regex\", value='\"[^,.;。]+[,.;。]?\"'\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"First Node text\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"First Node metadata\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"First Node relationships\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3, min_width=100):\n",
    "            out4 = gr.Textbox(label=\"Second Node text\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Textbox(label=\"Second Node metadata\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out6 = gr.Textbox(label=\"Second Node relationships\")\n",
    "    inbtw.click(\n",
    "        parse_nodes,\n",
    "        inputs=[text1, text2, text3, text4, text5, text6],\n",
    "        outputs=[out1, out2, out3, out4, out5, out6],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model and vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showcase the docsrag VectorStoreIndexRay (a very simple in-memory vector store) and how to use it to find similar nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.embedding.index import VectorStoreSpec, VectorStoreIndexRay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building our VectorStoreIndexRay from the nodes we parsed earlier. This will compute the embeddings for each node and store them in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psource VectorStoreIndexRay.build_from_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%psource VectorStoreIndexRay._get_node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_limit = None\n",
    "\n",
    "embedding_vector_store_spec = VectorStoreSpec.parse_obj(\n",
    "    {\"embedding_model_name\": \"BAAI/bge-small-en\"}\n",
    ")\n",
    "\n",
    "vector_store_index = VectorStoreIndexRay.build_from_spec(\n",
    "    nodes=nodes[:node_limit] if node_limit else nodes,\n",
    "    spec=embedding_vector_store_spec,\n",
    "    num_gpus=0,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "store.save(Path(f\"./data/vector_index/{hash(vector_store_index)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build the full vector store uncomment and run below command\n",
    "# docsrag build-embedding-vector-store-index --config-path ./data/config.yaml --data-path ./data --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the vector store that was built in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_vector_store = 525061202 # hash(vector_store_index)\n",
    "loaded_index = VectorStoreIndexRay.load(Path(f\"data/vector_index/{hash_vector_store}/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_scores = loaded_index.retrieve_most_similiar_nodes(\n",
    "    query=\"How can I migrate from a single-application config to a multi-application config in Ray Serve?\",\n",
    "    similarity_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of nodes fetched: {len(nodes_with_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar_node = nodes_with_scores[0]\n",
    "print(most_similar_node.node.text[-1170:], end=\"\\n\\n\")\n",
    "print(f\"{most_similar_node.node.metadata=}\")\n",
    "print(f\"{most_similar_node.score=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Embedding Index using standard ranking and classification metrics\n",
    "\n",
    "- Step1: Build a question and answer evaluation dataset from the ray documentation corpus\n",
    "- Step2: Assess the quality of our embedding index based on the built dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Evaluation Dataset\n",
    "[<img src=\"eval_build.jpeg\" height=\"500\"/>](eval_build.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from docsrag.evaluation_dataset_generator import EvaluationDatasetBuilder\n",
    "\n",
    "eval_dataset_builder = EvaluationDatasetBuilder.parse_obj(\n",
    "    {\n",
    "        \"qa_generator_open_ai\": {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"system_prompt\": dedent(\n",
    "                \"\"\"\n",
    "            You are a helpful assistant that generates questions and answers from a provided context.\n",
    "            The context will be selected documents from the ray's project documentation.\n",
    "            The questions you generate should be obvious on their own and should mimic what a developer might ask trying to work with ray, especially if they can't directly find the answer in the documentation.\n",
    "            The answers should be factually correct, can be of a variable length and can contain code.\n",
    "            If the provided context does not contain enough information to create a question and answer, you should respond with 'I can't generate a question and answer from this context'. \n",
    "            The following is an example of how the output should look:\n",
    "            Q1: How can I view ray dashboard from outside the Kubernetes cluster?\n",
    "            A1: You can use port-forwarding. Run the command 'kubectl port-forward --address 0.0.0.0 ${RAYCLUSTER_HEAD_POD} 8265:8265'\n",
    "\n",
    "            Q2: {question}\n",
    "            A2: {answer}\n",
    "            \"\"\"\n",
    "            ).lstrip(),\n",
    "            \"user_prompt_template\": dedent(\n",
    "                \"\"\"\n",
    "        Provide questions and answers from the following context:\n",
    "\n",
    "        {context}\n",
    "        \"\"\"\n",
    "            ).lstrip(),\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 0.85,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        },\n",
    "        \"noise_injector_from_parquet\": {\"dataset_name\": \"trivia_questions.parquet\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is the prompt used by llama-index in its finetuning module\n",
    "# \"\"\"\\\n",
    "# Context information is below.\n",
    "\n",
    "# ---------------------\n",
    "# {context_str}\n",
    "# ---------------------\n",
    "\n",
    "# Given the context information and not prior knowledge.\n",
    "# generate only questions based on the below query.\n",
    "\n",
    "# You are a Teacher/ Professor. Your task is to setup \\\n",
    "# {num_questions_per_chunk} questions for an upcoming \\\n",
    "# quiz/examination. The questions should be diverse in nature \\\n",
    "# across the document. Restrict the questions to the \\\n",
    "# context information provided.\"\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_generator_openai = eval_dataset_builder.qa_generator_open_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = qa_generator_openai.run(context=most_similar_node.node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our Embedding Vector Index Store\n",
    "\n",
    "[<img src=\"run_eval.jpeg\" height=\"600\"/>](run_eval.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.embedding.evaluation import VectorStoreEvaluator, load_evaluation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.embedding.index import VectorStoreIndexRay\n",
    "from pathlib import Path\n",
    "hash_vector_store = 525061202 # hash(vector_store_index)\n",
    "loaded_index = VectorStoreIndexRay.load(Path(f\"data/vector_index/{hash_vector_store}/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = VectorStoreEvaluator(\n",
    "    vector_store_index=loaded_index,\n",
    "    top_ks=[1, 3, 5, 7, 10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = load_evaluation_dataset(\n",
    "    evaluation_dataset_dir=Path(\"data/eval_data/\"),\n",
    "    evaluation_dataset_name=\"1618109849114044135\",\n",
    "    limit=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = evaluator.run(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to use the embedding vector store to augment our LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.llm.model import LLM, LLMPlusRag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_without_rag = LLM(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    max_retries=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I set a metric and mode in ray Tune?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_without_rag = predictor_without_rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_without_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_with_rag = LLMPlusRag(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    max_retries=10,\n",
    "    vector_store_path=f\"./data/vector_index/{hash_vector_store}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_with_rag = predictor_with_rag.query(query=query, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_with_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning embedding configuration using ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "with open(\"data/nodes/130956594988870197.pkl\", \"rb\") as f:\n",
    "    nodes = pickle.load(f)\n",
    "\n",
    "from docsrag.embedding.evaluation import load_evaluation_dataset\n",
    "\n",
    "eval_df = load_evaluation_dataset(\n",
    "    evaluation_dataset_dir=Path(\"data/eval_data/\"),\n",
    "    evaluation_dataset_name=\"1618109849114044135\",\n",
    "    limit=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-31 10:55:23</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:16.33        </td></tr>\n",
       "<tr><td>Memory:      </td><td>22.8/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 1.0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc            </th><th>embedding_model_name  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>my_objective_60150_00000</td><td>RUNNING </td><td>127.0.0.1:16420</td><td>BAAI/bge-small-en     </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 10:55:11,827\tWARNING tune.py:184 -- Stop signal received (e.g. via SIGINT/Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 10:55:23,714\tERROR tune.py:941 -- Trials did not complete: [my_objective_60150_00000]\n",
      "2023-08-31 10:55:23,715\tINFO tune.py:945 -- Total run time: 16.39 seconds (16.33 seconds for the tuning loop).\n",
      "2023-08-31 10:55:23,716\tWARNING tune.py:954 -- Experiment has been interrupted, but the most recent state was saved.\n",
      "Continue running this experiment with: Tuner.restore(path=\"/Users/marwansarieddine/ray_results/my_objective_2023-08-31_10-55-07\", trainable=...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_model_name': 'BAAI/bge-small-en'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 2023-08-31 10:55:23,733\tERROR worker.py:844 -- Worker exits with an exit code 1.\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m Traceback (most recent call last):\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1197, in ray._raylet.task_execution_handler\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 1100, in ray._raylet.execute_task_with_cancellation_handler\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 823, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 870, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 877, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 881, in ray._raylet.execute_task\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"python/ray/_raylet.pyx\", line 821, in ray._raylet.execute_task.function_executor\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/docfetcher-py39/lib/python3.9/site-packages/ray/_private/function_manager.py\", line 670, in actor_method_executor\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     return method(__ray_actor, *args, **kwargs)\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/docfetcher-py39/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 381, in train\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     result = self.step()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/ray/util/tracing/tracing_helper.py\", line 460, in _resume_span\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     return method(self, *_args, **_kwargs)\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/docfetcher-py39/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 376, in step\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     result = self._results_queue.get(\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/lib/python3.9/queue.py\", line 180, in get\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     self.not_empty.wait(remaining)\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/lib/python3.9/threading.py\", line 316, in wait\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     gotit = waiter.acquire(True, timeout)\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m   File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/docfetcher-py39/lib/python3.9/site-packages/ray/_private/worker.py\", line 841, in sigterm_handler\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m     sys.exit(1)\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m SystemExit: 1\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m [2023-08-31 10:55:23,761 C 16420 130939] core_worker.cc:768:  Check failed: _s.ok() Bad status: IOError: Broken pipe\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m *** StackTrace Information ***\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 0   _raylet.so                          0x000000010ce8d4f1 _ZN3raylsERNSt3__113basic_ostreamIcNS0_11char_traitsIcEEEERKNS_10StackTraceE + 65 ray::operator<<()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 1   _raylet.so                          0x000000010cebea42 _ZN3ray13SpdLogMessage5FlushEv + 114 ray::SpdLogMessage::Flush()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 2   _raylet.so                          0x000000010cebe912 _ZN3ray13SpdLogMessageD2Ev + 18 ray::SpdLogMessage::~SpdLogMessage()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 3   _raylet.so                          0x000000010ce90f0f _ZN3ray6RayLogD2Ev + 47 ray::RayLog::~RayLog()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 4   _raylet.so                          0x000000010c5d085a _ZN3ray4core10CoreWorker4ExitENS_3rpc14WorkerExitTypeERKNSt3__112basic_stringIcNS4_11char_traitsIcEENS4_9allocatorIcEEEERKNS4_10shared_ptrINS_17LocalMemoryBufferEEE + 954 ray::core::CoreWorker::Exit()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 5   _raylet.so                          0x000000010c5cbfcd _ZN3ray4core10CoreWorker11ExecuteTaskERKNS_17TaskSpecificationERKNSt3__110shared_ptrINS5_13unordered_mapINS5_12basic_stringIcNS5_11char_traitsIcEENS5_9allocatorIcEEEENS5_6vectorINS5_4pairIxdEENSB_ISG_EEEENS5_4hashISD_EENS5_8equal_toISD_EENSB_INSF_IKSD_SI_EEEEEEEEPNSE_INSF_INS_8ObjectIDENS6_INS_9RayObjectEEEEENSB_ISX_EEEES10_PN6google8protobuf16RepeatedPtrFieldINS_3rpc20ObjectReferenceCountEEEPbPSD_ + 8173 ray::core::CoreWorker::ExecuteTask()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 6   _raylet.so                          0x000000010c6537bd _ZNSt3__110__function6__funcINS_6__bindIMN3ray4core10CoreWorkerEFNS3_6StatusERKNS3_17TaskSpecificationERKNS_10shared_ptrINS_13unordered_mapINS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEENS_6vectorINS_4pairIxdEENSF_ISK_EEEENS_4hashISH_EENS_8equal_toISH_EENSF_INSJ_IKSH_SM_EEEEEEEEPNSI_INSJ_INS3_8ObjectIDENSA_INS3_9RayObjectEEEEENSF_IS11_EEEES14_PN6google8protobuf16RepeatedPtrFieldINS3_3rpc20ObjectReferenceCountEEEPbPSH_EJPS5_RKNS_12placeholders4__phILi1EEERKNS1I_ILi2EEERKNS1I_ILi3EEERKNS1I_ILi4EEERKNS1I_ILi5EEERKNS1I_ILi6EEERKNS1I_ILi7EEEEEENSF_IS24_EEFS6_S9_SV_S14_S14_S1B_S1C_S1D_EEclES9_OSV_OS14_S29_OS1B_OS1C_OS1D_ + 77 std::__1::__function::__func<>::operator()()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 7   _raylet.so                          0x000000010c71be08 _ZNSt3__110__function6__funcIZN3ray4core28CoreWorkerDirectTaskReceiver10HandleTaskENS2_3rpc15PushTaskRequestEPNS5_13PushTaskReplyENS_8functionIFvNS2_6StatusENS9_IFvvEEESC_EEEE3$_0NS_9allocatorISF_EEFvSE_EEclEOSE_ + 536 std::__1::__function::__func<>::operator()()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 8   _raylet.so                          0x000000010c6fd34e _ZN3ray4core14InboundRequest6AcceptEv + 110 ray::core::InboundRequest::Accept()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 9   _raylet.so                          0x000000010c6f77dc _ZN3ray4core20ActorSchedulingQueue16ScheduleRequestsEv + 1564 ray::core::ActorSchedulingQueue::ScheduleRequests()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 10  _raylet.so                          0x000000010c6f6f6e _ZN3ray4core20ActorSchedulingQueue3AddExxNSt3__18functionIFvNS3_IFvNS_6StatusENS3_IFvvEEES6_EEEEEESA_S8_RKNS2_12basic_stringIcNS2_11char_traitsIcEENS2_9allocatorIcEEEERKNS2_10shared_ptrINS_27FunctionDescriptorInterfaceEEENS_6TaskIDERKNS2_6vectorINS_3rpc15ObjectReferenceENSE_ISR_EEEE + 2206 ray::core::ActorSchedulingQueue::Add()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 11  _raylet.so                          0x000000010c719691 _ZN3ray4core28CoreWorkerDirectTaskReceiver10HandleTaskENS_3rpc15PushTaskRequestEPNS2_13PushTaskReplyENSt3__18functionIFvNS_6StatusENS7_IFvvEEESA_EEE + 3457 ray::core::CoreWorkerDirectTaskReceiver::HandleTask()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 12  _raylet.so                          0x000000010c68bba1 _ZNSt3__110__function6__funcIZN3ray4core10CoreWorker14HandlePushTaskENS2_3rpc15PushTaskRequestEPNS5_13PushTaskReplyENS_8functionIFvNS2_6StatusENS9_IFvvEEESC_EEEE4$_39NS_9allocatorISF_EESB_EclEv + 161 std::__1::__function::__func<>::operator()()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 13  _raylet.so                          0x000000010c9990a6 _ZN12EventTracker15RecordExecutionERKNSt3__18functionIFvvEEENS0_10shared_ptrI11StatsHandleEE + 86 EventTracker::RecordExecution()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 14  _raylet.so                          0x000000010c9719b0 _ZNSt3__110__function6__funcIZN23instrumented_io_context4postENS_8functionIFvvEEENS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEEE3$_0NS9_ISC_EES4_EclEv + 48 std::__1::__function::__func<>::operator()()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 15  _raylet.so                          0x000000010c96fef7 _ZN5boost4asio6detail18completion_handlerINSt3__18functionIFvvEEENS0_10io_context19basic_executor_typeINS3_9allocatorIvEELm0EEEE11do_completeEPvPNS1_19scheduler_operationERKNS_6system10error_codeEm + 167 boost::asio::detail::completion_handler<>::do_complete()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 16  _raylet.so                          0x000000010cee1a55 _ZN5boost4asio6detail9scheduler10do_run_oneERNS1_27conditionally_enabled_mutex11scoped_lockERNS1_21scheduler_thread_infoERKNS_6system10error_codeE + 725 boost::asio::detail::scheduler::do_run_one()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 17  _raylet.so                          0x000000010ced5941 _ZN5boost4asio6detail9scheduler3runERNS_6system10error_codeE + 289 boost::asio::detail::scheduler::run()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 18  _raylet.so                          0x000000010ced57d4 _ZN5boost4asio10io_context3runEv + 36 boost::asio::io_context::run()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 19  _raylet.so                          0x000000010c5e8398 _ZN3ray4core10CoreWorker20RunTaskExecutionLoopEv + 24 ray::core::CoreWorker::RunTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 20  _raylet.so                          0x000000010c692606 _ZN3ray4core21CoreWorkerProcessImpl26RunWorkerTaskExecutionLoopEv + 310 ray::core::CoreWorkerProcessImpl::RunWorkerTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 21  _raylet.so                          0x000000010c692499 _ZN3ray4core17CoreWorkerProcess20RunTaskExecutionLoopEv + 25 ray::core::CoreWorkerProcess::RunTaskExecutionLoop()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 22  _raylet.so                          0x000000010c4ff7c3 _ZL50__pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loopP7_objectS0_ + 19 __pyx_pw_3ray_7_raylet_10CoreWorker_7run_task_loop()\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 23  python3.9                           0x000000010a107e08 method_vectorcall_NOARGS + 136 method_vectorcall_NOARGS\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 24  python3.9                           0x000000010a1ec2ad call_function + 413 call_function\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 25  python3.9                           0x000000010a1e9571 _PyEval_EvalFrameDefault + 28689 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 26  python3.9                           0x000000010a0ff473 function_code_fastcall + 163 function_code_fastcall\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 27  python3.9                           0x000000010a1ec2ad call_function + 413 call_function\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 28  python3.9                           0x000000010a1e9571 _PyEval_EvalFrameDefault + 28689 _PyEval_EvalFrameDefault\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 29  python3.9                           0x000000010a1ed1e6 _PyEval_EvalCode + 2950 _PyEval_EvalCode\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 30  python3.9                           0x000000010a1e2461 PyEval_EvalCode + 81 PyEval_EvalCode\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 31  python3.9                           0x000000010a232517 pyrun_file + 327 pyrun_file\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 32  python3.9                           0x000000010a230595 PyRun_SimpleFileExFlags + 725 PyRun_SimpleFileExFlags\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 33  python3.9                           0x000000010a250093 Py_RunMain + 2083 Py_RunMain\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 34  python3.9                           0x000000010a25054f pymain_main + 335 pymain_main\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 35  python3.9                           0x000000010a2505ab Py_BytesMain + 43 Py_BytesMain\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m 36  dyld                                0x00007ff807a2341f start + 1903 start\n",
      "\u001b[2m\u001b[36m(my_objective pid=16420)\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "from docsrag.embedding.index import VectorStoreIndexRay, VectorStoreSpec\n",
    "from docsrag.embedding.evaluation import VectorStoreEvaluator\n",
    "from ray import tune\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def my_objective(config):  # ①\n",
    "\n",
    "    node_limit = None\n",
    "\n",
    "    embedding_vector_store_spec = VectorStoreSpec.parse_obj(\n",
    "        {\"embedding_model_name\": config[\"embedding_model_name\"]}\n",
    "    )\n",
    "\n",
    "    vector_store_index = VectorStoreIndexRay.build_from_spec(\n",
    "        nodes=nodes[:node_limit] if node_limit else nodes,\n",
    "        spec=embedding_vector_store_spec,\n",
    "        num_gpus=0,\n",
    "        batch_size=100,\n",
    "    )\n",
    "\n",
    "    evaluator = VectorStoreEvaluator(\n",
    "        vector_store_index=vector_store_index,\n",
    "        top_ks=[3],\n",
    "    )\n",
    "\n",
    "    scores = evaluator.run(eval_df.iloc[:4])\n",
    "\n",
    "    return {\"score\": scores[\"recall@k\"].mean()}\n",
    "\n",
    "\n",
    "search_space = {  # ②\n",
    "    \"embedding_model_name\": tune.choice(\n",
    "        [\n",
    "            \"BAAI/bge-small-en\",\n",
    "            # \"BAAI/bge-base-en\",\n",
    "        ],\n",
    "    ),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(my_objective, param_space=search_space)  # ③\n",
    "\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"max\").config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_objective(config):  # ①\n",
    "\n",
    "    node_limit = 100\n",
    "\n",
    "    embedding_vector_store_spec = VectorStoreSpec.parse_obj(\n",
    "        {\"embedding_model_name\": config[\"embedding_model_name\"]}\n",
    "    )\n",
    "\n",
    "    vector_store_index = VectorStoreIndexRay.build_from_spec(\n",
    "        nodes=nodes[:node_limit] if node_limit else nodes,\n",
    "        spec=embedding_vector_store_spec,\n",
    "        num_gpus=0,\n",
    "        batch_size=100,\n",
    "    )\n",
    "\n",
    "    evaluator = VectorStoreEvaluator(\n",
    "        vector_store_index=vector_store_index,\n",
    "        top_ks=[3],\n",
    "    )\n",
    "\n",
    "    scores = evaluator.run(eval_df.iloc[:4])\n",
    "\n",
    "    return {\"score\": scores[\"recall@k\"].mean()}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorStoreSpec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_objective(\n\u001b[1;32m      2\u001b[0m     {\n\u001b[1;32m      3\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39membedding_model_name\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39m\"\u001b[39;49m\u001b[39mBAAI/bge-small-en\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m     }\n\u001b[1;32m      5\u001b[0m )\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mmy_objective\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmy_objective\u001b[39m(config):  \u001b[39m# ①\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     node_limit \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> 5\u001b[0m     embedding_vector_store_spec \u001b[39m=\u001b[39m VectorStoreSpec\u001b[39m.\u001b[39mparse_obj(\n\u001b[1;32m      6\u001b[0m         {\u001b[39m\"\u001b[39m\u001b[39membedding_model_name\u001b[39m\u001b[39m\"\u001b[39m: config[\u001b[39m\"\u001b[39m\u001b[39membedding_model_name\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m      7\u001b[0m     )\n\u001b[1;32m      9\u001b[0m     vector_store_index \u001b[39m=\u001b[39m VectorStoreIndexRay\u001b[39m.\u001b[39mbuild_from_spec(\n\u001b[1;32m     10\u001b[0m         nodes\u001b[39m=\u001b[39mnodes[:node_limit] \u001b[39mif\u001b[39;00m node_limit \u001b[39melse\u001b[39;00m nodes,\n\u001b[1;32m     11\u001b[0m         spec\u001b[39m=\u001b[39membedding_vector_store_spec,\n\u001b[1;32m     12\u001b[0m         num_gpus\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     13\u001b[0m         batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     14\u001b[0m     )\n\u001b[1;32m     16\u001b[0m     evaluator \u001b[39m=\u001b[39m VectorStoreEvaluator(\n\u001b[1;32m     17\u001b[0m         vector_store_index\u001b[39m=\u001b[39mvector_store_index,\n\u001b[1;32m     18\u001b[0m         top_ks\u001b[39m=\u001b[39m[\u001b[39m3\u001b[39m],\n\u001b[1;32m     19\u001b[0m     )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorStoreSpec' is not defined"
     ]
    }
   ],
   "source": [
    "my_objective(\n",
    "    {\n",
    "        \"embedding_model_name\": \"BAAI/bge-small-en\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raybot-evaluator-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
