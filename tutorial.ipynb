{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augment Generation\n",
    "\n",
    "### Usecase: Building docsrag: A query-engine to help developers quickly find information in open-source documentation\n",
    "- More specifically we will be building **raybot** with our docsrag library: A retrieval-augmented question answering system using ray's documentation\n",
    "\n",
    "### Techstack:\n",
    "- `llama_index`\n",
    "   - `llama_hub` for document loading\n",
    "   - `openai` and `huggingface` for LLM models\n",
    "   - `langchain` for language chaining\n",
    "   - `nltk` for text processing\n",
    "- `ray`\n",
    "\n",
    "### Building a retrieval-augmented question answering system using ray documentation\n",
    "Retrieval augmented generation (RAG) is a paradigm for augmenting LLM with custom data. It generally consists of two stages:\n",
    "1. indexing stage: preparing a knowledge base\n",
    "2. querying stage: retrieving relevant context from the knowledge to assist the LLM in responding to a question\n",
    "\n",
    "[<img src=\"rag.jpeg\" height=\"500\"/>](rag.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing Stage\n",
    "Given a dataset of documents, we first need to index them. This is done by:\n",
    "- Load the documents\n",
    "- Parse the documents into passages which are called nodes\n",
    "- Use an embedding model to encode the nodes into embedding vectors\n",
    "- Index the embeddings using a vector similarity search database\n",
    "<!-- ![index](index_build.jpeg) -->\n",
    "[<img src=\"index_build.jpeg\" height=\"500\"/>](index_build.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Loader\n",
    "\n",
    "We will go over how a sample markdown document is loaded into a document object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also the llama-index markdown-reader does not support introducing document relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DocumentLoader implementation in docsrag\n",
    "\n",
    "We showcase the docsrag `GithubDocumentLoader` which simply an adapter for `llama_hub.github_repo.GithubRepositoryReader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, the `GithubDocumentLoader`:\n",
    "- consider only markdown (`.md`) and restructured-text (`.rst`) files inside the ray repo doc/source folder.\n",
    "- read the documents as raw text given the default `llama_index` readers have their flaws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.docs_loader import GithubDocumentLoader\n",
    "\n",
    "document_loader = GithubDocumentLoader(\n",
    "    owner=\"ray-project\",\n",
    "    repo=\"ray\",\n",
    "    version_tag=\"releases/2.6.3\",\n",
    "    paths_to_include=[\"doc/source/\"],\n",
    "    file_extensions_to_include=[\".md\", \".rst\"],\n",
    "    paths_to_exclude=[\n",
    "        \"doc/source/_ext/\",\n",
    "        \"doc/source/_includes/\",\n",
    "        \"doc/source/_static/\",\n",
    "        \"doc/source/_templates/\",\n",
    "    ],\n",
    "    filenames_to_exclude=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run this command to fetch the documents\n",
    "# docsrag fetch-documents --config-path ./data/config.yaml --data-path ./data --overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"./data/docs/{hash(document_loader)}.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 426\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of documents: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(observability-configure-manage-dashboard)=\n",
      "# Configuring and Managing Ray Dashboard\n",
      "{ref}`Ray Dashboard<observability-getting-started>` is one of the most important tools to monitor and debug Ray applications and Clusters. This page describes how to configure Ray Dashboard on your Clusters.\n",
      "\n",
      "Dashboard configurations may differ depending on how you launch Ray Clusters (e.g., local Ray Cluster v.s. KubeRay). Integrations with Prometheus and Grafana are optional for enhanced Dashboard experience.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_mkdown_doc = next(\n",
    "    doc for doc in docs if doc.metadata[\"file_path\"].endswith(\".md\")\n",
    ")\n",
    "\n",
    "print(sample_mkdown_doc.text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node Parser\n",
    "A node parser chunks a document into nodes\n",
    "\n",
    "The parser will:\n",
    "- run a text chunker\n",
    "- inject additional node metadata\n",
    "- construct node relationships\n",
    "\n",
    "A node is:\n",
    "- the chunk text plus metadata (e.g. node text hash, node relationships to other nodes)\n",
    "\n",
    "We showcase the docsrag `NodeParser` which simply an adapter for `llama_hub.github_repo.GithubRepositoryReader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.node_parser import NodeParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_parser = NodeParser.parse_obj(\n",
    "    {\n",
    "        \"inherit_metadata_from_doc\": True,\n",
    "        \"construct_prev_next_relations\": True,\n",
    "        \"text_chunker\": {\n",
    "            \"chunk_size\": 1024,\n",
    "            \"chunk_overlap\": 20,\n",
    "            \"paragraph_separator\": \"\\n\\n\\n\",\n",
    "            \"sentence_tokenizer\": {\"type\": \"tokenizers/punkt\"},\n",
    "            \"secondary_chunking_regex\": \"[^,.;。]+[,.;。]?\",\n",
    "            \"tokenizer\": {\"encoding\": \"gpt2\"},\n",
    "            \"word_seperator\": \" \",\n",
    "        },\n",
    "        \"metadata_pipeline\": {\n",
    "            \"extractors\": [\n",
    "                \"file_path_extractor\",\n",
    "                \"text_hash_extractor\",\n",
    "            ]\n",
    "        },\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment and run this command to parse the nodes\n",
    "# docsrag parse-nodes --config-path ./data/config.yaml --data-path ./data --overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Document\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_ray\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"BaseNode\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Parse the documents into nodes.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0muse_ray\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mimport\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_reinit_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mnode\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_document\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0mmap_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0;34m.\u001b[0m\u001b[0miter_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnode\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource node_parser.run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(f\"./data/nodes/{hash(node_parser)}.pkl\", \"rb\") as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 1212\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes: {len(nodes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "import gradio as gr\n",
    "from docsrag.node_parser import NodeParser\n",
    "import pickle\n",
    "\n",
    "with open(\"tutorial_docs.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)\n",
    "\n",
    "config = {\n",
    "    \"inherit_metadata_from_doc\": True,\n",
    "    \"construct_prev_next_relations\": True,\n",
    "    \"text_chunker\": {\n",
    "        \"chunk_size\": 1024,\n",
    "        \"chunk_overlap\": 20,\n",
    "        \"paragraph_separator\": \"\\n\\n\\n\",\n",
    "        \"sentence_tokenizer\": {\"type\": \"tokenizers/punkt\"},\n",
    "        \"secondary_chunking_regex\": \"[^,.;。]+[,.;。]?\",\n",
    "        \"tokenizer\": {\"encoding\": \"gpt2\"},\n",
    "        \"word_seperator\": \" \",\n",
    "    },\n",
    "    \"metadata_pipeline\": {\n",
    "        \"extractors\": [\n",
    "            \"file_path_extractor\",\n",
    "            \"text_hash_extractor\",\n",
    "        ]\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def parse_nodes(\n",
    "    text,\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=20,\n",
    "    paragraph_separator=\"\\n\\n\\n\",\n",
    "    sentence_tokenizer=\"tokenizers/punkt\",\n",
    "    secondary_chunking_regex=\"[^,.;。]+[,.;。]?\",\n",
    "    tokenizer=\"gpt2\",\n",
    "    word_seperator=\" \",\n",
    "    extractors=[\"file_path_extractor\", \"text_hash_extractor\"],\n",
    "):\n",
    "    config_dict = config\n",
    "    config_dict[\"text_chunker\"][\"chunk_size\"] = chunk_size\n",
    "    config_dict[\"text_chunker\"][\"chunk_overlap\"] = chunk_overlap\n",
    "    config_dict[\"text_chunker\"][\"paragraph_separator\"] = paragraph_separator\n",
    "    config_dict[\"text_chunker\"][\"sentence_tokenizer\"][\"type\"] = sentence_tokenizer\n",
    "    config_dict[\"text_chunker\"][\"secondary_chunking_regex\"] = secondary_chunking_regex\n",
    "    config_dict[\"text_chunker\"][\"tokenizer\"][\"encoding\"] = tokenizer\n",
    "    config_dict[\"text_chunker\"][\"word_seperator\"] = word_seperator\n",
    "    config_dict[\"metadata_pipeline\"][\"extractors\"] = extractors\n",
    "\n",
    "    node_parser = NodeParser.parse_obj(config_dict)\n",
    "    doc = docs[0]\n",
    "    doc.text = text\n",
    "    nodes = node_parser.run([doc], use_ray=False)\n",
    "    return (\n",
    "        nodes[0].text,\n",
    "        yaml.dump(nodes[0].metadata),\n",
    "        yaml.dump([str(rel) for rel in nodes[0].relationships]),\n",
    "        nodes[1].text,\n",
    "        yaml.dump(nodes[1].metadata),\n",
    "        yaml.dump([str(rel) for rel in nodes[1].relationships]),\n",
    "    )\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            title = gr.Markdown(\n",
    "                \"\"\"\n",
    "                # Node Parser Demo\n",
    "                Shows how configuration options affect the output of the node parser.\n",
    "                \"\"\"\n",
    "            )\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3, min_width=100):\n",
    "            text1 = gr.Textbox(label=\"Document\", value=docs[0].text)\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            text2 = gr.Textbox(label=\"NodeParser chunksize\", value=1024)\n",
    "            text3 = gr.Textbox(label=\"NodeParser chunk_overlap\", value=20)\n",
    "            text4 = gr.Textbox(label=\"NodeParser paragraph_separator\", value='\"\\n\\n\\n\"')\n",
    "            text5 = gr.Textbox(\n",
    "                label=\"NodeParser sentence_tokenizer\", value=\"tokenizers/punkt\"\n",
    "            )\n",
    "            text6 = gr.Textbox(\n",
    "                label=\"NodeParser secondary_chunking_regex\", value='\"[^,.;。]+[,.;。]?\"'\n",
    "            )\n",
    "\n",
    "    with gr.Row():\n",
    "        inbtw = gr.Button(\"Submit\", variant=\"primary\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3, min_width=100):\n",
    "            out1 = gr.Textbox(label=\"First Node text\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out2 = gr.Textbox(label=\"First Node metadata\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out3 = gr.Textbox(label=\"First Node relationships\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3, min_width=100):\n",
    "            out4 = gr.Textbox(label=\"Second Node text\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out5 = gr.Textbox(label=\"Second Node metadata\")\n",
    "        with gr.Column(scale=1, min_width=100):\n",
    "            out6 = gr.Textbox(label=\"Second Node relationships\")\n",
    "    inbtw.click(\n",
    "        parse_nodes,\n",
    "        inputs=[text1, text2, text3, text4, text5, text6],\n",
    "        outputs=[out1, out2, out3, out4, out5, out6],\n",
    "    )\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding model and vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We showcase the docsrag VectorStoreIndexRay (a very simple in-memory vector store) and how to use it to find similar nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.embedding.index import VectorStoreSpec, VectorStoreIndexRay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by building our VectorStoreIndexRay from the nodes we parsed earlier. This will compute the embeddings for each node and store them in a vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mbuild_from_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mVectorStoreSpec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mray_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdocstore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleDocumentStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleVectorStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mindex_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIndexDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0membed_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_embed_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mdocstore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_store\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_struct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_from_nodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdocstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mvector_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0membed_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mindex_struct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_struct\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m**\u001b[0m\u001b[0mray_kwargs\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mindex_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleIndexStore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mindex_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_index_struct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mdocstore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocstore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mvector_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvector_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mindex_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindex_store\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0membed_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0membedding_model_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource VectorStoreIndexRay.build_from_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_get_node_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mBaseNode\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mray_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m\"\"\"Get node embeddings.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_reinit_error\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mnode_with_embedding\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mnode_with_embedding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_items\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0mmap_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mray_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;34m.\u001b[0m\u001b[0miter_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%psource VectorStoreIndexRay._get_node_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorStoreSpec' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m node_limit \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m embedding_vector_store_spec \u001b[39m=\u001b[39m VectorStoreSpec\u001b[39m.\u001b[39mparse_obj(\n\u001b[1;32m      4\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39membedding_model_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBAAI/bge-small-en\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m vector_store_index \u001b[39m=\u001b[39m VectorStoreIndexRay\u001b[39m.\u001b[39mbuild_from_spec(\n\u001b[1;32m      8\u001b[0m     nodes\u001b[39m=\u001b[39mnodes[:node_limit] \u001b[39mif\u001b[39;00m node_limit \u001b[39melse\u001b[39;00m nodes,\n\u001b[1;32m      9\u001b[0m     spec\u001b[39m=\u001b[39membedding_vector_store_spec,\n\u001b[1;32m     10\u001b[0m     num_gpus\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m     11\u001b[0m     batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m,\n\u001b[1;32m     12\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorStoreSpec' is not defined"
     ]
    }
   ],
   "source": [
    "node_limit = None\n",
    "\n",
    "embedding_vector_store_spec = VectorStoreSpec.parse_obj(\n",
    "    {\"embedding_model_name\": \"BAAI/bge-small-en\"}\n",
    ")\n",
    "\n",
    "vector_store_index = VectorStoreIndexRay.build_from_spec(\n",
    "    nodes=nodes[:node_limit] if node_limit else nodes,\n",
    "    spec=embedding_vector_store_spec,\n",
    "    num_gpus=0,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "store.save(Path(f\"./data/vector_index/{hash(vector_store_index)}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to build the full vector store uncomment and run below command\n",
    "# docsrag build-embedding-vector-store-index --config-path ./data/config.yaml --data-path ./data --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the vector store that was built in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VectorStoreIndexRay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m hash_vector_store \u001b[39m=\u001b[39m \u001b[39m525061202\u001b[39m \u001b[39m# hash(vector_store_index)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m loaded_index \u001b[39m=\u001b[39m VectorStoreIndexRay\u001b[39m.\u001b[39mload(Path(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mdata/vector_index/\u001b[39m\u001b[39m{\u001b[39;00mhash_vector_store\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VectorStoreIndexRay' is not defined"
     ]
    }
   ],
   "source": [
    "hash_vector_store = 525061202 # hash(vector_store_index)\n",
    "loaded_index = VectorStoreIndexRay.load(Path(f\"data/vector_index/{hash_vector_store}/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_scores = loaded_index.retrieve_most_similiar_nodes(\n",
    "    query=\"How can I migrate from a single-application config to a multi-application config in Ray Serve?\",\n",
    "    similarity_top_k=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes fetched: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of nodes fetched: {len(nodes_with_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Migrating the single-application config `ServeApplicationSchema` to the multi-application config format `ServeDeploySchema` is straightforward. Each entry under the  `applications` field matches the old, single-application config format. To convert a single-application config to the multi-application config format:\n",
      "* Copy the entire old config to an entry under the `applications` field.\n",
      "* Remove `host` and `port` from the entry and move them under the `http_options` field.\n",
      "* Name the application.\n",
      "* If you haven't already, set the application-level `route_prefix` to the route prefix of the ingress deployment in the application. In a multi-application config, you should set route prefixes at the application level instead of for the ingress deployment in each application.\n",
      "* When needed, add more applications.\n",
      "\n",
      "For more details on the multi-application config format, see the documentation for [`ServeDeploySchema`](serve-rest-api-config-schema).\n",
      "\n",
      ":::{note} \n",
      "You must remove `host` and `port` from the application entry. In a multi-application config, specifying cluster-level options within an individual application isn't applicable, and is not supported.\n",
      ":::\n",
      "\n",
      "most_similar_node.node.metadata={'file_path': 'doc/source/serve/deploy-many-models/multi-app.md', 'file_name': 'multi-app.md', 'text_hash': 'bed9eabbfb332ac5af3cb6deb03bc347'}\n",
      "most_similar_node.score=0.9412376880645752\n"
     ]
    }
   ],
   "source": [
    "most_similar_node = nodes_with_scores[0]\n",
    "print(most_similar_node.node.text[-1170:], end=\"\\n\\n\")\n",
    "print(f\"{most_similar_node.node.metadata=}\")\n",
    "print(f\"{most_similar_node.score=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our Embedding Index using standard ranking and classification metrics\n",
    "\n",
    "- Step1: Build a question and answer evaluation dataset from the ray documentation corpus\n",
    "- Step2: Assess the quality of our embedding index based on the built dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Evaluation Dataset\n",
    "[<img src=\"eval_build.jpeg\" height=\"500\"/>](eval_build.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textwrap import dedent\n",
    "from docsrag.evaluation_dataset_generator import EvaluationDatasetBuilder\n",
    "\n",
    "eval_dataset_builder = EvaluationDatasetBuilder.parse_obj(\n",
    "    {\n",
    "        \"qa_generator_open_ai\": {\n",
    "            \"model\": \"gpt-3.5-turbo\",\n",
    "            \"system_prompt\": dedent(\n",
    "                \"\"\"\n",
    "            You are a helpful assistant that generates questions and answers from a provided context.\n",
    "            The context will be selected documents from the ray's project documentation.\n",
    "            The questions you generate should be obvious on their own and should mimic what a developer might ask trying to work with ray, especially if they can't directly find the answer in the documentation.\n",
    "            The answers should be factually correct, can be of a variable length and can contain code.\n",
    "            If the provided context does not contain enough information to create a question and answer, you should respond with 'I can't generate a question and answer from this context'. \n",
    "            The following is an example of how the output should look:\n",
    "            Q1: How can I view ray dashboard from outside the Kubernetes cluster?\n",
    "            A1: You can use port-forwarding. Run the command 'kubectl port-forward --address 0.0.0.0 ${RAYCLUSTER_HEAD_POD} 8265:8265'\n",
    "\n",
    "            Q2: {question}\n",
    "            A2: {answer}\n",
    "            \"\"\"\n",
    "            ).lstrip(),\n",
    "            \"user_prompt_template\": dedent(\n",
    "                \"\"\"\n",
    "        Provide questions and answers from the following context:\n",
    "\n",
    "        {context}\n",
    "        \"\"\"\n",
    "            ).lstrip(),\n",
    "            \"max_tokens\": 1024,\n",
    "            \"temperature\": 1.0,\n",
    "            \"top_p\": 0.85,\n",
    "            \"frequency_penalty\": 0,\n",
    "            \"presence_penalty\": 0,\n",
    "        },\n",
    "        \"noise_injector_from_parquet\": {\"dataset_name\": \"trivia_questions.parquet\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is the prompt used by llama-index in its finetuning module\n",
    "# \"\"\"\\\n",
    "# Context information is below.\n",
    "\n",
    "# ---------------------\n",
    "# {context_str}\n",
    "# ---------------------\n",
    "\n",
    "# Given the context information and not prior knowledge.\n",
    "# generate only questions based on the below query.\n",
    "\n",
    "# You are a Teacher/ Professor. Your task is to setup \\\n",
    "# {num_questions_per_chunk} questions for an upcoming \\\n",
    "# quiz/examination. The questions should be diverse in nature \\\n",
    "# across the document. Restrict the questions to the \\\n",
    "# context information provided.\"\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_generator_openai = eval_dataset_builder.qa_generator_open_ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = qa_generator_openai.run(context=most_similar_node.node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: How do you add a new application to Ray Serve?\n",
      "A1: To add a new application, you need to add a new entry under the `applications` field in the config. Each application must have a unique name and route prefix.\n",
      "\n",
      "Q2: How do you delete an application from Ray Serve?\n",
      "A2: To delete an application, you need to remove the corresponding entry under the `applications` field in the config.\n",
      "\n",
      "Q3: How do you update an application in Ray Serve?\n",
      "A3: To update an application, you need to modify the config options in the corresponding entry under the `applications` field in the config.\n",
      "\n",
      "Q4: How do you migrate from a single-application config to a multi-application config in Ray Serve?\n",
      "A4: To migrate from a single-application config to a multi-application config, you need to:\n",
      "- Copy the entire old config to an entry under the `applications` field.\n",
      "- Remove `host` and `port` from the entry and move them under the `http_options` field.\n",
      "- Name the application.\n",
      "- Set the application-level `route_prefix` to the route prefix of the ingress deployment in the application. In a multi-application config, you should set route prefixes at the application level instead of for the ingress deployment in each application.\n",
      "- Add more applications as needed.\n"
     ]
    }
   ],
   "source": [
    "print(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate our Embedding Vector Index Store\n",
    "\n",
    "[<img src=\"run_eval.jpeg\" height=\"600\"/>](run_eval.jpeg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docsrag.embedding.evaluation import VectorStoreEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = VectorStoreEvaluator(\n",
    "    vector_store_index=loaded_index,\n",
    "    evaluation_dataset_name=hash(eval_dataset_builder),\n",
    "    top_ks=[1, 3, 5, 7, 10]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 08:47:14,535\tINFO evaluation.py:227 -- Recall@1: 42.57%\n",
      "2023-08-31 08:47:14,536\tINFO evaluation.py:227 -- Recall@3: 60.10%\n",
      "2023-08-31 08:47:14,536\tINFO evaluation.py:227 -- Recall@5: 67.26%\n",
      "2023-08-31 08:47:14,537\tINFO evaluation.py:227 -- Recall@7: 71.20%\n",
      "2023-08-31 08:47:14,538\tINFO evaluation.py:227 -- Recall@10: 75.00%\n",
      "2023-08-31 08:47:14,539\tINFO evaluation.py:230 -- MRR@1: 0.4257\n",
      "2023-08-31 08:47:14,539\tINFO evaluation.py:230 -- MRR@3: 0.5036\n",
      "2023-08-31 08:47:14,540\tINFO evaluation.py:230 -- MRR@5: 0.5200\n",
      "2023-08-31 08:47:14,541\tINFO evaluation.py:230 -- MRR@7: 0.5262\n",
      "2023-08-31 08:47:14,542\tINFO evaluation.py:230 -- MRR@10: 0.5305\n",
      "2023-08-31 08:47:14,542\tINFO evaluation.py:233 -- NDCG@1: 0.4257\n",
      "2023-08-31 08:47:14,543\tINFO evaluation.py:233 -- NDCG@3: 0.5286\n",
      "2023-08-31 08:47:14,544\tINFO evaluation.py:233 -- NDCG@5: 0.5582\n",
      "2023-08-31 08:47:14,545\tINFO evaluation.py:233 -- NDCG@7: 0.5718\n",
      "2023-08-31 08:47:14,545\tINFO evaluation.py:233 -- NDCG@10: 0.5834\n"
     ]
    }
   ],
   "source": [
    "scores = evaluator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m scores[\u001b[39m\"\u001b[39m\u001b[39mrecall@k\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "scores[\"recall@k\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we are going to use the embedding vector store to augment our LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docsrag.llm.model import LLM, LLMPlusRag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_without_rag = LLM(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    max_retries=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"How can I set a metric and mode in ray Tune?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_without_rag = predictor_without_rag.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set a metric and mode in Ray Tune, you can use the `metric` and `mode` parameters when defining your `tune.run()` function. Here's an example:\n",
      "\n",
      "```python\n",
      "import ray\n",
      "from ray import tune\n",
      "\n",
      "# Define your training function\n",
      "def train_fn(config):\n",
      "    # Your training logic here\n",
      "    ...\n",
      "\n",
      "# Set up the configuration space\n",
      "config = {\n",
      "    \"learning_rate\": tune.loguniform(0.001, 0.1),\n",
      "    \"batch_size\": tune.choice([16, 32, 64]),\n",
      "    ...\n",
      "}\n",
      "\n",
      "# Set the metric and mode\n",
      "metric = \"mean_accuracy\"\n",
      "mode = \"max\"\n",
      "\n",
      "# Run the hyperparameter search\n",
      "analysis = tune.run(\n",
      "    train_fn,\n",
      "    config=config,\n",
      "    metric=metric,\n",
      "    mode=mode,\n",
      "    ...\n",
      ")\n",
      "```\n",
      "\n",
      "In this example, the `metric` is set to `\"mean_accuracy\"` and the `mode` is set to `\"max\"`. This means that Ray Tune will search for hyperparameters that maximize the mean accuracy. You can change the metric and mode to suit your specific use case.\n"
     ]
    }
   ],
   "source": [
    "print(answer_without_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_with_rag = LLMPlusRag(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=1000,\n",
    "    max_retries=10,\n",
    "    vector_store_path=f\"./data/vector_index/{hash_vector_store}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_with_rag = predictor_with_rag.query(query=query, similarity_top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To set a metric and mode in Ray Tune, you can use the `metric` and `mode` parameters when creating a `tune.Trainable` or when configuring a `tune.Tuner`.\n",
      "\n",
      "For example, in the Function training API:\n",
      "\n",
      "```python\n",
      "def trainable(config):\n",
      "    # ...\n",
      "    session.report({\"accuracy\": accuracy})\n",
      "\n",
      "tune.run(\n",
      "    trainable,\n",
      "    config=config,\n",
      "    metric=\"accuracy\",\n",
      "    mode=\"max\"\n",
      ")\n",
      "```\n",
      "\n",
      "And in the Class training API:\n",
      "\n",
      "```python\n",
      "class MyTrainable(tune.Trainable):\n",
      "    def step(self):\n",
      "        # ...\n",
      "        return {\"accuracy\": accuracy}\n",
      "\n",
      "tune.run(\n",
      "    MyTrainable,\n",
      "    config=config,\n",
      "    metric=\"accuracy\",\n",
      "    mode=\"max\"\n",
      ")\n",
      "```\n",
      "\n",
      "In both cases, the `metric` parameter specifies the name of the metric you want to optimize, and the `mode` parameter specifies whether to maximize or minimize the metric. The `mode` can be set to `\"max\"` or `\"min\"`.\n"
     ]
    }
   ],
   "source": [
    "print(answer_with_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning embedding configuration using ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"data/nodes/130956594988870197.pkl\", \"rb\") as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2023-08-31 09:13:55</td></tr>\n",
       "<tr><td>Running for: </td><td>00:00:07.52        </td></tr>\n",
       "<tr><td>Memory:      </td><td>31.4/64.0 GiB      </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 0/16 CPUs, 0/0 GPUs\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  \n",
       "  \n",
       "  Number of errored trials: 1<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th style=\"text-align: right;\">  # failures</th><th>error file                                                                                                                                                            </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_38782_00000</td><td style=\"text-align: right;\">           1</td><td>/Users/marwansarieddine/ray_results/objective_2023-08-31_09-13-47/objective_38782_00000_0_embedding_model_name=BAAI_bge-small-en,top_k=3_2023-08-31_09-13-47/error.txt</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>status  </th><th>loc            </th><th>embedding_model_name  </th><th style=\"text-align: right;\">  top_k</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_38782_00000</td><td>ERROR   </td><td>127.0.0.1:17250</td><td>BAAI/bge-small-en     </td><td style=\"text-align: right;\">      3</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 09:13:55,482\tERROR trial_runner.py:1450 -- Trial objective_38782_00000: Error happened when processing _ExecutorEventType.TRAINING_RESULT.\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::ImplicitFunc.train()\u001b[39m (pid=17250, ip=127.0.0.1, repr=objective)\n",
      "  File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/ray/tune/trainable/trainable.py\", line 384, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 336, in entrypoint\n",
      "    return self._trainable_func(\n",
      "  File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/ray/tune/trainable/function_trainable.py\", line 653, in _trainable_func\n",
      "    output = fn()\n",
      "  File \"/var/folders/b0/1qxcgh35671cbwgrdyyhzf5c0000gn/T/ipykernel_16043/135745827.py\", line 10, in objective\n",
      "  File \"/Users/marwansarieddine/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 284, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'data/nodes/130956594988870197.pkl'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(objective pid=17250)\u001b[0m path /Users/marwansarieddine/ray_results/objective_2023-08-31_09-13-47/objective_38782_00000_0_embedding_model_name=BAAI_bge-small-en,top_k=3_2023-08-31_09-13-47 <generator object Path.iterdir at 0x1bbab4270>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"trialProgress\">\n",
       "  <h3>Trial Progress</h3>\n",
       "  <table>\n",
       "<thead>\n",
       "<tr><th>Trial name           </th><th>date               </th><th>hostname       </th><th>node_ip  </th><th style=\"text-align: right;\">  pid</th><th style=\"text-align: right;\">  timestamp</th><th style=\"text-align: right;\">   trial_id</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>objective_38782_00000</td><td>2023-08-31_09-13-50</td><td>marwans-mbp.lan</td><td>127.0.0.1</td><td style=\"text-align: right;\">17250</td><td style=\"text-align: right;\"> 1693487630</td><td style=\"text-align: right;\">38782_00000</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "</div>\n",
       "<style>\n",
       ".trialProgress {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".trialProgress h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".trialProgress td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-31 09:13:55,496\tERROR tune.py:941 -- Trials did not complete: [objective_38782_00000]\n",
      "2023-08-31 09:13:55,497\tINFO tune.py:945 -- Total run time: 7.53 seconds (7.52 seconds for the tuning loop).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embedding_model_name': 'BAAI/bge-small-en', 'top_k': 3}\n"
     ]
    }
   ],
   "source": [
    "from ray import tune\n",
    "from pathlib import Path\n",
    "\n",
    "def objective(config):  # ①\n",
    "    from docsrag.embedding.index import VectorStoreIndexRay, VectorStoreSpec\n",
    "    from docsrag.embedding.evaluation import VectorStoreEvaluator\n",
    "\n",
    "    import pickle\n",
    "    print(\"path\", Path(\".\").resolve(), Path(\".\").iterdir())\n",
    "    with open(\"data/nodes/130956594988870197.pkl\", \"rb\") as f:\n",
    "        nodes = pickle.load(f)\n",
    "    node_limit = 100\n",
    "\n",
    "    embedding_vector_store_spec = VectorStoreSpec.parse_obj(\n",
    "        {\"embedding_model_name\": config[\"embedding_model_name\"]}\n",
    "    )\n",
    "\n",
    "    vector_store_index = VectorStoreIndexRay.build_from_spec(\n",
    "        nodes=nodes[:node_limit] if node_limit else nodes,\n",
    "        spec=embedding_vector_store_spec,\n",
    "        num_gpus=0,\n",
    "        batch_size=100,\n",
    "    )\n",
    "\n",
    "    evaluator = VectorStoreEvaluator(\n",
    "        vector_store_index=vector_store_index,\n",
    "        evaluation_dataset_name=\"1618109849114044135\",\n",
    "        top_ks=[3],\n",
    "    )\n",
    "    scores = evaluator.run()\n",
    "\n",
    "    return {\"score\": scores.scores[\"recall@k\"].squeeze()}\n",
    "\n",
    "\n",
    "search_space = {  # ②\n",
    "    \"embedding_model_name\": tune.choice(\n",
    "        [\n",
    "            \"BAAI/bge-small-en\",\n",
    "            # \"BAAI/bge-base-en\",\n",
    "        ],\n",
    "    ),\n",
    "    \"top_k\": tune.choice([1, 2, 3, 4]),\n",
    "}\n",
    "\n",
    "tuner = tune.Tuner(objective, param_space=search_space)  # ③\n",
    "\n",
    "results = tuner.fit()\n",
    "print(results.get_best_result(metric=\"score\", mode=\"max\").config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdocsrag\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39membedding\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mevaluation\u001b[39;00m \u001b[39mimport\u001b[39;00m VectorStoreEvaluator\n\u001b[1;32m      4\u001b[0m embedding_vector_store_spec \u001b[39m=\u001b[39m VectorStoreSpec\u001b[39m.\u001b[39mparse_obj(\n\u001b[1;32m      5\u001b[0m {\u001b[39m\"\u001b[39m\u001b[39membedding_model_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mBAAI/bge-base-en\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m      6\u001b[0m )\n\u001b[0;32m----> 8\u001b[0m vector_store_index \u001b[39m=\u001b[39m VectorStoreIndexRay\u001b[39m.\u001b[39;49mbuild_from_spec(\n\u001b[1;32m      9\u001b[0m nodes\u001b[39m=\u001b[39;49mnodes[:node_limit] \u001b[39mif\u001b[39;49;00m node_limit \u001b[39melse\u001b[39;49;00m nodes,\n\u001b[1;32m     10\u001b[0m spec\u001b[39m=\u001b[39;49membedding_vector_store_spec,\n\u001b[1;32m     11\u001b[0m num_gpus\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m     12\u001b[0m batch_size\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m )\n",
      "File \u001b[0;32m~/open_source/marwan/raybot/evaluation/evaluator/src/docsrag/embedding/index.py:91\u001b[0m, in \u001b[0;36mVectorStoreIndexRay.build_from_spec\u001b[0;34m(cls, nodes, spec, **ray_kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m vector_store \u001b[39m=\u001b[39m SimpleVectorStore()\n\u001b[1;32m     90\u001b[0m index_struct \u001b[39m=\u001b[39m IndexDict()\n\u001b[0;32m---> 91\u001b[0m embed_model \u001b[39m=\u001b[39m load_embed_model(spec\u001b[39m.\u001b[39;49membedding_model_name)\n\u001b[1;32m     92\u001b[0m docstore, vector_store, index_struct \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mupdate_from_nodes(\n\u001b[1;32m     93\u001b[0m     nodes\u001b[39m=\u001b[39mnodes,\n\u001b[1;32m     94\u001b[0m     docstore\u001b[39m=\u001b[39mdocstore,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mray_kwargs\n\u001b[1;32m     99\u001b[0m )\n\u001b[1;32m    100\u001b[0m index_store \u001b[39m=\u001b[39m SimpleIndexStore()\n",
      "File \u001b[0;32m~/open_source/marwan/raybot/evaluation/evaluator/src/docsrag/embedding/utils.py:42\u001b[0m, in \u001b[0;36mload_embed_model\u001b[0;34m(embedding_model_name)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mllama_index\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m get_cache_dir\n\u001b[1;32m     40\u001b[0m cache_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(get_cache_dir(), \u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m LangchainEmbedding(\n\u001b[0;32m---> 42\u001b[0m     HuggingFaceBgeEmbeddings(\n\u001b[1;32m     43\u001b[0m         model_name\u001b[39m=\u001b[39;49membedding_model_name,\n\u001b[1;32m     44\u001b[0m         cache_folder\u001b[39m=\u001b[39;49mcache_folder,\n\u001b[1;32m     45\u001b[0m         encode_kwargs\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39mshow_progress_bar\u001b[39;49m\u001b[39m\"\u001b[39;49m: \u001b[39mFalse\u001b[39;49;00m},\n\u001b[1;32m     46\u001b[0m     )\n\u001b[1;32m     47\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/langchain/embeddings/huggingface.py:231\u001b[0m, in \u001b[0;36mHuggingFaceBgeEmbeddings.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m    226\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(\n\u001b[1;32m    227\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mCould not import sentence_transformers python package. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    228\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease install it with `pip install sentence_transformers`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    229\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n\u001b[0;32m--> 231\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient \u001b[39m=\u001b[39m sentence_transformers\u001b[39m.\u001b[39;49mSentenceTransformer(\n\u001b[1;32m    232\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_name, cache_folder\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_folder, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_kwargs\n\u001b[1;32m    233\u001b[0m )\n\u001b[1;32m    234\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m-zh\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_name:\n\u001b[1;32m    235\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_instruction \u001b[39m=\u001b[39m DEFAULT_QUERY_BGE_INSTRUCTION_ZH\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/sentence_transformers/SentenceTransformer.py:87\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     83\u001b[0m     model_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[1;32m     86\u001b[0m         \u001b[39m# Download from hub with caching\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[1;32m     88\u001b[0m                             cache_dir\u001b[39m=\u001b[39;49mcache_folder,\n\u001b[1;32m     89\u001b[0m                             library_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39msentence-transformers\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     90\u001b[0m                             library_version\u001b[39m=\u001b[39;49m__version__,\n\u001b[1;32m     91\u001b[0m                             ignore_files\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mflax_model.msgpack\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrust_model.ot\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtf_model.h5\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     92\u001b[0m                             use_auth_token\u001b[39m=\u001b[39;49muse_auth_token)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(model_path, \u001b[39m'\u001b[39m\u001b[39mmodules.json\u001b[39m\u001b[39m'\u001b[39m)):    \u001b[39m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     modules \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/sentence_transformers/util.py:491\u001b[0m, in \u001b[0;36msnapshot_download\u001b[0;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[1;32m    486\u001b[0m \u001b[39mif\u001b[39;00m version\u001b[39m.\u001b[39mparse(huggingface_hub\u001b[39m.\u001b[39m__version__) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m version\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39m0.8.1\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    487\u001b[0m     \u001b[39m# huggingface_hub v0.8.1 introduces a new cache layout. We sill use a manual layout\u001b[39;00m\n\u001b[1;32m    488\u001b[0m     \u001b[39m# And need to pass legacy_cache_layout=True to avoid that a warning will be printed\u001b[39;00m\n\u001b[1;32m    489\u001b[0m     cached_download_args[\u001b[39m'\u001b[39m\u001b[39mlegacy_cache_layout\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 491\u001b[0m path \u001b[39m=\u001b[39m cached_download(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcached_download_args)\n\u001b[1;32m    493\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    494\u001b[0m     os\u001b[39m.\u001b[39mremove(path \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.lock\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[39m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[39m=\u001b[39mfn\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, has_token\u001b[39m=\u001b[39mhas_token, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/huggingface_hub/file_download.py:800\u001b[0m, in \u001b[0;36mcached_download\u001b[0;34m(url, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m    797\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m    798\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mdownloading \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m to \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, temp_file\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 800\u001b[0m     http_get(\n\u001b[1;32m    801\u001b[0m         url_to_download,\n\u001b[1;32m    802\u001b[0m         temp_file,\n\u001b[1;32m    803\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    804\u001b[0m         resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    805\u001b[0m         headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    806\u001b[0m         expected_size\u001b[39m=\u001b[39;49mexpected_size,\n\u001b[1;32m    807\u001b[0m     )\n\u001b[1;32m    809\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, url, cache_path)\n\u001b[1;32m    810\u001b[0m _chmod_and_replace(temp_file\u001b[39m.\u001b[39mname, cache_path)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/huggingface_hub/file_download.py:541\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[1;32m    531\u001b[0m     displayed_name \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(…)\u001b[39m\u001b[39m{\u001b[39;00mdisplayed_name[\u001b[39m-\u001b[39m\u001b[39m20\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    533\u001b[0m progress \u001b[39m=\u001b[39m tqdm(\n\u001b[1;32m    534\u001b[0m     unit\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mB\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m     unit_scale\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    539\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m(logger\u001b[39m.\u001b[39mgetEffectiveLevel() \u001b[39m==\u001b[39m logging\u001b[39m.\u001b[39mNOTSET),\n\u001b[1;32m    540\u001b[0m )\n\u001b[0;32m--> 541\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m r\u001b[39m.\u001b[39miter_content(chunk_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024\u001b[39m):\n\u001b[1;32m    542\u001b[0m     \u001b[39mif\u001b[39;00m chunk:  \u001b[39m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[1;32m    543\u001b[0m         progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw, \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw\u001b[39m.\u001b[39mstream(chunk_size, decode_content\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[39mexcept\u001b[39;00m ProtocolError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[39mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/urllib3/response.py:628\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    627\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m is_fp_closed(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp):\n\u001b[0;32m--> 628\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(amt\u001b[39m=\u001b[39;49mamt, decode_content\u001b[39m=\u001b[39;49mdecode_content)\n\u001b[1;32m    630\u001b[0m         \u001b[39mif\u001b[39;00m data:\n\u001b[1;32m    631\u001b[0m             \u001b[39myield\u001b[39;00m data\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/urllib3/response.py:567\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    564\u001b[0m fp_closed \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp, \u001b[39m\"\u001b[39m\u001b[39mclosed\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    566\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error_catcher():\n\u001b[0;32m--> 567\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp_read(amt) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fp_closed \u001b[39melse\u001b[39;00m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    569\u001b[0m         flush_decoder \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-evaluator-py39/lib/python3.9/site-packages/urllib3/response.py:533\u001b[0m, in \u001b[0;36mHTTPResponse._fp_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[39mreturn\u001b[39;00m buffer\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    531\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    532\u001b[0m     \u001b[39m# StringIO doesn't like amt=None\u001b[39;00m\n\u001b[0;32m--> 533\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fp\u001b[39m.\u001b[39;49mread(amt) \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fp\u001b[39m.\u001b[39mread()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/lib/python3.9/http/client.py:463\u001b[0m, in \u001b[0;36mHTTPResponse.read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mif\u001b[39;00m amt \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     \u001b[39m# Amount is given, implement using readinto\u001b[39;00m\n\u001b[1;32m    462\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mbytearray\u001b[39m(amt)\n\u001b[0;32m--> 463\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    464\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mmemoryview\u001b[39m(b)[:n]\u001b[39m.\u001b[39mtobytes()\n\u001b[1;32m    465\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    466\u001b[0m     \u001b[39m# Amount is not given (unbounded read) so we must check self.length\u001b[39;00m\n\u001b[1;32m    467\u001b[0m     \u001b[39m# and self.chunked\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/lib/python3.9/http/client.py:507\u001b[0m, in \u001b[0;36mHTTPResponse.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    502\u001b[0m         b \u001b[39m=\u001b[39m \u001b[39mmemoryview\u001b[39m(b)[\u001b[39m0\u001b[39m:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlength]\n\u001b[1;32m    504\u001b[0m \u001b[39m# we do not use _safe_read() here because this may be a .will_close\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[39m# connection, and the user is reading more bytes than will be provided\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[39m# (for example, reading in 1k chunks)\u001b[39;00m\n\u001b[0;32m--> 507\u001b[0m n \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadinto(b)\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m n \u001b[39mand\u001b[39;00m b:\n\u001b[1;32m    509\u001b[0m     \u001b[39m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[1;32m    510\u001b[0m     \u001b[39m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[1;32m    511\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_close_conn()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/lib/python3.9/socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 704\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    705\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    706\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/lib/python3.9/ssl.py:1241\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m flags \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1238\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1239\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m   1240\u001b[0m           \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m)\n\u001b[0;32m-> 1241\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nbytes, buffer)\n\u001b[1;32m   1242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/lib/python3.9/ssl.py:1099\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1097\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1098\u001b[0m     \u001b[39mif\u001b[39;00m buffer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sslobj\u001b[39m.\u001b[39;49mread(\u001b[39mlen\u001b[39;49m, buffer)\n\u001b[1;32m   1100\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1101\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sslobj\u001b[39m.\u001b[39mread(\u001b[39mlen\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "    from docsrag.embedding.index import VectorStoreIndexRay, VectorStoreSpec\n",
    "    from docsrag.embedding.evaluation import VectorStoreEvaluator\n",
    "\n",
    "embedding_vector_store_spec = VectorStoreSpec.parse_obj(\n",
    "    {\"embedding_model_name\": \"BAAI/bge-base-en\"}\n",
    ")\n",
    "\n",
    "vector_store_index = VectorStoreIndexRay.build_from_spec(\n",
    "    nodes=nodes[:node_limit] if node_limit else nodes,\n",
    "    # spec=embedding_vector_store_spec,\n",
    "    num_gpus=0,\n",
    "    batch_size=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raybot-evaluator-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
