{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/rag/nodes/130956594988870197.pkl\", \"rb\") as f:\n",
    "    nodes = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = nodes[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(node.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rag.generate_nodes import TikTokenTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TikTokenTokenizer(encoding=\"gpt2\").encoder\n",
    "token_lengths = [\n",
    "    len(encoder(node.text))\n",
    "    for node in nodes\n",
    "]\n",
    "\n",
    "np.histogram(token_lengths, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sum(token_lengths) + 1024 * len(nodes)) * 0.0015 / 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"src/rag/docs/816609263264840640.pkl\", \"rb\") as f:\n",
    "    docs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = nodes[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant that generates questions and answers from a provided context\n",
    "\n",
    "Here is an example of how the output should look:\n",
    "\n",
    "Q: How can I view the Ray Dashboard from outside the Kubernetes cluster?\n",
    "A: You can use port-forwarding. Run the command \"kubectl port-forward\"\n",
    "\n",
    "Provide questions and answers from the following context:\n",
    "\n",
    "{doc_text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(doc_text=node.text)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.Completion.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %psource openai.Completion\n",
    "# import os\n",
    "# openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.Completion.create(\n",
    "#     model=\"text-davinci-003\",\n",
    "#     prompt=prompt,\n",
    "#     max_tokens=256,\n",
    "#     temperature=1.0,\n",
    "#     top_p=0.8,\n",
    "#     n=1,\n",
    "#     stream=False,\n",
    "#     frequency_penalty=0,\n",
    "#     presence_penalty=0,\n",
    "#     best_of=1,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.choices[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.ChatCompletion.create?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = openai.ChatCompletion.create(\n",
    "#     model=\"gpt-3.5-turbo\",\n",
    "#     messages=[\n",
    "#         {\n",
    "#             \"role\": \"system\",\n",
    "#             \"content\": \"You are a helpful assistant that generates questions and answers from a provided context\",\n",
    "#         },\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": f\"\"\"Provide questions and answers from the following context:\n",
    "#             {node.text}\n",
    "#             \"\"\",\n",
    "#         },\n",
    "#     ],\n",
    "#     max_tokens=1024,\n",
    "#     temperature=1.0,\n",
    "#     top_p=0.8,\n",
    "#     n=1,\n",
    "#     stream=False,\n",
    "#     frequency_penalty=0,\n",
    "#     presence_penalty=0,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(response.choices[0].message[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Q: How can I set up ingress to access the Ray Dashboard?\n",
    "A: Follow the instructions provided in the link \"kuberay-ingress\".\n",
    "\n",
    "Q: How can I view the Ray Dashboard from outside the Kubernetes cluster?\n",
    "A: You can use port-forwarding. Run the command \"kubectl port-forward --address 0.0.0.0 service/${RAYCLUSTER_NAME}-head-svc 8265:8265\" and visit ${YOUR_IP}:8265 for the Dashboard.\n",
    "\n",
    "Q: Can I use port-forwarding in a production environment?\n",
    "A: No, it is not recommended to use port-forwarding in a production environment. Follow the instructions provided to expose the Dashboard with Ingress.\n",
    "\n",
    "Q: How should I access the Ray Dashboard when running behind a reverse proxy?\n",
    "A: Always access the dashboard with a trailing \"/\" at the end of the URL. For example, if your proxy is set up to handle requests to \"/ray/dashboard\", view the dashboard at \"www.my-website.com/ray/dashboard/\".\n",
    "\n",
    "Q: How can I make the Ray Dashboard visible without a trailing \"/\"?\n",
    "A: Include a rule in your reverse proxy that redirects the user's browser to \"/\". For example, \"/ray/dashboard\" should be redirected to \"/ray/dashboard/\".\n",
    "\n",
    "Q: How can I disable the Ray Dashboard?\n",
    "A: The Dashboard is included by default when using certain installation commands. To disable it, use the argument \"--include-dashboard\".\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = node.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata[\"question_answers\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(node.metadata[\"text_hash\"], 16) % 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1024 / 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet(\"src/rag/qa/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=[\"text_hash\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = df['generated_question_answers'][0]\n",
    "type(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_qa(row):\n",
    "    val = row.generated_question_answers\n",
    "    lines = [line for line in val.split(\"\\n\") if line.strip()]\n",
    "    questions = [line.split(\":\")[1].strip() for line in lines if line.startswith(\"Q\")]\n",
    "    answers = [line.split(\":\")[1].strip() for line in lines if line.startswith(\"A\")]\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            \"file_path\": row.file_path,\n",
    "            \"file_name\": row.file_name,\n",
    "            \"text_hash\": row.text_hash,\n",
    "            \"qa_generator_hash\": row.qa_generator_hash,\n",
    "            \"node_parser_hash\": row.node_parser_hash,\n",
    "            \"text_hash_bin\": row.text_hash_bin,\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "        }\n",
    "        for question, answer in zip(questions, answers)\n",
    "    )\n",
    "\n",
    "dfs = [parse_qa(row) for row in df.itertuples()]\n",
    "out = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.astype({\n",
    "    \"file_path\": \"category\",\n",
    "    \"file_name\": \"category\",\n",
    "    \"text_hash\": \"category\",\n",
    "    \"qa_generator_hash\": \"category\",\n",
    "    \"node_parser_hash\": \"category\",\n",
    "    \"text_hash_bin\": \"category\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out.to_parquet(\n",
    "#     \"src/rag/qa_parsed\",\n",
    "#     partition_cols=[\"qa_generator_hash\", \"node_parser_hash\", \"text_hash_bin\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import StorageContext, load_index_from_storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=\"src/rag/vector_store_index_beta\"),\n",
    "    index_id=\"dd3dc146-583b-4bcc-a5d7-a616d6766ec9\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.schema import NodeWithScore\n",
    "from llama_index.vector_stores.types import VectorStoreQuery\n",
    "from llama_index.indices.query.schema import QueryBundle\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "from llama_index.utils import get_cache_dir\n",
    "\n",
    "cache_folder = os.path.join(get_cache_dir(), \"models\")\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceBgeEmbeddings(\n",
    "        model_name=\"BAAI/bge-small-en\",\n",
    "        cache_folder=cache_folder,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"question\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_str = out[\"question\"][0]\n",
    "\n",
    "query_bundle = QueryBundle(\n",
    "    query_str=query_str,\n",
    "    embedding=embed_model.get_agg_embedding_from_queries([query_str]),\n",
    ")\n",
    "\n",
    "query_result = vector_store._vector_store.query(\n",
    "    VectorStoreQuery(\n",
    "        query_embedding=query_bundle.embedding,\n",
    "        similarity_top_k=10,\n",
    "        node_ids=None,\n",
    "        doc_ids=None,\n",
    "        filters=None,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(embedding1, embedding2):\n",
    "    product = np.dot(embedding1, embedding2)\n",
    "    norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)\n",
    "    return product / norm\n",
    "\n",
    "\n",
    "text_hash_to_nodes = {node.metadata[\"text_hash\"]: node for node in nodes}\n",
    "\n",
    "similarities = []\n",
    "for row in out.itertuples():\n",
    "    node = text_hash_to_nodes.get(row.text_hash)\n",
    "    question = row.question\n",
    "    embedded_question = embed_model.get_agg_embedding_from_queries([question])\n",
    "    embedded_node_context = embed_model.get_agg_embedding_from_queries([node.text])\n",
    "    similarity = cosine_similarity(embedded_question, embedded_node_context)\n",
    "    similarities.append(similarity)\n",
    "\n",
    "out[\"similarity\"] = similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[\"similarity\"].describe(\n",
    "    percentiles=[0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[out[\"similarity\"] < 0.78][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out[(out[\"similarity\"] > 0.80) & (out[\"similarity\"] < 0.84)][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result.ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store._index_struct.nodes_dict[query_result.ids[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(\n",
    "    [\n",
    "        node.metadata\n",
    "        for node in vector_store._docstore.get_nodes(\n",
    "            [vector_store._index_struct.nodes_dict[id_] for id_ in query_result.ids]\n",
    "        )\n",
    "    ],\n",
    "    query_result.similarities,\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get average cosine similarity between question and intended node according to chosen embedding model\n",
    "# compute precision, recall for different similarity thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever(similarity_top_k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_with_scores = retriever.retrieve(query_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.options.display.max_colwidth = 1000\n",
    "# out[out[\"similarity\"] > 0.9][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"~/Downloads/triviaqa-unfiltered/unfiltered-web-train.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Data\"][2]['Question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trivia_questions = [\n",
    "    row['Question']\n",
    "    for row in df[\"Data\"][:100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(trivia_questions).to_frame(\"question\").to_parquet(\"src/rag/qa_trivia_questions.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.indices.postprocessor.node import (\n",
    "    SimilarityPostprocessor,\n",
    "    KeywordNodePostprocessor,\n",
    "    PrevNextNodePostprocessor,\n",
    "    AutoPrevNextNodePostprocessor,\n",
    ")\n",
    "from llama_index.indices.postprocessor.node_recency import (\n",
    "    FixedRecencyPostprocessor,\n",
    "    EmbeddingRecencyPostprocessor,\n",
    "    TimeWeightedPostprocessor,\n",
    ")\n",
    "from llama_index.indices.postprocessor.pii import (\n",
    "    PIINodePostprocessor,\n",
    "    NERPIINodePostprocessor,\n",
    ")\n",
    "from llama_index.indices.postprocessor.llm_rerank import LLMRerank\n",
    "from llama_index.indices.postprocessor.cohere_rerank import CohereRerank\n",
    "from llama_index.indices.postprocessor.metadata_replacement import (\n",
    "    MetadataReplacementPostProcessor,\n",
    ")\n",
    "from llama_index.indices.postprocessor.optimizer import SentenceEmbeddingOptimizer\n",
    "from llama_index.indices.postprocessor.sbert_rerank import SentenceTransformerRerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = SimilarityPostprocessor(similarity_cutoff=0.80)\n",
    "\n",
    "postprocessor.postprocess_nodes(nodes_with_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AutoModelForCausalLM.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "BertLMHeadModel does not support `device_map='auto'`. To implement support, the modelclass needs to implement the `_no_split_modules` attribute.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m HuggingFaceLLM(\n\u001b[1;32m      2\u001b[0m     model_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      3\u001b[0m     tokenizer_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mbert-base-uncased\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      4\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-rag-py39/lib/python3.9/site-packages/llama_index/llms/huggingface.py:112\u001b[0m, in \u001b[0;36mHuggingFaceLLM.__init__\u001b[0;34m(self, context_window, max_new_tokens, system_prompt, query_wrapper_prompt, tokenizer_name, model_name, model, tokenizer, device_map, stopping_ids, tokenizer_kwargs, tokenizer_outputs_to_remove, model_kwargs, generate_kwargs, callback_manager)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[1;32m    105\u001b[0m     AutoModelForCausalLM,\n\u001b[1;32m    106\u001b[0m     AutoTokenizer,\n\u001b[1;32m    107\u001b[0m     StoppingCriteria,\n\u001b[1;32m    108\u001b[0m     StoppingCriteriaList,\n\u001b[1;32m    109\u001b[0m )\n\u001b[1;32m    111\u001b[0m model_kwargs \u001b[39m=\u001b[39m model_kwargs \u001b[39mor\u001b[39;00m {}\n\u001b[0;32m--> 112\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model \u001b[39m=\u001b[39m model \u001b[39mor\u001b[39;00m AutoModelForCausalLM\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    113\u001b[0m     model_name, device_map\u001b[39m=\u001b[39;49mdevice_map, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs\n\u001b[1;32m    114\u001b[0m )\n\u001b[1;32m    116\u001b[0m \u001b[39m# check context_window\u001b[39;00m\n\u001b[1;32m    117\u001b[0m config_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mto_dict()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-rag-py39/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:516\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(config) \u001b[39min\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    515\u001b[0m     model_class \u001b[39m=\u001b[39m _get_model_class(config, \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 516\u001b[0m     \u001b[39mreturn\u001b[39;00m model_class\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m    517\u001b[0m         pretrained_model_name_or_path, \u001b[39m*\u001b[39;49mmodel_args, config\u001b[39m=\u001b[39;49mconfig, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mhub_kwargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    520\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnrecognized configuration class \u001b[39m\u001b[39m{\u001b[39;00mconfig\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m for this kind of AutoModel: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    521\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel type should be one of \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(c\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m \u001b[39m\u001b[39mfor\u001b[39;00m\u001b[39m \u001b[39mc\u001b[39m \u001b[39m\u001b[39min\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_model_mapping\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.12/envs/raybot-rag-py39/lib/python3.9/site-packages/transformers/modeling_utils.py:2992\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2989\u001b[0m     target_dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mint8\n\u001b[1;32m   2991\u001b[0m \u001b[39mif\u001b[39;00m model\u001b[39m.\u001b[39m_no_split_modules \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 2992\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   2993\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m does not support `device_map=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mdevice_map\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m`. To implement support, the model\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2994\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mclass needs to implement the `_no_split_modules` attribute.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2995\u001b[0m     )\n\u001b[1;32m   2996\u001b[0m no_split_modules \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39m_no_split_modules\n\u001b[1;32m   2997\u001b[0m \u001b[39mif\u001b[39;00m device_map \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbalanced\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mbalanced_low_0\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msequential\u001b[39m\u001b[39m\"\u001b[39m]:\n",
      "\u001b[0;31mValueError\u001b[0m: BertLMHeadModel does not support `device_map='auto'`. To implement support, the modelclass needs to implement the `_no_split_modules` attribute."
     ]
    }
   ],
   "source": [
    "# HuggingFaceLLM(\n",
    "#     model_name=\"bert-base-uncased\",\n",
    "#     tokenizer_name=\"bert-base-uncased\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "raybot-rag-py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
