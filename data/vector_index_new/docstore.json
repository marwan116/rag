{"docstore/data": {"61bb8b60-d910-44ef-bbf3-009a5b087f0d": {"__data__": {"id_": "61bb8b60-d910-44ef-bbf3-009a5b087f0d", "embedding": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "c596ee609b9612fb3b6954377f58ec7b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "d2b7105e2b9fcc418193a6d0976271bc9b1091ae4ef60f9ff542878f7dc2e62a"}, "3": {"node_id": "56cc558b-ca80-425a-8c55-6124bc66f823", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "ff6ef2c5820447be91bc611845fe1292"}, "hash": "54da4738116b8c2cc321eaae1b21ee5d69179c3384af914f652388566d8d96f7"}}, "hash": "47cf3c5d503334e5c0324df9411fffa3f47e21b627b55544ce2e4db77ee5de17", "text": ".. _tune-60-seconds:\n\n========================\nKey Concepts of Ray Tune\n========================\n\n.. TODO: should we introduce checkpoints as well?.. TODO: should we at least mention \"Stopper\" classes here?Let's quickly walk through the key concepts you need to know to use Tune.If you want to see practical tutorials right away, go visit our :ref:`user guides <tune-guides>`.In essence, Tune has six crucial components that you need to understand.First, you define the hyperparameters you want to tune in a `search space` and pass them into a `trainable`\nthat specifies the objective you want to tune.Then you select a `search algorithm` to effectively optimize your parameters and optionally use a\n`scheduler` to stop searches early and speed up your experiments.Together with other configuration, your `trainable`, search algorithm, and scheduler are passed into ``Tuner``,\nwhich runs your experiments and creates `trials`.These trials can then be used in `analyses` to inspect your experiment results.The following figure shows an overview of these components, which we cover in detail in the next sections... image:: images/tune_flow.png\n\n.. _tune_60_seconds_trainables:\n\nRay Tune Trainables\n-------------------\n\nIn short, a :ref:`Trainable <trainable-docs>` is an object that you can pass into a Tune run.Ray Tune has two ways of defining a `trainable`, namely the :ref:`Function API <tune-function-api>`\nand the :ref:`Class API <tune-class-api>`.Both are valid ways of defining a `trainable`, but the Function API is generally recommended and is used\nthroughout the rest of this guide.Let's say we want to optimize a simple objective function like ``a (x ** 2) + b`` in which ``a`` and ``b`` are the\nhyperparameters we want to tune to `minimize` the objective.Since the objective also has a variable ``x``, we need to test for different values of ``x``.Given concrete choices for ``a``, ``b`` and ``x`` we can evaluate the objective function and get a `score` to minimize... tab-set::\n\n    .. tab-item:: Function API\n\n        With the :ref:`the function-based API <tune-function-api>` you create a function (here called ``trainable``) that\n        takes in a dictionary of hyperparameters.This function computes a ``score`` in a \"training loop\" and `reports` this score back to Tune:\n\n        .. literalinclude:: doc_code/key_concepts.py\n            :language: python\n            :start-after: __function_api_start__\n            :end-before: __function_api_end__\n\n        Note that we use ``session.report(...)`` to report the intermediate ``score`` in the training loop, which can be useful\n        in many machine learning tasks.If you just want to report the final ``score`` outside of this loop, you can simply return the score at the\n        end of the ``trainable`` function with ``return {\"score\": score}``.You can also use ``yield {\"score\": score}`` instead of ``session.report()``... tab-item:: Class API\n\n        Here's an example of specifying the objective function using the :ref:`class-based API <tune-class-api>`:\n\n        .. literalinclude:: doc_code/key_concepts.py\n            :language: python\n            :start-after: __class_api_start__\n            :end-before: __class_api_end__\n\n        .. tip:: ``session.report`` can't be used within a ``Trainable`` class.Learn more about the details of :ref:`Trainables here <trainable-docs>`\nand :ref:`have a look at our examples <tune-general-examples>`.Next, let's have a closer look at what the ``config`` dictionary is that you pass into your trainables.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "56cc558b-ca80-425a-8c55-6124bc66f823": {"__data__": {"id_": "56cc558b-ca80-425a-8c55-6124bc66f823", "embedding": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "ff6ef2c5820447be91bc611845fe1292"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "d2b7105e2b9fcc418193a6d0976271bc9b1091ae4ef60f9ff542878f7dc2e62a"}, "2": {"node_id": "61bb8b60-d910-44ef-bbf3-009a5b087f0d", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "c596ee609b9612fb3b6954377f58ec7b"}, "hash": "47cf3c5d503334e5c0324df9411fffa3f47e21b627b55544ce2e4db77ee5de17"}, "3": {"node_id": "c82eb84a-5951-459a-8aa6-3821207e70fd", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "7106bcb320b1c6f166b763c5b5ea9624"}, "hash": "f36b60dd16e161b7c3995b9d78797aa401eab14977048a0a4a8c62758e9195da"}}, "hash": "54da4738116b8c2cc321eaae1b21ee5d69179c3384af914f652388566d8d96f7", "text": ".. _tune-key-concepts-search-spaces:\n\nTune Search Spaces\n------------------\n\nTo optimize your *hyperparameters*, you have to define a *search space*.A search space defines valid values for your hyperparameters and can specify\nhow these values are sampled (e.g.from a uniform distribution or a normal\ndistribution).Tune offers various functions to define search spaces and sampling methods.:ref:`You can find the documentation of these search space definitions here <tune-search-space>`.Here's an example covering all search space functions.Again,\n:ref:`here is the full explanation of all these functions <tune-search-space>`... literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __config_start__\n    :end-before: __config_end__\n\n.. _tune_60_seconds_trials:\n\nTune Trials\n-----------\n\nYou use :ref:`Tuner.fit <tune-run-ref>` to execute and manage hyperparameter tuning and generate your `trials`.At a minimum, your ``Tuner`` call takes in a trainable as first argument, and a ``param_space`` dictionary\nto define the search space.The ``Tuner.fit()`` function also provides many features such as :ref:`logging <tune-logging>`,\n:ref:`checkpointing <tune-trial-checkpoint>`, and :ref:`early stopping <tune-stopping-ref>`.In the example, minimizing ``a (x ** 2) + b``, a simple Tune run with a simplistic search space for ``a`` and ``b``\nlooks like this:\n\n.. literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __run_tunable_start__\n    :end-before: __run_tunable_end__\n\n``Tuner.fit`` will generate a couple of hyperparameter configurations from its arguments,\nwrapping them into :ref:`Trial objects <trial-docstring>`.Trials contain a lot of information.For instance, you can get the hyperparameter configuration using (``trial.config``), the trial ID (``trial.trial_id``),\nthe trial's resource specification (``resources_per_trial`` or ``trial.placement_group_factory``) and many other values.By default ``Tuner.fit`` will execute until all trials stop or error.Here's an example output of a trial run:\n\n.. TODO: how to make sure this doesn't get outdated?.. code-block:: bash\n\n    == Status ==\n    Memory usage on this node: 11.4/16.0 GiB\n    Using FIFO scheduling algorithm.Resources requested: 1/12 CPUs, 0/0 GPUs, 0.0/3.17 GiB heap, 0.0/1.07 GiB objects\n    Result logdir: /Users/foo/ray_results/myexp\n    Number of trials: 1 (1 RUNNING)\n    +----------------------+----------+---------------------+-----------+--------+--------+----------------+-------+\n    | Trial name           | status   | loc                 |         a |      b |  score | total time (s) |  iter |\n    |----------------------+----------+---------------------+-----------+--------+--------+----------------+-------|\n    | Trainable_a826033a | RUNNING  | 10.234.98.164:31115 | 0.303706  | 0.0761 | 0.1289 |        7.54952 |    15 |\n    +----------------------+----------+---------------------+-----------+--------+--------+----------------+-------+", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c82eb84a-5951-459a-8aa6-3821207e70fd": {"__data__": {"id_": "c82eb84a-5951-459a-8aa6-3821207e70fd", "embedding": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "7106bcb320b1c6f166b763c5b5ea9624"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "d2b7105e2b9fcc418193a6d0976271bc9b1091ae4ef60f9ff542878f7dc2e62a"}, "2": {"node_id": "56cc558b-ca80-425a-8c55-6124bc66f823", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "ff6ef2c5820447be91bc611845fe1292"}, "hash": "54da4738116b8c2cc321eaae1b21ee5d69179c3384af914f652388566d8d96f7"}, "3": {"node_id": "6a357b7a-46f9-45c1-bd1d-c85bf353636c", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "849c2d6ae34cf8f02391a7cf3bc4a9c7"}, "hash": "92c0b10501addd9f99cb2f17a27784bc26a76b899c03effd6daaaa9e3705c909"}}, "hash": "f36b60dd16e161b7c3995b9d78797aa401eab14977048a0a4a8c62758e9195da", "text": "You can also easily run just 10 trials by specifying the number of samples (``num_samples``).\nTune automatically :ref:`determines how many trials will run in parallel <tune-parallelism>`.\nNote that instead of the number of samples, you can also specify a time budget in seconds through ``time_budget_s``,\nif you set ``num_samples=-1``.\n\n.. literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __run_tunable_samples_start__\n    :end-before: __run_tunable_samples_end__\n\n\nFinally, you can use more interesting search spaces to optimize your hyperparameters\nvia Tune's :ref:`search space API <tune-default-search-space>`, like using random samples or grid search.Here's an example of uniformly sampling between ``[0, 1]`` for ``a`` and ``b``:\n\n.. literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __search_space_start__\n    :end-before: __search_space_end__\n\nTo learn more about the various ways of configuring your Tune runs,\ncheck out the :ref:`Tuner API reference <tune-run-ref>`... _search-alg-ref:\n\nTune Search Algorithms\n----------------------\n\nTo optimize the hyperparameters of your training process, you use\na :ref:`Search Algorithm <tune-search-alg>` which suggests hyperparameter configurations.If you don't specify a search algorithm, Tune will use random search by default, which can provide you\nwith a good starting point for your hyperparameter optimization.For instance, to use Tune with simple Bayesian optimization through the ``bayesian-optimization`` package\n(make sure to first run ``pip install bayesian-optimization``), we can define an ``algo`` using ``BayesOptSearch``.Simply pass in a ``search_alg`` argument to ``tune.TuneConfig``, which is taken in by ``Tuner``:\n\n.. literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __bayes_start__\n    :end-before: __bayes_end__\n\nTune has Search Algorithms that integrate with many popular **optimization** libraries,\nsuch as :ref:`Nevergrad <nevergrad>`, :ref:`HyperOpt <tune-hyperopt>`, or :ref:`Optuna <tune-optuna>`.Tune automatically converts the provided search space into the search\nspaces the search algorithms and underlying libraries expect.See the :ref:`Search Algorithm API documentation <tune-search-alg>` for more details.Here's an overview of all available search algorithms in Tune:\n\n. list-table::\n   :widths: 5 5 2 10\n   :header-rows: 1\n\n   * - SearchAlgorithm\n     - Summary\n     - Website\n     - Code Example\n   * - :ref:`Random search/grid search <tune-basicvariant>`\n     - Random search/grid search\n     -\n     - :doc:`/tune/examples/includes/tune_basic_example`\n   * - :ref:`AxSearch <tune-ax>`\n     - Bayesian/Bandit Optimization\n     - [`Ax <https://ax.dev/>`__]\n     - :doc:`/tune/examples/includes/ax_example`\n   * - :ref:`BlendSearch <BlendSearch>`\n     - Blended Search\n     - [`Bs <https://github.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6a357b7a-46f9-45c1-bd1d-c85bf353636c": {"__data__": {"id_": "6a357b7a-46f9-45c1-bd1d-c85bf353636c", "embedding": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "849c2d6ae34cf8f02391a7cf3bc4a9c7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "d2b7105e2b9fcc418193a6d0976271bc9b1091ae4ef60f9ff542878f7dc2e62a"}, "2": {"node_id": "c82eb84a-5951-459a-8aa6-3821207e70fd", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "7106bcb320b1c6f166b763c5b5ea9624"}, "hash": "f36b60dd16e161b7c3995b9d78797aa401eab14977048a0a4a8c62758e9195da"}, "3": {"node_id": "a295e8ba-aac0-403f-964c-60bd516443af", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "9b09e41327d01f3d9c7d36b07564a7f8"}, "hash": "ef79ee6a784631b4f00c7663114576c87f0c3930f1b2e7accbc0181f416e7883"}}, "hash": "92c0b10501addd9f99cb2f17a27784bc26a76b899c03effd6daaaa9e3705c909", "text": "com/microsoft/FLAML/tree/main/flaml/tune>`__]\n     - :doc:`/tune/examples/includes/blendsearch_example`\n   * - :ref:`CFO <CFO>`\n     - Cost-Frugal hyperparameter Optimization\n     - [`Cfo <https://github.com/microsoft/FLAML/tree/main/flaml/tune>`__]\n     - :doc:`/tune/examples/includes/cfo_example`\n   * - :ref:`DragonflySearch <Dragonfly>`\n     - Scalable Bayesian Optimization\n     - [`Dragonfly <https://dragonfly-opt.readthedocs.io/>`__]\n     - :doc:`/tune/examples/includes/dragonfly_example`\n   * - :ref:`SkoptSearch <skopt>`\n     - Bayesian Optimization\n     - [`Scikit-Optimize <https://scikit-optimize.github.io>`__]\n     - :doc:`/tune/examples/includes/skopt_example`\n   * - :ref:`HyperOptSearch <tune-hyperopt>`\n     - Tree-Parzen Estimators\n     - [`HyperOpt <http://hyperopt.github.io/hyperopt>`__]\n     - :doc:`/tune/examples/hyperopt_example`\n   * - :ref:`BayesOptSearch <bayesopt>`\n     - Bayesian Optimization\n     - [`BayesianOptimization <https://github.com/fmfn/BayesianOptimization>`__]\n     - :doc:`/tune/examples/includes/bayesopt_example`\n   * - :ref:`TuneBOHB <suggest-TuneBOHB>`\n     - Bayesian Opt/HyperBand\n     - [`BOHB <https://github.com/automl/HpBandSter>`__]\n     - :doc:`/tune/examples/includes/bohb_example`\n   * - :ref:`NevergradSearch <nevergrad>`\n     - Gradient-free Optimization\n     - [`Nevergrad <https://github.com/facebookresearch/nevergrad>`__]\n     - :doc:`/tune/examples/includes/nevergrad_example`\n   * - :ref:`OptunaSearch <tune-optuna>`\n     - Optuna search algorithms\n     - [`Optuna <https://optuna.org/>`__]\n     - :doc:`/tune/examples/optuna_example`\n   * - :ref:`ZOOptSearch <zoopt>`\n     - Zeroth-order Optimization\n     - [`ZOOpt <https://github.com/polixir/ZOOpt>`__]\n     - :doc:`/tune/examples/includes/zoopt_example`\n   * - :ref:`SigOptSearch <sigopt>`\n     - Closed source\n     - [`SigOpt <https://sigopt.com/>`__]\n     - :doc:`/tune/examples/includes/sigopt_example`\n   * - :ref:`HEBOSearch <tune-hebo>`\n     - Heteroscedastic Evolutionary Bayesian Optimization\n     - [`HEBO <https://github.com/huawei-noah/HEBO/tree/master/HEBO>`__]\n     - :doc:`/tune/examples/includes/hebo_example`\n\n. note:: Unlike :ref:`Tune's Trial Schedulers <tune-schedulers>`,", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a295e8ba-aac0-403f-964c-60bd516443af": {"__data__": {"id_": "a295e8ba-aac0-403f-964c-60bd516443af", "embedding": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "9b09e41327d01f3d9c7d36b07564a7f8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "d2b7105e2b9fcc418193a6d0976271bc9b1091ae4ef60f9ff542878f7dc2e62a"}, "2": {"node_id": "6a357b7a-46f9-45c1-bd1d-c85bf353636c", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "849c2d6ae34cf8f02391a7cf3bc4a9c7"}, "hash": "92c0b10501addd9f99cb2f17a27784bc26a76b899c03effd6daaaa9e3705c909"}, "3": {"node_id": "ef5a6ff9-a4ab-42a2-841a-a02eaf1caa14", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "f6ae27fb6ab85d8cbcf2ccb180ffcad1"}, "hash": "cabf0a0a67a19fa8535cef19aa249e1d45d9ae97f520755e74574e94c9cf4811"}}, "hash": "ef79ee6a784631b4f00c7663114576c87f0c3930f1b2e7accbc0181f416e7883", "text": "Tune Search Algorithms cannot affect or stop training processes.However, you can use them together to early stop the evaluation of bad trials.In case you want to implement your own search algorithm, the interface is easy to implement,\nyou can :ref:`read the instructions here <byo-algo>`.Tune also provides helpful utilities to use with Search Algorithms:\n\n * :ref:`repeater`: Support for running each *sampled hyperparameter* with multiple random seeds.* :ref:`limiter`: Limits the amount of concurrent trials when running optimization.* :ref:`shim`: Allows creation of the search algorithm object given a string.Note that in the example above we  tell Tune to ``stop`` after ``20`` training iterations.This way of stopping trials with explicit rules is useful, but in many cases we can do even better with\n`schedulers`... _schedulers-ref:\n\nTune Schedulers\n---------------\n\nTo make your training process more efficient, you can use a :ref:`Trial Scheduler <tune-schedulers>`.For instance, in our ``trainable`` example minimizing a function in a training loop, we used ``session.report()``.This reported `incremental` results, given a hyperparameter configuration selected by a search algorithm.Based on these reported results, a Tune scheduler can decide whether to stop the trial early or not.If you don't specify a scheduler, Tune will use a first-in-first-out (FIFO) scheduler by default, which simply\npasses through the trials selected by your search algorithm in the order they were picked and does not perform any early stopping.In short, schedulers can stop, pause, or tweak the\nhyperparameters of running trials, potentially making your hyperparameter tuning process much faster.Unlike search algorithms, :ref:`Trial Scheduler <tune-schedulers>` do not select which hyperparameter\nconfigurations to evaluate.Here's a quick example of using the so-called ``HyperBand`` scheduler to tune an experiment.All schedulers take in a ``metric``, which is the value reported by your trainable.The ``metric`` is then maximized or minimized according to the ``mode`` you provide.To use a scheduler, just pass in a ``scheduler`` argument to ``tune.TuneConfig``, which is taken in by ``Tuner``:\n\n.. literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __hyperband_start__\n    :end-before: __hyperband_end__\n\n\nTune includes distributed implementations of early stopping algorithms such as\n`Median Stopping Rule <https://research.google.com/pubs/pub46180.html>`__, `HyperBand <https://arxiv.org/abs/1603.06560>`__,\nand `ASHA <https://openreview.net/forum?id=S1Y7OOlRZ>`__.Tune also includes a distributed implementation of `Population Based Training (PBT) <https://www.deepmind.com/blog/population-based-training-of-neural-networks>`__\nand `Population Based Bandits (PB2) <https://arxiv.org/abs/2002.02518>`__... tip:: The easiest scheduler to start with is the ``ASHAScheduler`` which will aggressively terminate low-performing trials.When using schedulers, you may face compatibility issues, as shown in the below compatibility matrix.Certain schedulers cannot be used with search algorithms,\nand certain schedulers require that you implement :ref:`checkpointing <tune-trial-checkpoint>`.Schedulers can dynamically change trial resource requirements during tuning.This is implemented in :ref:`ResourceChangingScheduler<tune-resource-changing-scheduler>`,\nwhich can wrap around any other scheduler... list-table:: Scheduler Compatibility Matrix\n   :header-rows: 1\n\n   * - Scheduler\n     - Need Checkpointing?- SearchAlg Compatible?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ef5a6ff9-a4ab-42a2-841a-a02eaf1caa14": {"__data__": {"id_": "ef5a6ff9-a4ab-42a2-841a-a02eaf1caa14", "embedding": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "f6ae27fb6ab85d8cbcf2ccb180ffcad1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "d2b7105e2b9fcc418193a6d0976271bc9b1091ae4ef60f9ff542878f7dc2e62a"}, "2": {"node_id": "a295e8ba-aac0-403f-964c-60bd516443af", "node_type": null, "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "9b09e41327d01f3d9c7d36b07564a7f8"}, "hash": "ef79ee6a784631b4f00c7663114576c87f0c3930f1b2e7accbc0181f416e7883"}}, "hash": "cabf0a0a67a19fa8535cef19aa249e1d45d9ae97f520755e74574e94c9cf4811", "text": "- Example\n   * - :ref:`ASHA <tune-scheduler-hyperband>`\n     - No\n     - Yes\n     - :doc:`Link </tune/examples/includes/async_hyperband_example>`\n   * - :ref:`Median Stopping Rule <tune-scheduler-msr>`\n     - No\n     - Yes\n     - :ref:`Link <tune-scheduler-msr>`\n   * - :ref:`HyperBand <tune-original-hyperband>`\n     - Yes\n     - Yes\n     - :doc:`Link </tune/examples/includes/hyperband_example>`\n   * - :ref:`BOHB <tune-scheduler-bohb>`\n     - Yes\n     - Only TuneBOHB\n     - :doc:`Link </tune/examples/includes/bohb_example>`\n   * - :ref:`Population Based Training <tune-scheduler-pbt>`\n     - Yes\n     - Not Compatible\n     - :doc:`Link </tune/examples/includes/pbt_function>`\n   * - :ref:`Population Based Bandits <tune-scheduler-pb2>`\n     - Yes\n     - Not Compatible\n     - :doc:`Basic Example </tune/examples/includes/pb2_example>`, :doc:`PPO example </tune/examples/includes/pb2_ppo_example>`\n\nLearn more about trial schedulers in :ref:`the scheduler API documentation <schedulers-ref>`... _tune-concepts-analysis:\n\nTune Run Analyses\n-----------------\n\n``Tuner.fit()`` returns an :ref:`ResultGrid <tune-analysis-docs>` object which has methods you can use for\nanalyzing your training.The following example shows you how to access various metrics from an ``ResultGrid`` object, like the best available\ntrial, or the best hyperparameter configuration for that trial:\n\n.. literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __analysis_start__\n    :end-before: __analysis_end__\n\nThis object can also retrieve all training runs as dataframes,\nallowing you to do ad-hoc data analysis over your results... literalinclude:: doc_code/key_concepts.py\n    :language: python\n    :start-after: __results_start__\n    :end-before: __results_end__\n\nSee :doc:`/tune/examples/tune_analyze_results` for more usage examples.What's Next?-------------\n\nNow that you have a working understanding of Tune, check out:\n\n* :ref:`tune-guides`: Tutorials for using Tune with your preferred machine learning library.* :doc:`/tune/examples/index`: End-to-end examples and templates for using Tune with your preferred machine learning library.* :doc:`/tune/getting-started`: A simple tutorial that walks you through the process of setting up a Tune experiment.\n\n\nFurther Questions or Issues?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. include:: /_includes/_help.rst", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "aa5f740c-f0f8-454e-aaee-b5cffa937825": {"__data__": {"id_": "aa5f740c-f0f8-454e-aaee-b5cffa937825", "embedding": null, "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst", "text_hash": "fdd5461656d1a9065a278d8ae0091753"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6936d0ca38dce9aaf4949d183dc741de0e6c9cad", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst"}, "hash": "74283eaec38ed26852c56d2573bf5f296fc7c407bdfe611f291b112c3a0e052d"}, "3": {"node_id": "615db3fb-c27d-429f-82fb-1327e872f740", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst", "text_hash": "1ad5cba656bc3fd377a4097fbacce9e3"}, "hash": "b0df750ca9806a57b8800d6339a0cd820b0b64b593f1146b6395f42d0e427232"}}, "hash": "aa29b26d38a7ec1d253c7ed888acc2a6ad4f8c4cc86e52f3c39af12997311802", "text": ".. _tune-guides:\n\n===========\nUser Guides\n===========\n\n.. tip:: We'd love to hear your feedback on using Tune - `get in touch <https://forms.gle/PTRvGLbKRdUfuzQo9>`_!\n\nIn this section, you can find material on how to use Tune and its various features.\nYou can follow our :ref:`Tune Feature Guides <tune-feature-guides>`, but can also  look into our\n:ref:`Practical Examples <tune-recipes>`, or go through some :ref:`Exercises <tune-exercises>` to get started.\n\n.. _tune-feature-guides:\n\nTune Feature Guides\n-------------------\n\n\n. grid:: 1 2 3 4\n    :gutter: 1\n    :class-container: container pb-3\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-run\n\n            Running Basic Experiments\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-output\n\n            Logging Tune Runs\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-resources\n\n            Setting Trial Resources\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-search-space-tutorial\n\n            Using Search Spaces\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-stopping\n\n            How to Define Stopping Criteria for a Ray Tune Experiment\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-trial-checkpoints\n\n            How to Save and Load Trial Checkpoints\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-storage\n\n            How to Configure Storage Options for a Distributed Tune Experiment\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "615db3fb-c27d-429f-82fb-1327e872f740": {"__data__": {"id_": "615db3fb-c27d-429f-82fb-1327e872f740", "embedding": null, "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst", "text_hash": "1ad5cba656bc3fd377a4097fbacce9e3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "6936d0ca38dce9aaf4949d183dc741de0e6c9cad", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst"}, "hash": "74283eaec38ed26852c56d2573bf5f296fc7c407bdfe611f291b112c3a0e052d"}, "2": {"node_id": "aa5f740c-f0f8-454e-aaee-b5cffa937825", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst", "text_hash": "fdd5461656d1a9065a278d8ae0091753"}, "hash": "aa29b26d38a7ec1d253c7ed888acc2a6ad4f8c4cc86e52f3c39af12997311802"}}, "hash": "b0df750ca9806a57b8800d6339a0cd820b0b64b593f1146b6395f42d0e427232", "text": "button-ref:: tune-fault-tolerance\n\n            How to Enable Fault Tolerance in Ray Tune\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-metrics\n\n            Using Callbacks and Metrics\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: ./tutorials/tune_get_data_in_and_out\n\n            Getting Data in and out of Tune\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: ./examples/tune_analyze_results\n\n            Analyzing Tune Experiment Results\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: ./examples/pbt_guide\n\n            A Guide to Population-Based Training\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-distributed\n\n            Deploying Tune in the Cloud\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-lifecycle\n\n            Tune Architecture\n\n    . grid-item-card::\n        :img-top: /images/tune.png\n        :class-img-top: pt-2 w-75 d-block mx-auto fixed-height-img\n\n        . button-ref:: tune-scalability\n\n            Scalability Benchmarks", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1e23d31a-3247-45b6-b5e6-4f0b41e413fc": {"__data__": {"id_": "1e23d31a-3247-45b6-b5e6-4f0b41e413fc", "embedding": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst", "text_hash": "008437fe140f18173812d5c120079c14"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95d4baa91a128128d64d956a0b6b98e03acadc5c", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst"}, "hash": "a0d8d6ba37b27386b2f2df5ca0282dd2ad746e49c7a75c735d671078fc55f602"}, "3": {"node_id": "17d5a6ba-25a6-4e09-ba6c-63413caa1db0", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst", "text_hash": "de87a4431745645f7d77fadb4f311455"}, "hash": "86e9cc57767d6994027c0124afb89f235e215f7003712e4042cdf93fbcde525e"}}, "hash": "bf1ae8ea39dc0a929c4ef6b736ac83dbb9108bd04d2faaf3dc57eb51f24002df", "text": ".. _tune-distributed-ref:\n\nRunning Distributed Experiments with Ray Tune\n==============================================\n\nTune is commonly used for large-scale distributed hyperparameter optimization. This page will overview how to setup and launch a distributed experiment along with :ref:`commonly used commands <tune-distributed-common>` for Tune when running distributed experiments.\n\n.. contents::\n    :local:\n    :backlinks: none\n\nSummary\n-------\n\nTo run a distributed experiment with Tune, you need to:\n\n1. First, :ref:`start a Ray cluster <cluster-index>` if you have not already.\n2. Run the script on the head node, or use :ref:`ray submit <ray-submit-doc>`, or use :ref:`Ray Job Submission <jobs-overview>`.\n\n.. tune-distributed-cloud:\n\nExample: Distributed Tune on AWS VMs\n------------------------------------\n\nFollow the instructions below to launch nodes on AWS (using the Deep Learning AMI). See the :ref:`cluster setup documentation <cluster-index>`. Save the below cluster configuration (``tune-default.yaml``):\n\n.. literalinclude:: /../../python/ray/tune/examples/tune-default.yaml\n   :language: yaml\n   :name: tune-default.yaml\n\n``ray up`` starts Ray on the cluster of nodes.\n\n.. code-block:: bash\n\n    ray up tune-default.yaml\n\n``ray submit --start`` starts a cluster as specified by the given cluster configuration YAML file, uploads ``tune_script.py`` to the cluster, and runs ``python tune_script.py [args]``.\n\n.. code-block:: bash\n\n    ray submit tune-default.yaml tune_script.py --start -- --ray-address=localhost:6379\n\n.. image:: /images/tune-upload.png\n    :scale: 50%\n    :align: center\n\nAnalyze your results on TensorBoard by starting TensorBoard on the remote head machine.\n\n.. code-block:: bash\n\n    # Go to http://localhost:6006 to access TensorBoard.\n    ray exec tune-default.yaml 'tensorboard --logdir=~/ray_results/ --port 6006' --port-forward 6006\n\n\nNote that you can customize the directory of results by specifying: ``air.RunConfig(storage_path=..)``, taken in by ``Tuner``. You can then point TensorBoard to that directory to visualize results. You can also use `awless <https://github.com/wallix/awless>`_ for easy cluster management on AWS.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "17d5a6ba-25a6-4e09-ba6c-63413caa1db0": {"__data__": {"id_": "17d5a6ba-25a6-4e09-ba6c-63413caa1db0", "embedding": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst", "text_hash": "de87a4431745645f7d77fadb4f311455"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "95d4baa91a128128d64d956a0b6b98e03acadc5c", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst"}, "hash": "a0d8d6ba37b27386b2f2df5ca0282dd2ad746e49c7a75c735d671078fc55f602"}, "2": {"node_id": "1e23d31a-3247-45b6-b5e6-4f0b41e413fc", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst", "text_hash": "008437fe140f18173812d5c120079c14"}, "hash": "bf1ae8ea39dc0a929c4ef6b736ac83dbb9108bd04d2faaf3dc57eb51f24002df"}, "3": {"node_id": "afe813b8-92c8-4eff-9547-c1a317f34bde", "node_type": null, "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst", "text_hash": "e63a8980c09785fdcea17549b2bcce8e"}, "hash": "73586854840e198c84906d5be3d7164a86160a065a82d4f3a76f3d9070b57d8a"}}, "hash": "86e9cc57767d6994027c0124afb89f235e215f7003712e4042cdf93fbcde525e", "text": "Running a Distributed Tune Experiment\n-------------------------------------\n\nRunning a distributed (multi-node) experiment requires Ray to be started already.\nYou can do this on local machines or on the cloud.\n\nAcross your machines, Tune will automatically detect the number of GPUs and CPUs without you needing to manage ``CUDA_VISIBLE_DEVICES``.\n\nTo execute a distributed experiment, call ``ray.init(address=XXX)`` before ``Tuner.fit()``, where ``XXX`` is the Ray address, which defaults to ``localhost:6379``. The Tune python script should be executed only on the head node of the Ray cluster.\n\nOne common approach to modifying an existing Tune experiment to go distributed is to set an ``argparse`` variable so that toggling between distributed and single-node is seamless.\n\n.. code-block:: python\n\n    import ray\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--address\")\n    args = parser.parse_args()\n    ray.init(address=args.address)\n\n    tuner = tune.Tuner(...)\n    tuner.fit()\n\n.. code-block:: bash\n\n    # On the head node, connect to an existing ray cluster\n    $ python tune_script.py --ray-address=localhost:XXXX\n\nIf you used a cluster configuration (starting a cluster with ``ray up`` or ``ray submit --start``), use:\n\n.. code-block:: bash\n\n    ray submit tune-default.yaml tune_script.py -- --ray-address=localhost:6379\n\n.. tip::\n\n    1. In the examples, the Ray address commonly used is ``localhost:6379``.\n    2. If the Ray cluster is already started, you should not need to run anything on the worker nodes.\n\n\nStorage Options in a Distributed Tune Run\n-----------------------------------------\n\nIn a distributed experiment, you should try to use :ref:`cloud checkpointing <tune-cloud-checkpointing>` to\nreduce synchronization overhead. For this, you just have to specify a remote ``storage_path`` in the\n:class:`air.RunConfig <ray.air.RunConfig>`.\n\n`my_trainable` is a user-defined :ref:`Tune Trainable <tune_60_seconds_trainables>` in the following example:\n\n.. code-block:: python\n\n    from ray import air, tune\n    from my_module import my_trainable\n\n    tuner = tune.Tuner(\n        my_trainable,\n        run_config=air.RunConfig(\n            name=\"experiment_name\",\n            storage_path=\"s3://bucket-name/sub-path/\",\n        )\n    )\n    tuner.fit()\n\nFor more details or customization, see our\n:ref:`guide on configuring storage in a distributed Tune experiment <tune-storage-options>`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"dde9f2ea5483a6d128e2e55f508cff97a6eda9a8": {"node_ids": ["61bb8b60-d910-44ef-bbf3-009a5b087f0d", "56cc558b-ca80-425a-8c55-6124bc66f823", "c82eb84a-5951-459a-8aa6-3821207e70fd", "6a357b7a-46f9-45c1-bd1d-c85bf353636c", "a295e8ba-aac0-403f-964c-60bd516443af", "ef5a6ff9-a4ab-42a2-841a-a02eaf1caa14"], "metadata": {"file_path": "doc/source/tune/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "c596ee609b9612fb3b6954377f58ec7b"}}, "6936d0ca38dce9aaf4949d183dc741de0e6c9cad": {"node_ids": ["aa5f740c-f0f8-454e-aaee-b5cffa937825", "615db3fb-c27d-429f-82fb-1327e872f740"], "metadata": {"file_path": "doc/source/tune/tutorials/overview.rst", "file_name": "overview.rst", "text_hash": "fdd5461656d1a9065a278d8ae0091753"}}, "95d4baa91a128128d64d956a0b6b98e03acadc5c": {"node_ids": ["1e23d31a-3247-45b6-b5e6-4f0b41e413fc", "17d5a6ba-25a6-4e09-ba6c-63413caa1db0"], "metadata": {"file_path": "doc/source/tune/tutorials/tune-distributed.rst", "file_name": "tune-distributed.rst", "text_hash": "008437fe140f18173812d5c120079c14"}}}, "docstore/metadata": {"61bb8b60-d910-44ef-bbf3-009a5b087f0d": {"doc_hash": "47cf3c5d503334e5c0324df9411fffa3f47e21b627b55544ce2e4db77ee5de17", "ref_doc_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8"}, "56cc558b-ca80-425a-8c55-6124bc66f823": {"doc_hash": "54da4738116b8c2cc321eaae1b21ee5d69179c3384af914f652388566d8d96f7", "ref_doc_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8"}, "c82eb84a-5951-459a-8aa6-3821207e70fd": {"doc_hash": "f36b60dd16e161b7c3995b9d78797aa401eab14977048a0a4a8c62758e9195da", "ref_doc_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8"}, "6a357b7a-46f9-45c1-bd1d-c85bf353636c": {"doc_hash": "92c0b10501addd9f99cb2f17a27784bc26a76b899c03effd6daaaa9e3705c909", "ref_doc_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8"}, "a295e8ba-aac0-403f-964c-60bd516443af": {"doc_hash": "ef79ee6a784631b4f00c7663114576c87f0c3930f1b2e7accbc0181f416e7883", "ref_doc_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8"}, "ef5a6ff9-a4ab-42a2-841a-a02eaf1caa14": {"doc_hash": "cabf0a0a67a19fa8535cef19aa249e1d45d9ae97f520755e74574e94c9cf4811", "ref_doc_id": "dde9f2ea5483a6d128e2e55f508cff97a6eda9a8"}, "aa5f740c-f0f8-454e-aaee-b5cffa937825": {"doc_hash": "aa29b26d38a7ec1d253c7ed888acc2a6ad4f8c4cc86e52f3c39af12997311802", "ref_doc_id": "6936d0ca38dce9aaf4949d183dc741de0e6c9cad"}, "615db3fb-c27d-429f-82fb-1327e872f740": {"doc_hash": "b0df750ca9806a57b8800d6339a0cd820b0b64b593f1146b6395f42d0e427232", "ref_doc_id": "6936d0ca38dce9aaf4949d183dc741de0e6c9cad"}, "1e23d31a-3247-45b6-b5e6-4f0b41e413fc": {"doc_hash": "bf1ae8ea39dc0a929c4ef6b736ac83dbb9108bd04d2faaf3dc57eb51f24002df", "ref_doc_id": "95d4baa91a128128d64d956a0b6b98e03acadc5c"}, "17d5a6ba-25a6-4e09-ba6c-63413caa1db0": {"doc_hash": "86e9cc57767d6994027c0124afb89f235e215f7003712e4042cdf93fbcde525e", "ref_doc_id": "95d4baa91a128128d64d956a0b6b98e03acadc5c"}}}