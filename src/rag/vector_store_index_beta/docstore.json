{"docstore/data": {"cf7447bd-c777-490d-a105-15d791d5cb49": {"__data__": {"id_": "cf7447bd-c777-490d-a105-15d791d5cb49", "embedding": null, "metadata": {"file_path": "doc/source/cluster/cli.rst", "file_name": "cli.rst", "text_hash": "4a09c37c744b7b3c032dbceea812a319"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "619f0028e26d2ca0229c368ae863b591d1ae0f1e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/cli.rst", "file_name": "cli.rst"}, "hash": "18c69eb8585841a03320748281e22e39e89deb803d6455a5713d6bde17643203"}}, "hash": "18c69eb8585841a03320748281e22e39e89deb803d6455a5713d6bde17643203", "text": ".. _ray-cluster-cli:\n\nCluster Management CLI\n======================\nThis section contains commands for managing Ray clusters.\n\n.. _ray-start-doc:\n\n.. click:: ray.scripts.scripts:start\n   :prog: ray start\n   :show-nested:\n\n.. _ray-stop-doc:\n\n.. click:: ray.scripts.scripts:stop\n   :prog: ray stop\n   :show-nested:\n\n.. _ray-up-doc:\n\n.. click:: ray.scripts.scripts:up\n   :prog: ray up\n   :show-nested:\n\n.. _ray-down-doc:\n\n.. click:: ray.scripts.scripts:down\n   :prog: ray down\n   :show-nested:\n\n.. _ray-exec-doc:\n\n.. click:: ray.scripts.scripts:exec\n   :prog: ray exec\n   :show-nested:\n\n.. _ray-submit-doc:\n\n.. click:: ray.scripts.scripts:submit\n   :prog: ray submit\n   :show-nested:\n\n.. _ray-attach-doc:\n\n.. click:: ray.scripts.scripts:attach\n   :prog: ray attach\n   :show-nested:\n\n.. _ray-get_head_ip-doc:\n\n.. click:: ray.scripts.scripts:get_head_ip\n   :prog: ray get_head_ip\n   :show-nested:\n\n.. _ray-monitor-doc:\n\n.. click:: ray.scripts.scripts:monitor\n   :prog: ray monitor\n   :show-nested:", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ba92b10a-578b-445b-bb26-b66260508667": {"__data__": {"id_": "ba92b10a-578b-445b-bb26-b66260508667", "embedding": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "ca1d2f3171e8d84e084f59b1d27cbb7a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md"}, "hash": "8f87d806db97b9c619436045a2d4f2f9750c44b64e2d83fac18705c106561b62"}, "3": {"node_id": "3953411a-b66d-4891-a4e2-f3ff12be661e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "34fb4d19706bcf494b027e9b3c726ee4"}, "hash": "2b2049e9e18d261863df04f5b83df3e4d988d8f1fbb3671eb35e0969d63cafbe"}}, "hash": "4c5fea293a438a2d99f8231dd117e1945ac8106d1ae3904da89534bd627f52e2", "text": "(observability-configure-manage-dashboard)=\n# Configuring and Managing Ray Dashboard\n{ref}`Ray Dashboard<observability-getting-started>` is one of the most important tools to monitor and debug Ray applications and Clusters. This page describes how to configure Ray Dashboard on your Clusters.\n\nDashboard configurations may differ depending on how you launch Ray Clusters (e.g., local Ray Cluster v.s. KubeRay). Integrations with Prometheus and Grafana are optional for enhanced Dashboard experience.\n\n:::{note}\nRay Dashboard is only intended for interactive development and debugging because the Dashboard UI and the underlying data are not accessible after Clusters are terminated. For production monitoring and debugging, users should rely on [persisted logs](../cluster/kubernetes/user-guides/logging.md), [persisted metrics](./metrics.md), [persisted Ray states](../ray-observability/user-guides/cli-sdk.rst), and other observability tools.\n:::\n\n## Changing the Ray Dashboard port\nRay Dashboard runs on port `8265` of the head node. Follow the instructions below to customize the port if needed.\n\n::::{tab-set}\n\n:::{tab-item} Single-node local cluster\n**Start the cluster explicitly with CLI** <br/>\nPass the ``--dashboard-port`` argument with ``ray start`` in the command line.\n\n**Start the cluster implicitly with `ray.init`** <br/>\nPass the keyword argument ``dashboard_port`` in your call to ``ray.init()``.\n:::\n\n:::{tab-item} VM Cluster Launcher\nInclude the ``--dashboard-port`` argument in the `head_start_ray_commands` section of the [Cluster Launcher's YAML file](https://github.com/ray-project/ray/blob/0574620d454952556fa1befc7694353d68c72049/python/ray/autoscaler/aws/example-full.yaml#L172).\n```yaml\nhead_start_ray_commands: \n  - ray stop \n  # Replace ${YOUR_PORT} with the port number you need.\n  - ulimit -n 65536; ray start --head --dashboard-port=${YOUR_PORT} --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml \n\n```\n:::\n\n:::{tab-item} KubeRay\nView the [specifying non-default ports](https://docs.ray.io/en/latest/cluster/kubernetes/user-guides/config.html#specifying-non-default-ports) page for details.\n:::\n\n::::\n\n(dashboard-in-browser)=\n## Viewing Ray Dashboard in browsers\nWhen you start a single-node Ray cluster on your laptop, you can access the dashboard through a URL printed when Ray is initialized (the default URL is `http://localhost:8265`).\n\n\nWhen you start a remote Ray cluster with the {ref}`VM cluster launcher <vm-cluster-quick-start>`, {ref}`KubeRay operator <kuberay-quickstart>`, or manual configuration, the Ray Dashboard launches on the head node but the dashboard port may not be publicly exposed.You need an additional setup to access the Ray Dashboard from outside the head node.:::{danger}\nFor security purpose, do not expose Ray Dashboard publicly without proper authentication in place.:::\n\n::::{tab-set}\n\n:::{tab-item} VM Cluster Launcher\n**Port forwarding** <br/>\nYou can securely port-forward local traffic to the dashboard with the ``ray\ndashboard`` command.```shell\n$ ray dashboard [-p <port, 8265 by default>] <cluster config file>\n```\n\nThe dashboard is now visible at ``http://localhost:8265``.:::\n\n:::{tab-item} KubeRay\n\nThe KubeRay operator makes Dashboard available via a Service targeting the Ray head pod, named ``<RayCluster name>-head-svc``.Access\nDashboard from within the Kubernetes cluster at ``http://<RayCluster name>-head-svc:8265``.There are two ways to expose Dashboard outside the Cluster:\n\n**1.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3953411a-b66d-4891-a4e2-f3ff12be661e": {"__data__": {"id_": "3953411a-b66d-4891-a4e2-f3ff12be661e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "34fb4d19706bcf494b027e9b3c726ee4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md"}, "hash": "8f87d806db97b9c619436045a2d4f2f9750c44b64e2d83fac18705c106561b62"}, "2": {"node_id": "ba92b10a-578b-445b-bb26-b66260508667", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "ca1d2f3171e8d84e084f59b1d27cbb7a"}, "hash": "4c5fea293a438a2d99f8231dd117e1945ac8106d1ae3904da89534bd627f52e2"}, "3": {"node_id": "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "d95a9ae7cd4d3bade8017fd0c24d1a54"}, "hash": "3323e5baa911286f2b03d7c49f6a1376e173aedd1008a8a8554c7f2470731a78"}}, "hash": "2b2049e9e18d261863df04f5b83df3e4d988d8f1fbb3671eb35e0969d63cafbe", "text": "Setting up ingress** <br/>\nFollow the [instructions](kuberay-ingress) to set up ingress to access Ray Dashboard.**2.Port forwarding** <br/>\nYou can also view the dashboard from outside the Kubernetes cluster by using port-forwarding:\n\n```shell\n$ kubectl port-forward --address 0.0.0.0 service/${RAYCLUSTER_NAME}-head-svc 8265:8265 \n# Visit ${YOUR_IP}:8265 for the Dashboard (e.g.127.0.0.1:8265 or ${YOUR_VM_IP}:8265)\n```\n\n```{admonition} Note\n:class: note\nDo not use port forwarding for production environment.Follow the instructions above to expose the Dashboard with Ingress.```\n\nFor more information about configuring network access to a Ray cluster on Kubernetes, see the {ref}`networking notes <kuberay-networking>`.:::\n\n::::\n\n## Running behind a reverse proxy\n\nRay Dashboard should work out-of-the-box when accessed via a reverse proxy.API requests don't need to be proxied individually.Always access the dashboard with a trailing ``/`` at the end of the URL.For example, if your proxy is set up to handle requests to ``/ray/dashboard``, view the dashboard at ``www.my-website.com/ray/dashboard/``.The dashboard sends HTTP requests with relative URL paths.Browsers handle these requests as expected when the ``window.location.href`` ends in a trailing ``/``.This is a peculiarity of how many browsers handle requests with relative URLs, despite what [MDN](https://developer.mozilla.org/en-US/docs/Learn/Common_questions/What_is_a_URL#examples_of_relative_urls) defines as the expected behavior.Make your dashboard visible without a trailing ``/`` by including a rule in your reverse proxy that redirects the user's browser to ``/``, i.e.``/ray/dashboard`` --> ``/ray/dashboard/``.Below is an example with a [traefik](https://doc.traefik.io/traefik/getting-started/quick-start/) TOML file that accomplishes this:\n\n```yaml\n[http]\n  [http.routers]\n    [http.routers.to-dashboard]\n      rule = \"PathPrefix(`/ray/dashboard`)\"\n      middlewares = [\"test-redirectregex\", \"strip\"]\n      service = \"dashboard\"\n  [http.middlewares]\n    [http.middlewares.test-redirectregex.redirectRegex]\n      regex = \"^(.*)/ray/dashboard$\"\n      replacement = \"${1}/ray/dashboard/\"\n    [http.middlewares.strip.stripPrefix]\n      prefixes = [\"/ray/dashboard\"]\n  [http.services]\n    [http.services.dashboard.loadBalancer]\n      [[http.services.dashboard.loadBalancer.servers]]\n        url = \"http://localhost:8265\"\n```\n\n## Disabling the Dashboard\n\nDashboard is included if you use `ray[default]`, `ray[air]`, or {ref}`other installation commands <installation>` and automatically started.To disable Dashboard, use the following arguments `--include-dashboard`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984": {"__data__": {"id_": "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984", "embedding": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "d95a9ae7cd4d3bade8017fd0c24d1a54"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md"}, "hash": "8f87d806db97b9c619436045a2d4f2f9750c44b64e2d83fac18705c106561b62"}, "2": {"node_id": "3953411a-b66d-4891-a4e2-f3ff12be661e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "34fb4d19706bcf494b027e9b3c726ee4"}, "hash": "2b2049e9e18d261863df04f5b83df3e4d988d8f1fbb3671eb35e0969d63cafbe"}, "3": {"node_id": "a2d5ebeb-0b87-4056-861f-eadb30cac546", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "652f4cdb3659199dab424e508ca8b31d"}, "hash": "f605728e3645caefca616b86ce9dd0d8f4a763d402038975b32be7d6253e0ba8"}}, "hash": "3323e5baa911286f2b03d7c49f6a1376e173aedd1008a8a8554c7f2470731a78", "text": "::::{tab-set}\n\n:::{tab-item} Single-node local cluster\n\n**Start the cluster explicitly with CLI** <br/>\n\n```bash\nray start --include-dashboard=False\n```\n\n**Start the cluster implicitly with `ray.init`** <br/>\n\n```{testcode}\n:hide:\n\nimport ray\nray.shutdown()\n```\n\n```{testcode}\nimport ray\nray.init(include_dashboard=False)\n```\n\n:::\n\n:::{tab-item} VM Cluster Launcher\nInclude the `ray start --head --include-dashboard=False` argument\nin the `head_start_ray_commands` section of the [Cluster Launcher's YAML file](https://github.com/ray-project/ray/blob/0574620d454952556fa1befc7694353d68c72049/python/ray/autoscaler/aws/example-full.yaml#L172).:::\n\n:::{tab-item} KubeRay\n\n```{admonition} Warning\n:class: warning\nIt's not recommended to disable Dashboard because several KubeRay features like `RayJob` and `RayService` depend on it.```\n\nSet `spec.headGroupSpec.rayStartParams.include-dashboard` to `False`.Check out this [example YAML file](https://gist.github.com/kevin85421/0e6a8dd02c056704327d949b9ec96ef9).:::\n::::\n\n\n(observability-visualization-setup)=\n## Embed Grafana visualizations into Ray Dashboard\n\nFor the enhanced Ray Dashboard experience, like {ref}`viewing time-series metrics<dash-metrics-view>` together with logs, Job info, etc., set up Prometheus and Grafana and integrate them with Ray Dashboard.\n\n### Setting up Prometheus\nTo render Grafana visualizations, you need Prometheus to scrape metrics from Ray Clusters. Follow {ref}`the instructions <prometheus-setup>` to set up your Prometheus server and start to scrape system and application metrics from Ray Clusters.\n\n\n### Setting up Grafana\nGrafana is a tool that supports advanced visualizations of Prometheus metrics and allows you to create custom dashboards with your favorite metrics. Follow {ref}`the instructions <grafana>` to set up Grafana.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a2d5ebeb-0b87-4056-861f-eadb30cac546": {"__data__": {"id_": "a2d5ebeb-0b87-4056-861f-eadb30cac546", "embedding": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "652f4cdb3659199dab424e508ca8b31d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md"}, "hash": "8f87d806db97b9c619436045a2d4f2f9750c44b64e2d83fac18705c106561b62"}, "2": {"node_id": "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "d95a9ae7cd4d3bade8017fd0c24d1a54"}, "hash": "3323e5baa911286f2b03d7c49f6a1376e173aedd1008a8a8554c7f2470731a78"}, "3": {"node_id": "a8d93d42-4550-4559-a0c9-e2c5a1bbd9ef", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "67cae496d699d7cb5604047c78030f34"}, "hash": "b20ceb61f17ee4327b0d761108a561ad9900886376dd984f3365093e97069938"}}, "hash": "f605728e3645caefca616b86ce9dd0d8f4a763d402038975b32be7d6253e0ba8", "text": "(embed-grafana-in-dashboard)=\n### Embedding Grafana visualizations into Ray Dashboard\nTo view embedded time-series visualizations in Ray Dashboard, the following must be set up:\n\n1. The head node of the cluster is able to access Prometheus and Grafana.\n2. The browser of the dashboard user is able to access Grafana. \n\nConfigure these settings using the `RAY_GRAFANA_HOST`, `RAY_PROMETHEUS_HOST`, `RAY_PROMETHEUS_NAME`, and `RAY_GRAFANA_IFRAME_HOST` environment variables when you start the Ray Clusters.\n\n* Set `RAY_GRAFANA_HOST` to an address that the head node can use to access Grafana. Head node does health checks on Grafana on the backend.\n* Set `RAY_PROMETHEUS_HOST` to an address the head node can use to access Prometheus.\n* Set `RAY_PROMETHEUS_NAME` to select a different data source to use for the Grafana dashboard panles to use. Default is \"Prometheus\".\n* Set `RAY_GRAFANA_IFRAME_HOST` to an address that the user's browsers can use to access Grafana and embed visualizations. If `RAY_GRAFANA_IFRAME_HOST` is not set, Ray Dashboard uses the value of `RAY_GRAFANA_HOST`.\n\nFor example, if the IP of the head node is 55.66.77.88 and Grafana is hosted on port 3000. Set the value to `RAY_GRAFANA_HOST=http://55.66.77.88:3000`.\n\nIf all the environment variables are set properly, you should see time-series metrics in {ref}`Ray Dashboard <observability-getting-started>`.\n\n:::{note}\nIf you use a different Prometheus server for each Ray Cluster and use the same Grafana server for all Clusters, set the `RAY_PROMETHEUS_NAME` environment variable to different values for each Ray Cluster and add these datasources in Grafana. Follow {ref}`these instructions <grafana>` to set up Grafana.\n:::\n\n#### Alternate Prometheus host location\nBy default, Ray Dashboard assumes Prometheus is hosted at `localhost:9090`. You can choose to run Prometheus on a non-default port or on a different machine. In this case, make sure that Prometheus can scrape the metrics from your Ray nodes following instructions {ref}`here <scrape-metrics>`.\n\nThen, configure `RAY_PROMETHEUS_HOST` environment variable properly as stated above. For example, if Prometheus is hosted at port 9000 on a node with ip 55.66.77.88, set `RAY_PROMETHEUS_HOST=http://55.66.77.88:9000`.\n\n\n#### Alternate Grafana host location\nBy default, Ray Dashboard assumes Grafana is hosted at `localhost:3000` You can choose to run Grafana on a non-default port or on a different machine as long as the head node and the browsers of dashboard users can access it.\n\nIf Grafana is exposed with NGINX ingress on a Kubernetes cluster, the following line should be present in the Grafana ingress annotation:\n\n```yaml\nnginx.ingress.kubernetes.io/configuration-snippet: |\n    add_header X-Frame-Options SAMEORIGIN always;\n```\n\nWhen both Grafana and the Ray Cluster are on the same Kubernetes cluster, set `RAY_GRAFANA_HOST` to the external URL of the Grafana ingress.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a8d93d42-4550-4559-a0c9-e2c5a1bbd9ef": {"__data__": {"id_": "a8d93d42-4550-4559-a0c9-e2c5a1bbd9ef", "embedding": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "67cae496d699d7cb5604047c78030f34"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md"}, "hash": "8f87d806db97b9c619436045a2d4f2f9750c44b64e2d83fac18705c106561b62"}, "2": {"node_id": "a2d5ebeb-0b87-4056-861f-eadb30cac546", "node_type": null, "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "652f4cdb3659199dab424e508ca8b31d"}, "hash": "f605728e3645caefca616b86ce9dd0d8f4a763d402038975b32be7d6253e0ba8"}}, "hash": "b20ceb61f17ee4327b0d761108a561ad9900886376dd984f3365093e97069938", "text": "#### User authentication for Grafana\nWhen the Grafana instance requires user authentication, the following settings have to be in its `configuration file <https://grafana.com/docs/grafana/latest/setup-grafana/configure-grafana/>`_ to correctly embed in Ray Dashboard:\n\n```ini\n  [security]\n  allow_embedding = true\n  cookie_secure = true\n  cookie_samesite = none\n```\n\n#### Troubleshooting\n\n##### Grafana dashboards are not embedded in the Ray dashboard\nIf you're getting an error that says `RAY_GRAFANA_HOST` is not setup despite having set it up, check that:\n* You've included the protocol in the URL (e.g., `http://your-grafana-url.com` instead of `your-grafana-url.com`).\n* The URL doesn't have a trailing slash (e.g., `http://your-grafana-url.com` instead of `http://your-grafana-url.com/`).\n\n##### Certificate Authority (CA error)\nYou may see a CA error if your Grafana instance is hosted behind HTTPS. Contact the Grafana service owner to properly enable HTTPS traffic.\n\n\n## Viewing built-in Dashboard API metrics\n\nDashboard is powered by a server that serves both the UI code and the data about the cluster via API endpoints.\nRay emits basic Prometheus metrics for each API endpoint:\n\n`ray_dashboard_api_requests_count_requests_total`: Collects the total count of requests. This is tagged by endpoint, method, and http_status.\n\n`ray_dashboard_api_requests_duration_seconds_bucket`: Collects the duration of requests. This is tagged by endpoint and method.\n\nFor example, you can view the p95 duration of all requests with this query:\n\n```text\n\nhistogram_quantile(0.95, sum(rate(ray_dashboard_api_requests_duration_seconds_bucket[5m])) by (le))\n```\n\nYou can query these metrics from the Prometheus or Grafana UI. Find instructions above for how to set these tools up.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e688aa7d-7a01-4d26-a0f8-53ab714d6ee8": {"__data__": {"id_": "e688aa7d-7a01-4d26-a0f8-53ab714d6ee8", "embedding": null, "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst", "text_hash": "e4bf36472cb68ab2e04d6f72710b5b65"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a44cc38177f171d99f64433945351c51348a6412", "node_type": null, "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst"}, "hash": "e186fac699b5af3b218e5da9ea4d2b4af6ed0fe17ec95591f0d2612cb6e90e84"}, "3": {"node_id": "863aac4d-a07a-451b-8fdc-d12ad2d43134", "node_type": null, "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst", "text_hash": "712ceff9c2499a5469f61e94e1058d54"}, "hash": "c9f10dba061096ef2f6ba0ac4a75efda5e1a61de2dfcdd9f38f3ab3a580bfb29"}}, "hash": "c1daf4f228cb15c8e2493b8dc0fded098a1dc3ebc573fa2e868cb73e9aab9f6a", "text": ".. _cluster-FAQ:\n\n===\nFAQ\n===\n\nThese are some Frequently Asked Questions that we've seen pop up for using Ray clusters.\nIf you still have questions after reading this FAQ,  please reach out on\n`our Discourse <https://discuss.ray.io/>`__!\n\nDo Ray clusters support multi-tenancy?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYes, you can run multiple :ref:`jobs <jobs-overview>` from different users simultaneously in a Ray cluster\nbut it's NOT recommended in production.\nThe reason is that Ray currently still misses some features for multi-tenancy in production:\n\n* Ray doesn't provide strong resource isolation:\n  Ray :ref:`resources <core-resources>` are logical and they don't limit the physical resources a task or actor can use while running.\n  This means simultaneous jobs can interfere with each other and makes them less reliable to run in production.\n\n* Ray doesn't support priorities: All jobs, tasks and actors have the same priority so there is no way to prioritize important jobs under load.\n\n* Ray doesn't support access control: jobs have full access to a Ray cluster and all of the resources within it.\n\nOn the other hand, you can run the same job multiple times using the same cluster to save the cluster startup time.\n\n.. note::\n    A Ray :ref:`namespace <namespaces-guide>` is just a logical grouping of jobs and named actors. Unlike a Kubernetes namespace, it doesn't provide any other multi-tenancy functions like resource quotas.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "863aac4d-a07a-451b-8fdc-d12ad2d43134": {"__data__": {"id_": "863aac4d-a07a-451b-8fdc-d12ad2d43134", "embedding": null, "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst", "text_hash": "712ceff9c2499a5469f61e94e1058d54"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a44cc38177f171d99f64433945351c51348a6412", "node_type": null, "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst"}, "hash": "e186fac699b5af3b218e5da9ea4d2b4af6ed0fe17ec95591f0d2612cb6e90e84"}, "2": {"node_id": "e688aa7d-7a01-4d26-a0f8-53ab714d6ee8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst", "text_hash": "e4bf36472cb68ab2e04d6f72710b5b65"}, "hash": "c1daf4f228cb15c8e2493b8dc0fded098a1dc3ebc573fa2e868cb73e9aab9f6a"}}, "hash": "c9f10dba061096ef2f6ba0ac4a75efda5e1a61de2dfcdd9f38f3ab3a580bfb29", "text": "I have multiple Ray users. What's the right way to deploy Ray for them?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIt's recommended to start a Ray cluster for each user so that their workloads are isolated.\n\nWhat is the difference between ``--node-ip-address`` and ``--address``?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nWhen starting a head node on a machine with more than one network address, you\nmay need to specify the externally-available address so worker nodes can\nconnect. This is done with:\n\n.. code:: bash\n\n    ray start --head --node-ip-address xx.xx.xx.xx --port nnnn``\n\nThen when starting the worker node, use this command to connect to the head node:\n\n.. code:: bash\n\n    ray start --address xx.xx.xx.xx:nnnn\n\nWhat does a worker node failure to connect look like?\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf the worker node cannot connect to the head node, you should see this error\n\n    Unable to connect to GCS at xx.xx.xx.xx:nnnn. Check that (1) Ray GCS with\n    matching version started successfully at the specified address, and (2)\n    there is no firewall setting preventing access.\n\nThe most likely cause is that the worker node cannot access the IP address\ngiven. You can use ``ip route get xx.xx.xx.xx`` on the worker node to start\ndebugging routing issues.\n\nYou may also see failures in the log like\n\n    This node has an IP address of xx.xx.xx.xx, while we can not found the\n    matched Raylet address. This maybe come from when you connect the Ray\n    cluster with a different IP address or connect a container.\n\nwhich can be caused by overloading the head node with too many simultaneous\nconnections. The solution for this is to start the worker nodes more slowly.\n\nI am having problems getting my SLURM cluster to work\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThere seem to be a class of issues starting ray on SLURM clusters.  While we\nhave not been able to pin down the exact causes (as of June 2023), work has\nbeen done to mitigate some of the resource contention. Some of the issues\nreported:\n\n* Using a machine with a large number of CPUs, and starting one worker per CPU\n  together with OpenBLAS (as used in NumPy) may allocate too many threads. This\n  is an `known OpenBLAS limitation`_ and can be mitigated by limiting OpenBLAS\n  to one thread per process as explained in the link.\n\n* Resource allocation is not what was expected: usually too many CPUs per node\n  were allocated. Best practice is to verify your SLURM configuration without\n  starting ray to verify that the allocations are as expected. For more\n  detailed information see :ref:`ray-slurm-deploy`.\n\n.. _`known OpenBLAS limitation`: https://github.com/xianyi/OpenBLAS/wiki/faq#how-can-i-use-openblas-in-multi-threaded-applications", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae00c381-47aa-408e-a304-9bc55be4ce1c": {"__data__": {"id_": "ae00c381-47aa-408e-a304-9bc55be4ce1c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "e246227ff25d5515d65348f76823ce3f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d86c419db93279c5f8e539528563b29a3ef9cad5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "b5332bb11a24ee5e98f46aefc470cf2a5f61ce9abeeb8b81d423424b222d8e9c"}, "3": {"node_id": "eb7fded7-c10b-47bb-b1db-518895993fee", "node_type": null, "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "07f41dba9db2a09bdda655412d7ac682"}, "hash": "217c51bd6e9d89bc613e581c3cc70ccad4a567332426e2ff6f3ae793a16f4207"}}, "hash": "3706e9260949cc82436e23f9f34c564f92088061a6876ae172ae09ec96b8a653", "text": ".. _cluster-index:\n\nRay Clusters Overview\n=====================\n\nRay enables seamless scaling of workloads from a laptop to a large cluster.While Ray\nworks out of the box on single machines with just a call to ``ray.init``, to run Ray\napplications on multiple nodes you must first *deploy a Ray cluster*.A Ray cluster is a set of worker nodes connected to a common :ref:`Ray head node <cluster-head-node>`.Ray clusters can be fixed-size, or they may :ref:`autoscale up and down <cluster-autoscaler>` according\nto the resources requested by applications running on the cluster.Where can I deploy Ray clusters?--------------------------------\n\nRay provides native cluster deployment support on the following technology stacks:\n\n* On :ref:`AWS and GCP <cloud-vm-index>`.Community-supported Azure, Aliyun and vSphere integrations also exist.* On :ref:`Kubernetes <kuberay-index>`, via the officially supported KubeRay project.Advanced users may want to :ref:`deploy Ray manually <on-prem>`\nor onto :ref:`platforms not listed here <ref-cluster-setup>`... note::\n\n    Multi-node Ray clusters are only supported on Linux.At your own risk, you\n    may deploy Windows and OSX clusters by setting the environment variable\n    ``RAY_ENABLE_WINDOWS_OR_OSX_CLUSTER=1`` during deployment.What's next?------------\n\n.. grid:: 1 2 2 2\n    :gutter: 1\n    :class-container: container pb-3\n\n    .. grid-item-card::\n\n        **I want to learn key Ray cluster concepts**\n        ^^^\n        Understand the key concepts and main ways of interacting with a Ray cluster.+++\n        .. button-ref:: cluster-key-concepts\n            :color: primary\n            :outline:\n            :expand:\n\n            Learn Key Concepts\n\n    .. grid-item-card::\n\n        **I want to run Ray on Kubernetes**\n        ^^^\n        Deploy a Ray application to a Kubernetes cluster.You can run the tutorial on a\n        Kubernetes cluster or on your laptop via Kind.+++\n        .. button-ref:: kuberay-quickstart\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray on Kubernetes\n\n    .. grid-item-card::\n\n        **I want to run Ray on a cloud provider**\n        ^^^\n        Take a sample application designed to run on a laptop and scale it up in the\n        cloud.Access to an AWS or GCP account is required.+++\n        .. button-ref:: vm-cluster-quick-start\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray on VMs\n\n    .. grid-item-card::\n\n        **I want to run my application on an existing Ray cluster**\n        ^^^\n        Guide to submitting applications as Jobs to existing Ray clusters.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "eb7fded7-c10b-47bb-b1db-518895993fee": {"__data__": {"id_": "eb7fded7-c10b-47bb-b1db-518895993fee", "embedding": null, "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "07f41dba9db2a09bdda655412d7ac682"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d86c419db93279c5f8e539528563b29a3ef9cad5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "b5332bb11a24ee5e98f46aefc470cf2a5f61ce9abeeb8b81d423424b222d8e9c"}, "2": {"node_id": "ae00c381-47aa-408e-a304-9bc55be4ce1c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "e246227ff25d5515d65348f76823ce3f"}, "hash": "3706e9260949cc82436e23f9f34c564f92088061a6876ae172ae09ec96b8a653"}}, "hash": "217c51bd6e9d89bc613e581c3cc70ccad4a567332426e2ff6f3ae793a16f4207", "text": "+++\n        .. button-ref:: jobs-quickstart\n            :color: primary\n            :outline:\n            :expand:\n\n            Job Submission\n\n.. include:: /_includes/clusters/announcement_bottom.rst", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ca3ee395-69e9-4f14-b779-437199ce23d1": {"__data__": {"id_": "ca3ee395-69e9-4f14-b779-437199ce23d1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "6c8bcff56cfd7524c8ee83ce40e56f5e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "09b54b0a2a9f4cf455148263384a536fd1c639e0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/key-concepts.rst", "file_name": "key-concepts.rst"}, "hash": "5ef32bb3be31fe0695940c20ffd64e6205d1aeaa474acdbe0242af623a09d89b"}}, "hash": "5ef32bb3be31fe0695940c20ffd64e6205d1aeaa474acdbe0242af623a09d89b", "text": "Key Concepts\n============\n\n.. _cluster-key-concepts:\n\nThis page introduces key concepts for Ray clusters:\n\n.. contents::\n    :local:\n\nRay Cluster\n-----------\nA Ray cluster consists of a single :ref:`head node <cluster-head-node>`\nand any number of connected :ref:`worker nodes <cluster-worker-nodes>`:\n\n.. figure:: images/ray-cluster.svg\n    :align: center\n    :width: 600px\n\n    *A Ray cluster with two worker nodes. Each node runs Ray helper processes to\n    facilitate distributed scheduling and memory management. The head node runs\n    additional control processes (highlighted in blue).*\n\nThe number of worker nodes may be *autoscaled* with application demand as specified\nby your Ray cluster configuration. The head node runs the :ref:`autoscaler <cluster-autoscaler>`.\n\n.. note::\n    Ray nodes are implemented as pods when :ref:`running on Kubernetes <kuberay-index>`.\n\nUsers can submit jobs for execution on the Ray cluster, or can interactively use the\ncluster by connecting to the head node and running `ray.init`. See\n:ref:`Ray Jobs <jobs-quickstart>` for more information.\n\n.. _cluster-head-node:\n\nHead Node\n---------\nEvery Ray cluster has one node which is designated as the *head node* of the cluster.\nThe head node is identical to other worker nodes, except that it also runs singleton processes responsible for cluster management such as the\n:ref:`autoscaler <cluster-autoscaler>`, :term:`GCS <GCS / Global Control Service>` and the Ray driver processes\n:ref:`which run Ray jobs <cluster-clients-and-jobs>`. Ray may schedule\ntasks and actors on the head node just like any other worker node, which is not desired in large-scale clusters.\nSee :ref:`vms-large-cluster-configure-head-node` for the best practice in large-scale clusters.\n\n.. _cluster-worker-nodes:\n\nWorker Node\n------------\n*Worker nodes* do not run any head node management processes, and serve only to run user code in Ray tasks and actors. They participate in distributed scheduling, as well as the storage and distribution of Ray objects in :ref:`cluster memory <memory>`.\n\n.. _cluster-autoscaler:\n\nAutoscaling\n-----------\n\nThe *Ray autoscaler* is a process that runs on the :ref:`head node <cluster-head-node>` (or as a sidecar container in the head pod if :ref:`using Kubernetes <kuberay-index>`).\nWhen the resource demands of the Ray workload exceed the\ncurrent capacity of the cluster, the autoscaler will try to increase the number of worker nodes. When worker nodes\nsit idle, the autoscaler will remove worker nodes from the cluster.\n\nIt is important to understand that the autoscaler only reacts to task and actor resource requests, and not application metrics or physical resource utilization.\nTo learn more about autoscaling, refer to the user guides for Ray clusters on :ref:`VMs <cloud-vm-index>` and :ref:`Kubernetes <kuberay-index>`.\n\n\n.. _cluster-clients-and-jobs:\n\nRay Jobs\n--------\n\nA Ray job is a single application: it is the collection of Ray tasks, objects, and actors that originate from the same script.\nThe worker that runs the Python script is known as the *driver* of the job.\n\nThere are three ways to run a Ray job on a Ray cluster:\n\n1. (Recommended) Submit the job using the :ref:`Ray Jobs API <jobs-overview>`.\n2. Run the driver script directly on any node of the Ray cluster, for interactive development.\n3. (For Experts only) Use :ref:`Ray Client <ray-client-ref>` to connect remotely to the cluster within a driver script.\n\nFor details on these workflows, refer to the :ref:`Ray Jobs API guide <jobs-overview>`.\n\n.. figure:: images/ray-job-diagram.svg\n    :align: center\n    :width: 650px\n\n    *Three ways of running a job on a Ray cluster.*", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1fe0ce81-f9ec-47b0-93fb-c3780961a18e": {"__data__": {"id_": "1fe0ce81-f9ec-47b0-93fb-c3780961a18e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks.md", "file_name": "benchmarks.md", "text_hash": "26c34ed09dffbd56cd5a93a244fb896c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0bf5221971f6b8b5df8ae3196a2172ed0d59bcef", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks.md", "file_name": "benchmarks.md"}, "hash": "d0b4203bddb2478e39f085fbaf574d7a32b14caa626c699b93bc48f1dd1033f8"}}, "hash": "d0b4203bddb2478e39f085fbaf574d7a32b14caa626c699b93bc48f1dd1033f8", "text": "(kuberay-benchmarks)=\n\n# KubeRay Benchmarks\n\n- {ref}`kuberay-mem-scalability`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "254f4b24-2fae-416c-8335-602e73835fce": {"__data__": {"id_": "254f4b24-2fae-416c-8335-602e73835fce", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md", "text_hash": "bd95ae63529c2bbde5ae05c3aeb18559"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8a469417f56973e8e700003f5a35e61cf96beab", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md"}, "hash": "6845751418aec1a36f1342aa40ef7e122a1a4c497c3a94e77381b4e5f2530ef4"}, "3": {"node_id": "672ee971-7834-49fb-a0a0-128ec84ed8d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md", "text_hash": "1a7b4596182177134471609cbfe80df9"}, "hash": "61c7f6964e94dff9711d60b0f3b5b9b4d128ad60ba5858f8fb6b863328cfe986"}}, "hash": "500718868f473af384e2f7aca857863920b43c2772b6b4319e7dcf5fece187ce", "text": "(kuberay-mem-scalability)=\n\n# KubeRay memory and scalability benchmark\n\n## Architecture\n\n![benchmark architecture](../images/benchmark_architecture.png)\n\nThis architecture is not a good practice, but it can fulfill the current requirements.## Preparation\n\nClone the [KubeRay repository](https://github.com/ray-project/kuberay) and checkout the `master` branch.This tutorial requires several files in the repository.## Step 1: Create a new Kubernetes cluster\n\nCreate a GKE cluster with autoscaling enabled.The following command creates a Kubernetes cluster named `kuberay-benchmark-cluster` on Google GKE.The cluster can scale up to 16 nodes, and each node of type `e2-highcpu-16` has 16 CPUs and 16 GB of memory.The following experiments may create up to ~150 Pods in the Kubernetes cluster, and each Ray Pod requires 1 CPU and 1 GB of memory.```sh\ngcloud container clusters create kuberay-benchmark-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 16 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-highcpu-16\n```\n\n## Step 2: Install Prometheus and Grafana\n\n```sh\n# Path: kuberay/\n./install/prometheus/install.sh\n```\n\nFollow \"Step 2: Install Kubernetes Prometheus Stack via Helm chart\" in [prometheus-grafana.md](https://github.com/ray-project/kuberay/blob/master/docs/guidance/prometheus-grafana.md#step-2-install-kubernetes-prometheus-stack-via-helm-chart) to install the [kube-prometheus-stack v48.2.1](https://github.com/prometheus-community/helm-charts/tree/kube-prometheus-stack-48.2.1/charts/kube-prometheus-stack) chart and related custom resources.## Step 3: Install a KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.## Step 4: Run experiments\n\n* Step 4.1: Make sure the `kubectl` CLI can connect to your GKE cluster.If not, run `gcloud auth login`.* Step 4.2: Run an experiment.```sh\n  # You can modify `memory_benchmark_utils` to run the experiment you want to run.# (path: benchmark/memory_benchmark/scripts)\n  python3 memory_benchmark_utils.py | tee benchmark_log\n  ```\n* Step 4.3: Follow [prometheus-grafana.md](https://github.com/ray-project/kuberay/blob/master/docs/guidance/prometheus-grafana.md#step-2-install-kubernetes-prometheus-stack-via-helm-chart) to access Grafana's dashboard.* Sign into the Grafana dashboard.* Click on \"Dashboards\".* Select \"Kubernetes / Compute Resources / Pod\".* Locate the \"Memory Usage\" panel for the KubeRay operator Pod.* Select the time range, then click on \"Inspect\" followed by \"Data\" to download the memory usage data of the KubeRay operator Pod.* Step 4.4: Delete all RayCluster custom resources.```sh\n  kubectl delete --all rayclusters.ray.io --namespace=default\n  ```\n* Step 4.5: Repeat Step 4.2 to Step 4.4 for other experiments.# Experiments\n\nThis benchmark is based on three benchmark experiments:\n\n* Experiment 1: Launch a RayCluster with 1 head and no workers.A new cluster is initiated every 20 seconds until there are a total of 150 RayCluster custom resources.* Experiment 2: Create a Kubernetes cluster, with only 1 RayCluster.Add 5 new worker Pods to this RayCluster every 60 seconds until the total reaches 150 Pods.* Experiment 3: Create a 5-node (1 head + 4 workers) RayCluster every 60 seconds up to 30 RayCluster custom resources.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "672ee971-7834-49fb-a0a0-128ec84ed8d0": {"__data__": {"id_": "672ee971-7834-49fb-a0a0-128ec84ed8d0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md", "text_hash": "1a7b4596182177134471609cbfe80df9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d8a469417f56973e8e700003f5a35e61cf96beab", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md"}, "hash": "6845751418aec1a36f1342aa40ef7e122a1a4c497c3a94e77381b4e5f2530ef4"}, "2": {"node_id": "254f4b24-2fae-416c-8335-602e73835fce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md", "text_hash": "bd95ae63529c2bbde5ae05c3aeb18559"}, "hash": "500718868f473af384e2f7aca857863920b43c2772b6b4319e7dcf5fece187ce"}}, "hash": "61c7f6964e94dff9711d60b0f3b5b9b4d128ad60ba5858f8fb6b863328cfe986", "text": "Based on [the survey](https://forms.gle/KtMLzjXcKoeSTj359) for KubeRay users, the benchmark target is set at 150 Ray Pods to cover most use cases.## Experiment results (KubeRay v0.6.0)\n\n![benchmark result](../images/benchmark_result.png)\n\n* You can generate the above figure by running:\n  ```sh\n  # (path: benchmark/memory_benchmark/scripts)\n  python3 experiment_figures.py\n  # The output image `benchmark_result.png` will be stored in `scripts/`.```\n\n* As shown in the figure, the memory usage of the KubeRay operator Pod is highly and positively correlated to the number of Pods in the Kubernetes cluster.In addition, the number of custom resources in the Kubernetes cluster does not have a significant impact on the memory usage.* Note that the x-axis \"Number of Pods\" is the number of Pods that are created rather than running.If the Kubernetes cluster does not have enough computing resources, the GKE Autopilot adds a new Kubernetes node into the cluster.This process may take a few minutes, so some Pods may be pending in the process.This lag may can explain why the memory usage is somewhat throttled.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1f4bb2b-9424-4dbc-83e6-d52f25d2c885": {"__data__": {"id_": "a1f4bb2b-9424-4dbc-83e6-d52f25d2c885", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples.md", "file_name": "examples.md", "text_hash": "0100bf44d0783b71c5ad7c5b3e744483"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "b6158a2e1f824622aadd10423a4b92218966f453", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples.md", "file_name": "examples.md"}, "hash": "04c1e91b094d254367678079d1c52a20cfbe6a36be27737a0c3d979db0a4130e"}}, "hash": "81fbe54f1a39336c9a7ec6206c7d4ee6a74e8ec03673a6d2495b6defeb68da3c", "text": "(kuberay-examples)=\n\n# Examples\n\nThis section presents example Ray workloads to try out on your Kubernetes cluster.\n\n- {ref}`kuberay-ml-example` (CPU-only)\n- {ref}`kuberay-gpu-training-example`\n- {ref}`kuberay-mobilenet-rayservice-example` (CPU-only)\n- {ref}`kuberay-stable-diffusion-rayservice-example`\n- {ref}`kuberay-text-summarizer-rayservice-example`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d1292c35-6ed4-4667-84b4-c1a45dba25cf": {"__data__": {"id_": "d1292c35-6ed4-4667-84b4-c1a45dba25cf", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "6881a56b993ab4920b9f746e4c46bf3a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md"}, "hash": "32c5eeec61b327415cec2a68f8c3a25509d0b2d7b4c3186d7cd96b19fefc1aa3"}, "3": {"node_id": "25fa0e5c-0ca9-433b-9c62-73307a129946", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "dc5504b58d1bb6af4c13881861bdcb6f"}, "hash": "7e45e460387559851108440b982056d6d61727400a6f700daa113ee7429e1223"}}, "hash": "a6c198631d35737724d061f89adbd00e8a86c3ec66a08ad1e1447d0befcebce6", "text": "(kuberay-gpu-training-example)=\n\n# Train PyTorch ResNet model with GPUs on Kubernetes\nThis guide runs a sample Ray machine learning training workload with GPU on Kubernetes infrastructure.It runs Ray's {ref}`PyTorch image training benchmark <pytorch_gpu_training_benchmark>` with a 1 gigabyte training set.:::{note}\nTo learn the basics of Ray on Kubernetes, we recommend taking a look\nat the {ref}`introductory guide <kuberay-quickstart>` first.:::\n\nNote that a version of at least 1.19 is required for Kubernetes and Kubectl.## The end-to-end workflow\nThe following script summarizes the end-to-end workflow for GPU training.These instructions are for GCP, but a similar setup would work for any major cloud provider.The following script consists of:\n- Step 1: Set up a Kubernetes cluster on GCP.- Step 2: Deploy a Ray cluster on Kubernetes with the KubeRay operator.- Step 3: Run the PyTorch image training benchmark.```shell\n# Step 1: Set up a Kubernetes cluster on GCP\n# Create a node-pool for a CPU-only head node\n# e2-standard-8 => 8 vCPU; 32 GB RAM\ngcloud container clusters create gpu-cluster-1 \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-central1-c --machine-type e2-standard-8\n\n# Create a node-pool for GPU.The node is for a GPU Ray worker node.# n1-standard-8 => 8 vCPU; 30 GB RAM\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-t4,count=1 \\\n  --zone us-central1-c --cluster gpu-cluster-1 \\\n  --num-nodes 1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n  --machine-type n1-standard-8\n\n# Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded.yaml\n\n# Step 2: Deploy a Ray cluster on Kubernetes with the KubeRay operator.# Please make sure you are connected to your Kubernetes cluster.For GCP, you can do so by:\n#   (Method 1) Copy the connection command from the GKE console\n#   (Method 2) \"gcloud container clusters get-credentials <your-cluster-name> --region <your-region> --project <your-project>\"\n#   (Method 3) \"kubectl config use-context ...\"\n\n# Install both CRDs and KubeRay operator v0.6.0.helm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.6.0\n\n# Create a Ray cluster\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/ray-cluster.gpu.yaml\n\n# Set up port-forwarding\nkubectl port-forward --address 0.0.0.0 services/raycluster-head-svc 8265:8265\n\n# Step 3: Run the PyTorch image training benchmark.# Install Ray if needed\npip3 install -U \"ray[default]\"\n\n# Download the Python script\ncurl https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/doc_code/pytorch_training_e2e_submit.py -o pytorch_training_e2e_submit.py\n\n# Submit the training job to your ray cluster\npython3 pytorch_training_e2e_submit.py\n\n# Use the following command to follow this Job's logs:\n# Substitute the Ray Job's submission id.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "25fa0e5c-0ca9-433b-9c62-73307a129946": {"__data__": {"id_": "25fa0e5c-0ca9-433b-9c62-73307a129946", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "dc5504b58d1bb6af4c13881861bdcb6f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md"}, "hash": "32c5eeec61b327415cec2a68f8c3a25509d0b2d7b4c3186d7cd96b19fefc1aa3"}, "2": {"node_id": "d1292c35-6ed4-4667-84b4-c1a45dba25cf", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "6881a56b993ab4920b9f746e4c46bf3a"}, "hash": "a6c198631d35737724d061f89adbd00e8a86c3ec66a08ad1e1447d0befcebce6"}, "3": {"node_id": "754d4905-e97d-4d97-909d-ed531f7b0f32", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "d35ec5e6de3dfb2de8bab7f78fb11e72"}, "hash": "aadb9e42e91400b3298e4c810f91b806a6f2becf3b2666334f7d423d8ce3a93d"}}, "hash": "7e45e460387559851108440b982056d6d61727400a6f700daa113ee7429e1223", "text": "ray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --address http://127.0.0.1:8265 --follow\n```\nIn the rest of this document, we present a more detailed breakdown of the above workflow.## Step 1: Set up a Kubernetes cluster on GCP.In this section, we set up a Kubernetes cluster with CPU and GPU node pools.These instructions are for GCP, but a similar setup would work for any major cloud provider.If you have an existing Kubernetes cluster with GPU, you can ignore this step.If you are new to Kubernetes and you are planning to deploy Ray workloads on a managed\nKubernetes service, we recommend taking a look at this {ref}`introductory guide <kuberay-k8s-setup>` first.It is not necessary to run this example with a cluster having that much RAM (>30GB per node in the following commands).Feel free to update\nthe option `machine-type` and the resource requirements in `ray-cluster.gpu.yaml`.In the first command, we create a Kubernetes cluster `gpu-cluster-1` with one CPU node (`e2-standard-8`: 8 vCPU; 32 GB RAM).In the second command,\nwe add a new node (`n1-standard-8`: 8 vCPU; 30 GB RAM) with a GPU (`nvidia-tesla-t4`) to the cluster.```shell\n# Step 1: Set up a Kubernetes cluster on GCP.# e2-standard-8 => 8 vCPU; 32 GB RAM\ngcloud container clusters create gpu-cluster-1 \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-central1-c --machine-type e2-standard-8\n\n# Create a node-pool for GPU\n# n1-standard-8 => 8 vCPU; 30 GB RAM\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-tesla-t4,count=1 \\\n  --zone us-central1-c --cluster gpu-cluster-1 \\\n  --num-nodes 1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n  --machine-type n1-standard-8\n\n# Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n```\n\n## Step 2: Deploy a Ray cluster on Kubernetes with the KubeRay operator.To execute the following steps, please make sure you are connected to your Kubernetes cluster.For GCP, you can do so by:\n* Copy the connection command from the GKE console\n* `gcloud container clusters get-credentials <your-cluster-name> --region <your-region> --project <your-project>` ([Link](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials))\n* `kubectl config use-context` ([Link](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/))\n\nThe first command will deploy KubeRay (ray-operator) to your Kubernetes cluster.The second command will create a ray cluster with the help of KubeRay.The third command is used to map port 8265 of the `ray-head` pod to **127.0.0.1:8265**.You can check\n**127.0.0.1:8265** to see the dashboard.The last command is used to test your Ray cluster by submitting a simple job.It is optional.```shell\n# Step 2: Deploy a Ray cluster on Kubernetes with the KubeRay operator.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "754d4905-e97d-4d97-909d-ed531f7b0f32": {"__data__": {"id_": "754d4905-e97d-4d97-909d-ed531f7b0f32", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "d35ec5e6de3dfb2de8bab7f78fb11e72"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md"}, "hash": "32c5eeec61b327415cec2a68f8c3a25509d0b2d7b4c3186d7cd96b19fefc1aa3"}, "2": {"node_id": "25fa0e5c-0ca9-433b-9c62-73307a129946", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "dc5504b58d1bb6af4c13881861bdcb6f"}, "hash": "7e45e460387559851108440b982056d6d61727400a6f700daa113ee7429e1223"}}, "hash": "aadb9e42e91400b3298e4c810f91b806a6f2becf3b2666334f7d423d8ce3a93d", "text": "# Create the KubeRay operator\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.6.0\n\n# Create a Ray cluster\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/ray-cluster.gpu.yaml\n\n# port forwarding\nkubectl port-forward --address 0.0.0.0 services/raycluster-head-svc 8265:8265\n\n# Test cluster (optional)\nray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n## Step 3: Run the PyTorch image training benchmark.We will use the [Ray Job Python SDK](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/sdk.html#ray-job-sdk) to submit the PyTorch workload.```{literalinclude} /cluster/doc_code/pytorch_training_e2e_submit.py\n:language: python\n```\n\nTo submit the workload, run the above Python script.The script is available in the [Ray repository](https://github.com/ray-project/ray/tree/master/doc/source/cluster/doc_code/pytorch_training_e2e_submit.py)\n```shell\n# Step 3: Run the PyTorch image training benchmark.# Install Ray if needed\npip3 install -U \"ray[default]\"\n\n# Download the Python script\ncurl https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/doc_code/pytorch_training_e2e_submit.py -o pytorch_training_e2e_submit.py\n\n# Submit the training job to your ray cluster\npython3 pytorch_training_e2e_submit.py\n# Example STDOUT:\n# Use the following command to follow this Job's logs:\n# ray job logs 'raysubmit_jNQxy92MJ4zinaDX' --follow\n\n# Track job status\n# Substitute the Ray Job's submission id.ray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --address http://127.0.0.1:8265 --follow\n```\n\n## Clean-up\nDelete your Ray cluster and KubeRay with the following commands:\n```shell\nkubectl delete raycluster raycluster\n\n# Please make sure the ray cluster has already been removed before delete the operator.helm uninstall kuberay-operator\n```\nIf you're on a public cloud, don't forget to clean up the underlying\nnode group and/or Kubernetes cluster.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "477198a6-6281-404f-93d5-e34ffd8f67a0": {"__data__": {"id_": "477198a6-6281-404f-93d5-e34ffd8f67a0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "a32f6af9b9bd4643fb975f337996d763"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99ef5905657b4160f73fcb6785446d890372bf00", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md"}, "hash": "7d956a0ee72b3af064d8ed52c30aafb369e185c563e6a3dae9659c611dc55aa6"}, "3": {"node_id": "4deac6ab-2a2f-4869-80b5-1bee877f0bb8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "3eb6722ad91cdc8f2d27875bf505bc81"}, "hash": "94bd9f77a5cd09c3abe1955a6b0eb69ded3157d0553328e41fb8972de174bc85"}}, "hash": "64bafc7cfae69b41bfa987e9b9414992bf504673c382c59b4376c93910d9a88c", "text": "(kuberay-ml-example)=\n\n# Ray Train XGBoostTrainer on Kubernetes\n\n:::{note}\nTo learn the basics of Ray on Kubernetes, we recommend taking a look\nat the {ref}`introductory guide <kuberay-quickstart>` first.\n:::", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4deac6ab-2a2f-4869-80b5-1bee877f0bb8": {"__data__": {"id_": "4deac6ab-2a2f-4869-80b5-1bee877f0bb8", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "3eb6722ad91cdc8f2d27875bf505bc81"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99ef5905657b4160f73fcb6785446d890372bf00", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md"}, "hash": "7d956a0ee72b3af064d8ed52c30aafb369e185c563e6a3dae9659c611dc55aa6"}, "2": {"node_id": "477198a6-6281-404f-93d5-e34ffd8f67a0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "a32f6af9b9bd4643fb975f337996d763"}, "hash": "64bafc7cfae69b41bfa987e9b9414992bf504673c382c59b4376c93910d9a88c"}, "3": {"node_id": "70782ece-9927-4f94-8b0d-ab19026587fe", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "6c7e5238f95d2b7c04202152fa2a5fc3"}, "hash": "d6c01d387af497deadf5271b4fe291dc512145f3d7f5990ed3e7ad260fefd41c"}}, "hash": "94bd9f77a5cd09c3abe1955a6b0eb69ded3157d0553328e41fb8972de174bc85", "text": "In this guide, we show you how to run a sample Ray machine learning\nworkload on Kubernetes infrastructure.\n\nWe will run Ray's {ref}`XGBoost training benchmark <xgboost-benchmark>` with a 100 gigabyte training set.\nTo learn more about using Ray's XGBoostTrainer, check out {ref}`the XGBoostTrainer documentation <train-gbdt-guide>`.\n\n## Kubernetes infrastructure setup on GCP\n\nThis document provides instructions for GCP to create a Kubernetes cluster, but a similar setup would work for any major cloud provider.\nCheck out the {ref}`introductory guide <kuberay-k8s-setup>` if you want to set up a Kubernetes cluster on other cloud providers.\nIf you have an existing Kubernetes cluster, you can ignore this step.\n\n```shell\n# Set up a cluster on Google Kubernetes Engine (GKE)\ngcloud container clusters create autoscaler-ray-cluster \\\n    --num-nodes=10 --zone=us-central1-c --machine-type e2-standard-16 --disk-size 1000GB\n\n# (Optional) Set up a cluster with autopilot on Google Kubernetes Engine (GKE).\n# The following command creates an autoscaling node pool with a 1 node minimum and a 10 node maximum.\n# The 1 static node will be used to run the Ray head pod. This node may also host the KubeRay\n# operator and Kubernetes system components. After the workload is submitted, 9 additional nodes will\n# scale up to accommodate Ray worker pods. These nodes will scale back down after the workload is complete.\ngcloud container clusters create autoscaler-ray-cluster \\\n    --num-nodes=1 --min-nodes 1 --max-nodes 10 --enable-autoscaling \\\n    --zone=us-central1-c --machine-type e2-standard-16 --disk-size 1000GB\n```\n\nMake sure you are connected to your Kubernetes cluster. For GCP, you can do so by:\n* Navigate to your GKE cluster page, and click \"CONNECT\" button. Then, copy \"Command-line access\".\n* `gcloud container clusters get-credentials <your-cluster-name> --region <your-region> --project <your-project>` ([Link](https://cloud.google.com/sdk/gcloud/reference/container/clusters/get-credentials))\n* `kubectl config use-context` ([Link](https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/))\n\nFor the workload in this guide, it is recommended to use a pool or group of Kubernetes nodes\nwith the following properties:\n- 10 nodes total\n- A capacity of 16 CPU and 64 Gi memory per node. For the major cloud providers, suitable instance types include\n    * m5.4xlarge (Amazon Web Services)\n    * Standard_D5_v2 (Azure)\n    * e2-standard-16 (Google Cloud)\n- Each node should be configured with 1000 gigabytes of disk space (to store the training set).\n\n## Deploy the KubeRay operator\n\nOnce you have set up your Kubernetes cluster, deploy the KubeRay operator.\nRefer to the {ref}`Getting Started guide <kuberay-operator-deploy>`\nfor instructions on this step.\n\n## Deploy a Ray cluster\n\nNow we're ready to deploy the Ray cluster that will execute our workload.\n\n:::{tip}\nThe Ray cluster we'll deploy is configured such that one Ray pod will be scheduled\nper 16-CPU Kubernetes node. The pattern of one Ray pod per Kubernetes node is encouraged, but not required.\nBroadly speaking, it is more efficient to use a few large Ray pods than many small ones.\n:::\n\nWe recommend taking a look at the [config file][ConfigLink] applied in the following command.\n```shell\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/kubernetes/configs/xgboost-benchmark.yaml\n```\n\nA Ray head pod and 9 Ray worker pods will be created.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "70782ece-9927-4f94-8b0d-ab19026587fe": {"__data__": {"id_": "70782ece-9927-4f94-8b0d-ab19026587fe", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "6c7e5238f95d2b7c04202152fa2a5fc3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99ef5905657b4160f73fcb6785446d890372bf00", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md"}, "hash": "7d956a0ee72b3af064d8ed52c30aafb369e185c563e6a3dae9659c611dc55aa6"}, "2": {"node_id": "4deac6ab-2a2f-4869-80b5-1bee877f0bb8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "3eb6722ad91cdc8f2d27875bf505bc81"}, "hash": "94bd9f77a5cd09c3abe1955a6b0eb69ded3157d0553328e41fb8972de174bc85"}, "3": {"node_id": "e94c4278-d68b-4377-91fc-feebe5aa1951", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "5214770f66a127d58fd4ffed60b319f3"}, "hash": "e87fd1751153778c85f018579e880726882840c8d43aa78432e9733b98f2abc8"}}, "hash": "d6c01d387af497deadf5271b4fe291dc512145f3d7f5990ed3e7ad260fefd41c", "text": "```{admonition} Optional: Deploying an autoscaling Ray cluster\nIf you've set up an autoscaling node group or pool, you may wish to deploy\nan autoscaling cluster by applying the config [xgboost-benchmark-autoscaler.yaml][ConfigLinkAutoscaling].One Ray head pod will be created.Once the workload starts, the Ray autoscaler will trigger\ncreation of Ray worker pods.Kubernetes autoscaling will then create nodes to place the Ray pods.```\n\n## Run the workload\n\nTo observe the startup progress of the Ray head pod, run the following command.```shell\n# If you're on MacOS, first `brew install watch`.watch -n 1 kubectl get pod\n```\n\nOnce the Ray head pod enters `Running` state, we are ready to execute the XGBoost workload.We will use {ref}`Ray Job Submission <jobs-overview>` to kick off the workload.### Connect to the cluster.We use port-forwarding in this guide as a simple way to experiment with a Ray cluster's services.See the {ref}`networking notes <kuberay-networking>` for production use-cases.```shell\n# Run the following blocking command in a separate shell.kubectl port-forward --address 0.0.0.0 service/raycluster-xgboost-benchmark-head-svc 8265:8265\n```\n\n### Submit the workload.We'll use the {ref}`Ray Job Python SDK <ray-job-sdk>` to submit the XGBoost workload.```{literalinclude} /cluster/doc_code/xgboost_submit.py\n:language: python\n```\n\nTo submit the workload, run the above Python script.The script is available [in the Ray repository][XGBSubmit].```shell\n# Download the above script.curl https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/doc_code/xgboost_submit.py -o xgboost_submit.py\n# Run the script.python xgboost_submit.py\n```\n\n### Observe progress.The benchmark may take up to 60 minutes to run.Use the following tools to observe its progress.#### Job logs\n\nTo follow the job's logs, use the command printed by the above submission script.```shell\n# Substitute the Ray Job's submission id.ray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --follow --address http://127.0.0.1:8265\n```\n\n#### Kubectl\n\nObserve the pods in your cluster with\n```shell\n# If you're on MacOS, first `brew install watch`.watch -n 1 kubectl get pod\n```\n\n#### Ray Dashboard\n\nView `localhost:8265` in your browser to access the Ray Dashboard.#### Ray Status\n\nObserve autoscaling status and Ray resource usage with\n```shell\n# Substitute the name of your Ray cluster's head pod.watch -n 1 kubectl exec -it raycluster-xgboost-benchmark-head-xxxxx -- ray status\n```\n\n:::{note}\nUnder some circumstances and for certain cloud providers,\nthe K8s API server may become briefly unavailable during Kubernetes\ncluster resizing events.Don't worry if that happens -- the Ray workload should be uninterrupted.For the example in this guide, wait until the API server is back up, restart the port-forwarding process,\nand re-run the job log command.:::\n\n### Job completion\n\n#### Benchmark results\n\nOnce the benchmark is complete, the job log will display the results:\n\n```\nResults: {'training_time': 1338.488839321999, 'prediction_time': 403.36653568099973}\n```\n\nThe performance of the benchmark is sensitive to the underlying cloud infrastructure --\nyou might not match {ref}`the numbers quoted in the benchmark docs <xgboost-benchmark>`.#### Model parameters\nThe file `model.json` in the Ray head pod contains the parameters for the trained model.Other result data will be available in the directory `ray_results` in the head pod.Refer to the {ref}`the XGBoostTrainer documentation <train-gbdt-guide>` for details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e94c4278-d68b-4377-91fc-feebe5aa1951": {"__data__": {"id_": "e94c4278-d68b-4377-91fc-feebe5aa1951", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "5214770f66a127d58fd4ffed60b319f3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "99ef5905657b4160f73fcb6785446d890372bf00", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md"}, "hash": "7d956a0ee72b3af064d8ed52c30aafb369e185c563e6a3dae9659c611dc55aa6"}, "2": {"node_id": "70782ece-9927-4f94-8b0d-ab19026587fe", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "6c7e5238f95d2b7c04202152fa2a5fc3"}, "hash": "d6c01d387af497deadf5271b4fe291dc512145f3d7f5990ed3e7ad260fefd41c"}}, "hash": "e87fd1751153778c85f018579e880726882840c8d43aa78432e9733b98f2abc8", "text": "```{admonition} Scale-down\nIf autoscaling is enabled, Ray worker pods will scale down after 60 seconds.After the Ray worker pods are gone, your Kubernetes infrastructure should scale down\nthe nodes that hosted these pods.```\n\n#### Clean-up\nDelete your Ray cluster with the following command:\n```shell\nkubectl delete raycluster raycluster-xgboost-benchmark\n```\nIf you're on a public cloud, don't forget to clean up the underlying\nnode group and/or Kubernetes cluster.[ConfigLink]:https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/kubernetes/configs/xgboost-benchmark.yaml\n[ConfigLinkAutoscaling]: https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/kubernetes/configs/xgboost-benchmark-autoscaler.yaml\n[XGBSubmit]: https://github.com/ray-project/ray/blob/releases/2.0.0/doc/source/cluster/doc_code/xgboost_submit.py", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d013edf-234c-43ec-ad73-186085f999c7": {"__data__": {"id_": "0d013edf-234c-43ec-ad73-186085f999c7", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md", "file_name": "mobilenet-rayservice.md", "text_hash": "000839d36da9759a415420ee38135b0c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "51564e249ad3441ea771c142becb87d99c6862c8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md", "file_name": "mobilenet-rayservice.md"}, "hash": "231ca66331a0d509fa9d5d6d6ffa026824dfed29b692818d557cfe03af8a9862"}}, "hash": "3a819f815c0ece0644b4857e7b46e3a00da6bd6859a2bc1e33e54d319767230e", "text": "(kuberay-mobilenet-rayservice-example)=\n\n# Serve a MobileNet image classifier on Kubernetes\n\n> **Note:** The Python files for the Ray Serve application and its client are in the repository [ray-project/serve_config_examples](https://github.com/ray-project/serve_config_examples).\n\n## Step 1: Create a Kubernetes cluster with Kind\n\n```sh\nkind create cluster --image=kindest/node:v1.23.0\n```\n\n## Step 2: Install KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator from the Helm repository.\nNote that the YAML file in this example uses `serveConfigV2`, which is supported by KubeRay version v0.6.0 and later.\n\n## Step 3: Install a RayService\n\n```sh\n# Download `ray-service.mobilenet.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.mobilenet.yaml\n\n# Create a RayService\nkubectl apply -f ray-service.mobilenet.yaml\n```\n\n* The [mobilenet.py](https://github.com/ray-project/serve_config_examples/blob/master/mobilenet/mobilenet.py) file requires `tensorflow` as a dependency. Hence, the YAML file uses `rayproject/ray-ml:2.5.0` instead of `rayproject/ray:2.5.0`.\n* `python-multipart` is required for the request parsing function `starlette.requests.form()`, so the YAML file includes `python-multipart` in the runtime environment.\n\n## Step 4: Forward the port for Ray Serve\n\n```sh\nkubectl port-forward svc/rayservice-mobilenet-serve-svc 8000\n```\n\nNote that the Serve service is created after the Ray Serve applications are ready and running. This process may take approximately 1 minute after all Pods in the RayCluster are running.\n\n## Step 5: Send a request to the ImageClassifier\n\n* Step 5.1: Prepare an image file.\n* Step 5.2: Update `image_path` in [mobilenet_req.py](https://github.com/ray-project/serve_config_examples/blob/master/mobilenet/mobilenet_req.py)\n* Step 5.3: Send a request to the `ImageClassifier`.\n  ```sh\n  python mobilenet_req.py\n  # sample output: {\"prediction\":[\"n02099601\",\"golden_retriever\",0.17944198846817017]}\n  ```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "724f461c-d500-4bfe-a873-98591710ab4f": {"__data__": {"id_": "724f461c-d500-4bfe-a873-98591710ab4f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md", "file_name": "stable-diffusion-rayservice.md", "text_hash": "d5531d93cd99b346bf02bce1bcf39b03"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "24cd0b5eca2e29f47acda73bddf678471ba71237", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md", "file_name": "stable-diffusion-rayservice.md"}, "hash": "1357b6c351a5627471a8fe856130b86daab1c51c69961f0440f8f13dd4161020"}}, "hash": "1357b6c351a5627471a8fe856130b86daab1c51c69961f0440f8f13dd4161020", "text": "(kuberay-stable-diffusion-rayservice-example)=\n\n# Serve a StableDiffusion text-to-image model on Kubernetes\n\n> **Note:** The Python files for the Ray Serve application and its client are in the [ray-project/serve_config_examples](https://github.com/ray-project/serve_config_examples) repo \nand [the Ray documentation](https://docs.ray.io/en/latest/serve/tutorials/stable-diffusion.html).\n\n## Step 1: Create a Kubernetes cluster with GPUs\n\nFollow [aws-eks-gpu-cluster.md](kuberay-eks-gpu-cluster-setup) or [gcp-gke-gpu-cluster.md](kuberay-gke-gpu-cluster-setup) to create a Kubernetes cluster with 1 CPU node and 1 GPU node.\n\n## Step 2: Install KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.\nPlease note that the YAML file in this example uses `serveConfigV2`, which is supported starting from KubeRay v0.6.0.\n\n## Step 3: Install a RayService\n\n```sh\n# Step 3.1: Download `ray-service.stable-diffusion.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.stable-diffusion.yaml\n\n# Step 3.2: Create a RayService\nkubectl apply -f ray-service.stable-diffusion.yaml\n```\n\nThis RayService configuration contains some important settings:\n\n* The `tolerations` for workers allow them to be scheduled on nodes without any taints or on nodes with specific taints. However, workers will only be scheduled on GPU nodes because we set `nvidia.com/gpu: 1` in the Pod's resource configurations.\n    ```yaml\n    # Please add the following taints to the GPU node.\n    tolerations:\n        - key: \"ray.io/node-type\"\n        operator: \"Equal\"\n        value: \"worker\"\n        effect: \"NoSchedule\"\n    ```\n* It includes `diffusers` in `runtime_env` since this package is not included by default in the `ray-ml` image.\n\n## Step 4: Forward the port of Serve\n\nFirst get the service name from this command.\n\n```sh\nkubectl get services\n```\n\nThen, port forward to the serve.\n\n```sh\nkubectl port-forward svc/stable-diffusion-serve-svc 8000\n```\n\nNote that the RayService's Kubernetes service will be created after the Serve applications are ready and running. This process may take approximately 1 minute after all Pods in the RayCluster are running.\n\n## Step 5: Send a request to the text-to-image model\n\n```sh\n# Step 5.1: Download `stable_diffusion_req.py` \ncurl -LO https://raw.githubusercontent.com/ray-project/serve_config_examples/master/stable_diffusion/stable_diffusion_req.py\n\n# Step 5.2: Set your `prompt` in `stable_diffusion_req.py`.\n\n# Step 5.3: Send a request to the Stable Diffusion model.\npython stable_diffusion_req.py\n# Check output.png\n```\n\n* You can refer to the document [\"Serving a Stable Diffusion Model\"](https://docs.ray.io/en/latest/serve/tutorials/stable-diffusion.html) for an example output image.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cdbd593a-840e-444a-a656-9c09705d441f": {"__data__": {"id_": "cdbd593a-840e-444a-a656-9c09705d441f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md", "file_name": "text-summarizer-rayservice.md", "text_hash": "b1755dc872ac4f80045a8006dae21365"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "740309fdd83face28fb6334740e83223522f5b52", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md", "file_name": "text-summarizer-rayservice.md"}, "hash": "c835e2a3c3a603cc6a5e50eb6f06da3b80da9cd63dadd2a3103e62f54b0d7dc0"}}, "hash": "c835e2a3c3a603cc6a5e50eb6f06da3b80da9cd63dadd2a3103e62f54b0d7dc0", "text": "(kuberay-text-summarizer-rayservice-example)=\n\n# Serve a text summarizer on Kubernetes\n\n> **Note:** The Python files for the Ray Serve application and its client are in the [ray-project/serve_config_examples](https://github.com/ray-project/serve_config_examples) repo.\n\n## Step 1: Create a Kubernetes cluster with GPUs\n\nFollow [aws-eks-gpu-cluster.md](kuberay-eks-gpu-cluster-setup) or [gcp-gke-gpu-cluster.md](kuberay-gke-gpu-cluster-setup) to create a Kubernetes cluster with 1 CPU node and 1 GPU node.\n\n## Step 2: Install KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.\nPlease note that the YAML file in this example uses `serveConfigV2`, which is supported starting from KubeRay v0.6.0.\n\n## Step 3: Install a RayService\n\n```sh\n# Step 3.1: Download `ray-service.text-summarizer.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-service.text-summarizer.yaml\n\n# Step 3.2: Create a RayService\nkubectl apply -f ray-service.text-sumarizer.yaml\n```\n\nThis RayService configuration contains some important settings:\n\n* The `tolerations` for workers allow them to be scheduled on nodes without any taints or on nodes with specific taints. However, workers will only be scheduled on GPU nodes because we set `nvidia.com/gpu: 1` in the Pod's resource configurations.\n    ```yaml\n    # Please add the following taints to the GPU node.\n    tolerations:\n        - key: \"ray.io/node-type\"\n        operator: \"Equal\"\n        value: \"worker\"\n        effect: \"NoSchedule\"\n    ```\n\n## Step 4: Forward the port of Serve\n\nFirst get the service name from this command.\n\n```sh\nkubectl get services\n```\n\nThen, port forward to the serve.\n\n```sh\nkubectl port-forward svc/text-summarizer-serve-svc 8000\n```\n\nNote that the RayService's Kubernetes service will be created after the Serve applications are ready and running. This process may take approximately 1 minute after all Pods in the RayCluster are running.\n\n## Step 5: Send a request to the text_summarizer model\n\n```sh\n# Step 5.1: Download `text_summarizer_req.py` \ncurl -LO https://raw.githubusercontent.com/ray-project/serve_config_examples/master/text_summarizer/text_summarizer_req.py\n\n# Step 5.2: Send a request to the Summarizer model.\npython text_summarizer_req.py\n# Check printed to console\n```\n\n## Step 6: Delete your service\n\n```sh\n# path: ray-operator/config/samples/\nkubectl delete -f ray-service.text-sumarizer.yaml\n```\n\n## Step 7: Uninstall your kuberay operator\n\nFollow [this document](https://github.com/ray-project/kuberay/tree/master/helm-chart/kuberay-operator) to uninstall the latest stable KubeRay operator via Helm repository.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6e5614c6-564e-4916-9e3b-b1a32d00fea0": {"__data__": {"id_": "6e5614c6-564e-4916-9e3b-b1a32d00fea0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started.md", "file_name": "getting-started.md", "text_hash": "133377325d7cd68ce638e977d569f5d5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13a936d2637d0c51a1a223abbd5e988368442ae4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started.md", "file_name": "getting-started.md"}, "hash": "a6122ca274351c0b2dda9b38624bfeecfd56e79191e0f548900701d21457badc"}}, "hash": "c1e1b68bdd108119eac7d2ca560a34a41cb625f7ff15565385444cb8866544ee", "text": "(kuberay-quickstart)=\n\n# Getting Started with KubeRay\n\n## Custom Resource Definitions (CRDs)\n\n[KubeRay](https://github.com/ray-project/kuberay) is a powerful, open-source Kubernetes operator that simplifies the deployment and management of Ray applications on Kubernetes.\nIt offers 3 custom resource definitions (CRDs):\n\n* **RayCluster**: KubeRay fully manages the lifecycle of RayCluster, including cluster creation/deletion, autoscaling, and ensuring fault tolerance.\n\n* **RayJob**: With RayJob, KubeRay automatically creates a RayCluster and submits a job when the cluster is ready. You can also configure RayJob to automatically delete the RayCluster once the job finishes.\n\n* **RayService**: RayService is made up of two parts: a RayCluster and Ray Serve deployment graphs. RayService offers zero-downtime upgrades for RayCluster and high availability.\n\n## Which CRD should you choose?\n\nUsing [RayService](kuberay-rayservice-quickstart) to serve models and using [RayCluster](kuberay-raycluster-quickstart) to develop Ray applications are no-brainer recommendations from us.\nHowever, if the use case is not model serving or prototyping, how do you choose between [RayCluster](kuberay-raycluster-quickstart) and [RayJob](kuberay-rayjob-quickstart)?\n\n### Q: Is downtime acceptable during a cluster upgrade (e.g. Upgrade Ray version)?\n\nIf not, use RayJob. RayJob can be configured to automatically delete the RayCluster once the job is completed. You can switch between Ray versions and configurations for each job submission using RayJob.\n\nIf yes, use RayCluster. Ray doesn't natively support rolling upgrades; thus, you'll need to manually shut down and create a new RayCluster.\n\n### Q: Are you deploying on public cloud providers (e.g. AWS, GCP, Azure)?\n\nIf yes, use RayJob. It allows automatic deletion of the RayCluster upon job completion, helping you reduce costs.\n\n### Q: Do you care about the latency introduced by spinning up a RayCluster?\n\nIf yes, use RayCluster.\nUnlike RayJob, which creates a new RayCluster every time a job is submitted, RayCluster creates the cluster just once and can be used multiple times.\n\n## Run your first Ray application on Kubernetes!\n\n* [RayCluster Quick Start](kuberay-raycluster-quickstart)\n* [RayJob Quick Start](kuberay-rayjob-quickstart)\n* [RayService Quick Start](kuberay-rayservice-quickstart)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f639521d-65a4-4474-b6f0-c4aff6d62a77": {"__data__": {"id_": "f639521d-65a4-4474-b6f0-c4aff6d62a77", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "134fdd75f8224140ba58099b04195ab4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d44a40b06a831809a1a01e668d3def8d2333e2a6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md"}, "hash": "836f76d79e72651c249d4ce47856396a3e7253eb780db0aebe366a4815e7ffd6"}, "3": {"node_id": "774a6a5a-3743-4677-ba88-6937a3282653", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "6f6c2f77df45001d0ff9b4914f4c0e02"}, "hash": "76b98eb63489dfad4894b19bf245deabe3019621639f32bbade02d25f6603cef"}}, "hash": "8f092c2b231a15c045e0f9451dbede87a0456cd86dc4e4c19d98be8222900861", "text": "(kuberay-raycluster-quickstart)=\n\n# RayCluster\n\nIn this guide, we show you how to manage and interact with Ray clusters on Kubernetes.## Preparation\n\n* Install [kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl) (>= 1.19), [Helm](https://helm.sh/docs/intro/install/) (>= v3.4), and [Kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation).* Make sure your Kubernetes cluster has at least 4 CPU and 4 GB RAM.## Step 1: Create a Kubernetes cluster\n\nThis step creates a local Kubernetes cluster using [Kind](https://kind.sigs.k8s.io/).If you already have a Kubernetes cluster, you can skip this step.```sh\nkind create cluster --image=kindest/node:v1.23.0\n```\n\n(kuberay-operator-deploy)=\n## Step 2: Deploy a KubeRay operator\n\nDeploy the KubeRay operator with the [Helm chart repository](https://github.com/ray-project/kuberay-helm).```sh\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\n\n# Install both CRDs and KubeRay operator v0.6.0.helm install kuberay-operator kuberay/kuberay-operator --version 0.6.0\n\n# Confirm that the operator is running in the namespace `default`.kubectl get pods\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-7fbdbf8c89-pt8bk   1/1     Running   0          27s\n```\n\nKubeRay offers multiple options for operator installations, such as Helm, Kustomize, and a single-namespaced operator.For further information, please refer to [the installation instructions in the KubeRay documentation](https://ray-project.github.io/kuberay/deploy/installation/).## Step 3: Deploy a RayCluster custom resource\n\nOnce the KubeRay operator is running, we are ready to deploy a RayCluster.To do so, we create a RayCluster Custom Resource (CR) in the `default` namespace.```sh\n# Deploy a sample RayCluster CR from the KubeRay Helm chart repo:\nhelm install raycluster kuberay/ray-cluster --version 0.6.0\n\n# Once the RayCluster CR has been created, you can view it by running:\nkubectl get rayclusters\n\n# NAME                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# raycluster-kuberay   1                 1                   ready    72s\n```\n\nThe KubeRay operator will detect the RayCluster object.The operator will then start your Ray cluster by creating head and worker pods.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "774a6a5a-3743-4677-ba88-6937a3282653": {"__data__": {"id_": "774a6a5a-3743-4677-ba88-6937a3282653", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "6f6c2f77df45001d0ff9b4914f4c0e02"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d44a40b06a831809a1a01e668d3def8d2333e2a6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md"}, "hash": "836f76d79e72651c249d4ce47856396a3e7253eb780db0aebe366a4815e7ffd6"}, "2": {"node_id": "f639521d-65a4-4474-b6f0-c4aff6d62a77", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "134fdd75f8224140ba58099b04195ab4"}, "hash": "8f092c2b231a15c045e0f9451dbede87a0456cd86dc4e4c19d98be8222900861"}, "3": {"node_id": "e0bbdec8-1489-482d-846f-65c98f3fa75a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "012bc808fa82c2ca98b64f04e6e08b04"}, "hash": "2dc364870a3dd60c09095431a17266fe410a980753ec99bac9db8e5efa397051"}}, "hash": "76b98eb63489dfad4894b19bf245deabe3019621639f32bbade02d25f6603cef", "text": "To view Ray cluster's pods, run the following command:\n\n```sh\n# View the pods in the RayCluster named \"raycluster-kuberay\"\nkubectl get pods --selector=ray.io/cluster=raycluster-kuberay\n\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# raycluster-kuberay-head-vkj4n                 1/1     Running   0          XXs\n# raycluster-kuberay-worker-workergroup-xvfkr   1/1     Running   0          XXs\n```\n\nWait for the pods to reach Running state.This may take a few minutes -- most of this time is spent downloading the Ray images.If your pods are stuck in the Pending state, you can check for errors via `kubectl describe pod raycluster-kuberay-xxxx-xxxxx` and ensure that your Docker resource limits are set high enough.Note that in production scenarios, you will want to use larger Ray pods.In fact, it is advantageous to size each Ray pod to take up an entire Kubernetes node.See the [configuration guide](kuberay-config) for more details.## Step 4: Run an application on a RayCluster\n\nNow, let's interact with the RayCluster we've deployed.### Method 1: Execute a Ray job in the head Pod\n\nThe most straightforward way to experiment with your RayCluster is to exec directly into the head pod.First, identify your RayCluster's head pod:\n\n```sh\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\necho $HEAD_POD\n# raycluster-kuberay-head-vkj4n\n\n# Print the cluster resources.kubectl exec -it $HEAD_POD -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n\n# 2023-04-07 10:57:46,472 INFO worker.py:1243 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS\n# 2023-04-07 10:57:46,472 INFO worker.py:1364 -- Connecting to existing Ray cluster at address: 10.244.0.6:6379...\n# 2023-04-07 10:57:46,482 INFO worker.py:1550 -- Connected to Ray cluster.View the dashboard at http://10.244.0.6:8265 \n# {'object_store_memory': 802572287.0, 'memory': 3000000000.0, 'node:10.244.0.6': 1.0, 'CPU': 2.0, 'node:10.244.0.7': 1.0}\n```\n\n### Method 2: Submit a Ray job to the RayCluster via [ray job submission SDK](jobs-quickstart)\n\nUnlike Method 1, this method does not require you to execute commands in the Ray head pod.Instead, you can use the [Ray job submission SDK](jobs-quickstart) to submit Ray jobs to the RayCluster via the Ray Dashboard port (8265 by default) where Ray listens for Job requests.The KubeRay operator configures a [Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/) targeting the Ray head Pod.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e0bbdec8-1489-482d-846f-65c98f3fa75a": {"__data__": {"id_": "e0bbdec8-1489-482d-846f-65c98f3fa75a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "012bc808fa82c2ca98b64f04e6e08b04"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d44a40b06a831809a1a01e668d3def8d2333e2a6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md"}, "hash": "836f76d79e72651c249d4ce47856396a3e7253eb780db0aebe366a4815e7ffd6"}, "2": {"node_id": "774a6a5a-3743-4677-ba88-6937a3282653", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "6f6c2f77df45001d0ff9b4914f4c0e02"}, "hash": "76b98eb63489dfad4894b19bf245deabe3019621639f32bbade02d25f6603cef"}}, "hash": "2dc364870a3dd60c09095431a17266fe410a980753ec99bac9db8e5efa397051", "text": "```sh\nkubectl get service raycluster-kuberay-head-svc\n\n# NAME                          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                                         AGE\n# raycluster-kuberay-head-svc   ClusterIP   10.96.93.74   <none>        8265/TCP,8080/TCP,8000/TCP,10001/TCP,6379/TCP   15m\n```\n\nNow that we have the name of the service, we can use port-forwarding to access the Ray Dashboard port (8265 by default).```sh\n# Execute this in a separate shell.kubectl port-forward --address 0.0.0.0 service/raycluster-kuberay-head-svc 8265:8265\n\n# Visit ${YOUR_IP}:8265 in your browser for the Dashboard (e.g.127.0.0.1:8265)\n```\n\nNote: We use port-forwarding in this guide as a simple way to experiment with a RayCluster's services.For production use-cases, you would typically either \n- Access the service from within the Kubernetes cluster or\n- Use an ingress controller to expose the service outside the cluster.See the {ref}`networking notes <kuberay-networking>` for details.Now that we have access to the Dashboard port, we can submit jobs to the RayCluster:\n\n```sh\n# The following job's logs will show the Ray cluster's total resource capacity, including 2 CPUs.ray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n## Step 5: Cleanup\n\n```sh\n# [Step 5.1]: Delete the RayCluster CR\n# Uninstall the RayCluster Helm chart\nhelm uninstall raycluster\n# release \"raycluster\" uninstalled\n\n# Note that it may take several seconds for the Ray pods to be fully terminated.# Confirm that the RayCluster's pods are gone by running\nkubectl get pods\n\n# NAME                                READY   STATUS    RESTARTS   AGE\n# kuberay-operator-7fbdbf8c89-pt8bk   1/1     Running   0          XXm\n\n# [Step 5.2]: Delete the KubeRay operator\n# Uninstall the KubeRay operator Helm chart\nhelm uninstall kuberay-operator\n# release \"kuberay-operator\" uninstalled\n\n# Confirm that the KubeRay operator pod is gone by running\nkubectl get pods\n# No resources found in default namespace.# [Step 5.3]: Delete the Kubernetes cluster\nkind delete cluster\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "438f481b-9c81-45c0-8772-f4779a35edb6": {"__data__": {"id_": "438f481b-9c81-45c0-8772-f4779a35edb6", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "01280a0e6637466fd3ed5ec13f54ea10"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86171dc5b178459bde22346e99afb9e484eb2b29", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md"}, "hash": "459ee5b3df06460e2a739f851edafd705e2a1fdf3626dd649fadf52401383b44"}, "3": {"node_id": "68517bd2-bcee-4082-83be-4d0df05c58ff", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "dd435f2d6046ddbd90b2d19e53cfa072"}, "hash": "88aca3c6dc8aa8874de4aec8bf85b635efc268254a9f45b5e1233163bca5c031"}}, "hash": "5d2d1d78ab59ea67a2a0062b5ad06a6ae2598d8511a38f251bec67f0eb058c6c", "text": "(kuberay-rayjob-quickstart)=\n\n# RayJob\n\n:::{warning}\nThis is the alpha version of RayJob Support in KubeRay.There will be ongoing improvements for RayJob in the future releases.:::\n\n## Prerequisites\n\n* Ray 1.10 or higher\n* KubeRay v0.3.0+.(v0.6.0+ is recommended)\n\n## What is a RayJob?A RayJob manages two aspects:\n\n* **RayCluster**: Manages resources in a Kubernetes cluster.* **Job**: A Kubernetes Job runs `ray job submit` to submit a Ray job to the RayCluster.## What does the RayJob provide?* **Kubernetes-native support for Ray clusters and Ray jobs**: You can use a Kubernetes config to define a Ray cluster and job, and use `kubectl` to create them.The cluster can be deleted automatically once the job is finished.## RayJob Configuration\n\n* `entrypoint` - The shell command to run for this job.* `rayClusterSpec` - The spec for the **RayCluster** to run the job on.* `jobId` - _(Optional)_ Job ID to specify for the job.If not provided, one will be generated.* `metadata` - _(Optional)_ Arbitrary user-provided metadata for the job.* `runtimeEnv` - _(Optional)_ base64-encoded string of the runtime env json string.* `shutdownAfterJobFinishes` - _(Optional)_ whether to recycle the cluster after the job finishes.Defaults to false.* `ttlSecondsAfterFinished` - _(Optional)_ TTL to clean up the cluster.This only works if `shutdownAfterJobFinishes` is set.* `submitterPodTemplate` - _(Optional)_ Pod template spec for the pod that runs `ray job submit` against the Ray cluster.## Example: Run a simple Ray job with RayJob\n\n## Step 1: Create a Kubernetes cluster with Kind\n\n```sh\nkind create cluster --image=kindest/node:v1.23.0\n```\n\n## Step 2: Install the KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.Please note that the YAML file in this example uses `serveConfigV2` to specify a multi-application Serve config, which is supported starting from KubeRay v0.6.0.## Step 3: Install a RayJob\n\n```sh\n# Step 3.1: Download `ray_v1alpha1_rayjob.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray_v1alpha1_rayjob.yaml\n\n# Step 3.2: Create a RayJob\nkubectl apply -f ray_v1alpha1_rayjob.yaml\n```\n\n## Step 4: Verify the Kubernetes cluster status\n\n```shell\n# Step 4.1: List all RayJob custom resources in the `default` namespace.kubectl get rayjob\n\n# [Example output]\n# NAME            AGE\n# rayjob-sample   7s\n\n# Step 4.2: List all RayCluster custom resources in the `default` namespace.kubectl get raycluster\n\n# [Example output]\n# NAME                                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# rayservice-sample-raycluster-6mj28   1                 1                   ready    2m27s\n\n# Step 4.3: List all Pods in the `default` namespace.# The Pod created by the Kubernetes Job will be terminated after the Kubernetes Job finishes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "68517bd2-bcee-4082-83be-4d0df05c58ff": {"__data__": {"id_": "68517bd2-bcee-4082-83be-4d0df05c58ff", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "dd435f2d6046ddbd90b2d19e53cfa072"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86171dc5b178459bde22346e99afb9e484eb2b29", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md"}, "hash": "459ee5b3df06460e2a739f851edafd705e2a1fdf3626dd649fadf52401383b44"}, "2": {"node_id": "438f481b-9c81-45c0-8772-f4779a35edb6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "01280a0e6637466fd3ed5ec13f54ea10"}, "hash": "5d2d1d78ab59ea67a2a0062b5ad06a6ae2598d8511a38f251bec67f0eb058c6c"}, "3": {"node_id": "1c21c17d-8b75-467b-8ca2-52413d50981b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "55a19f5936af2cb00bdd6f5378289d4a"}, "hash": "7fbffe288045905f7707a1e38c469c55df10c8fa2961df603e1cd25fe24fd017"}}, "hash": "88aca3c6dc8aa8874de4aec8bf85b635efc268254a9f45b5e1233163bca5c031", "text": "kubectl get pods\n\n# [Example output]\n# kuberay-operator-7456c6b69b-rzv25                         1/1     Running     0          3m57s\n# rayjob-sample-lk9jx                                       0/1     Completed   0          2m49s => Pod created by a Kubernetes Job\n# rayjob-sample-raycluster-9c546-head-gdxkg                 1/1     Running     0          3m46s\n# rayjob-sample-raycluster-9c546-worker-small-group-nfbxm   1/1     Running     0          3m46s\n\n# Step 4.4: Check the status of the RayJob.# The field `jobStatus` in the RayJob custom resource will be updated to `SUCCEEDED` once the job finishes.kubectl get rayjobs.ray.io rayjob-sample -o json | jq '.status.jobStatus'\n\n# [Example output]\n# \"SUCCEEDED\"\n```\n\nThe KubeRay operator will create a RayCluster as defined by the `rayClusterSpec` custom resource, as well as a Kubernetes Job to submit a Ray job to the RayCluster.The Ray job is defined in the `entrypoint` field of the RayJob custom resource.In this example, the `entrypoint` is `python /home/ray/samples/sample_code.py`,\nand `sample_code.py` is a Python script stored in a Kubernetes ConfigMap mounted to the head Pod of the RayCluster.Since the default value of `shutdownAfterJobFinishes` is false, the RayCluster will not be deleted after the job finishes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1c21c17d-8b75-467b-8ca2-52413d50981b": {"__data__": {"id_": "1c21c17d-8b75-467b-8ca2-52413d50981b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "55a19f5936af2cb00bdd6f5378289d4a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "86171dc5b178459bde22346e99afb9e484eb2b29", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md"}, "hash": "459ee5b3df06460e2a739f851edafd705e2a1fdf3626dd649fadf52401383b44"}, "2": {"node_id": "68517bd2-bcee-4082-83be-4d0df05c58ff", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "dd435f2d6046ddbd90b2d19e53cfa072"}, "hash": "88aca3c6dc8aa8874de4aec8bf85b635efc268254a9f45b5e1233163bca5c031"}}, "hash": "7fbffe288045905f7707a1e38c469c55df10c8fa2961df603e1cd25fe24fd017", "text": "## Step 5: Check the output of the Ray job\n\n```sh\nkubectl logs -l=job-name=rayjob-sample\n\n# [Example output]\n# 2023-08-21 17:08:22,530 INFO cli.py:27 -- Job submission server address: http://rayjob-sample-raycluster-9c546-head-svc.default.svc.cluster.local:8265\n# 2023-08-21 17:08:23,726 SUCC cli.py:33 -- ------------------------------------------------\n# 2023-08-21 17:08:23,727 SUCC cli.py:34 -- Job 'rayjob-sample-5ntcr' submitted successfully\n# 2023-08-21 17:08:23,727 SUCC cli.py:35 -- ------------------------------------------------\n# 2023-08-21 17:08:23,727 INFO cli.py:226 -- Next steps\n# 2023-08-21 17:08:23,727 INFO cli.py:227 -- Query the logs of the job:\n# 2023-08-21 17:08:23,727 INFO cli.py:229 -- ray job logs rayjob-sample-5ntcr\n# 2023-08-21 17:08:23,727 INFO cli.py:231 -- Query the status of the job:\n# 2023-08-21 17:08:23,727 INFO cli.py:233 -- ray job status rayjob-sample-5ntcr\n# 2023-08-21 17:08:23,727 INFO cli.py:235 -- Request the job to be stopped:\n# 2023-08-21 17:08:23,728 INFO cli.py:237 -- ray job stop rayjob-sample-5ntcr\n# 2023-08-21 17:08:23,739 INFO cli.py:245 -- Tailing logs until the job exits (disable with --no-wait):\n# 2023-08-21 17:08:34,288 INFO worker.py:1335 -- Using address 10.244.0.6:6379 set in the environment variable RAY_ADDRESS\n# 2023-08-21 17:08:34,288 INFO worker.py:1452 -- Connecting to existing Ray cluster at address: 10.244.0.6:6379...\n# 2023-08-21 17:08:34,302 INFO worker.py:1633 -- Connected to Ray cluster.View the dashboard at http://10.244.0.6:8265\n# test_counter got 1\n# test_counter got 2\n# test_counter got 3\n# test_counter got 4\n# test_counter got 5\n# 2023-08-21 17:08:46,040 SUCC cli.py:33 -- -----------------------------------\n# 2023-08-21 17:08:46,040 SUCC cli.py:34 -- Job 'rayjob-sample-5ntcr' succeeded\n# 2023-08-21 17:08:46,040 SUCC cli.py:35 -- -----------------------------------\n```\n\nThe Python script `sample_code.py` used by `entrypoint` is a simple Ray script that executes a counter's increment function 5 times.\n\n\n## Step 6: Cleanup\n\n```sh\n# Step 6.1: Delete the RayJob\nkubectl delete -f ray_v1alpha1_rayjob.yaml\n\n# Step 6.2: Delete the KubeRay operator\nhelm uninstall kuberay-operator\n\n# Step 6.3: Delete the Kubernetes cluster\nkind delete cluster\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "563f9c64-b857-46d4-be44-db12cdb17e38": {"__data__": {"id_": "563f9c64-b857-46d4-be44-db12cdb17e38", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "99f3b0724bc0c1024e06a1447296baae"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md"}, "hash": "aa01bb99fca8d7ee8e6cb5c8c4a7c2b380fea8adb9e18d2b839278cd12baece4"}, "3": {"node_id": "3ea219e7-30a4-48b8-afa1-3abd52244608", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "66583d34b56b1ae5bd49516de86e24da"}, "hash": "ca136f7c9398583583a36c94e810f79b4a23793210050b9a2fd36865f9074ff7"}}, "hash": "67367b947c0c1efd882610d1087c9b5fb7ae489fa7bf5d386bc5d9713d2731e4", "text": "(kuberay-rayservice-quickstart)=\n\n# RayService\n\n## Prerequisites\n\nThis guide focuses solely on the Ray Serve multi-application API, which is available starting from Ray version 2.4.0.* Ray 2.4.0 or newer.* KubeRay 0.6.0, KubeRay nightly, or newer.## What is a RayService?A RayService manages 2 things:\n\n* **RayCluster**: Manages resources in a Kubernetes cluster.* **Ray Serve Applications**: Manages users' applications.## What does the RayService provide?* **Kubernetes-native support for Ray clusters and Ray Serve applications:** After using a Kubernetes config to define a Ray cluster and its Ray Serve applications, you can use `kubectl` to create the cluster and its applications.* **In-place update for Ray Serve applications:** Users can update the Ray Serve config in the RayService CR config and use `kubectl apply` to update the applications.See [Step 7](#step-7-in-place-update-for-ray-serve-applications) for more details.* **Zero downtime upgrade for Ray clusters:** Users can update the Ray cluster config in the RayService CR config and use `kubectl apply` to update the cluster.RayService will temporarily create a pending cluster and wait for it to be ready, then switch traffic to the new cluster and terminate the old one.See [Step 8](#step-8-zero-downtime-upgrade-for-ray-clusters) for more details.* **Services HA:** RayService will monitor the Ray cluster and Serve deployments' health statuses.If RayService detects an unhealthy status for a period of time, RayService will try to create a new Ray cluster and switch traffic to the new cluster when it is ready.## Example: Serve two simple Ray Serve applications using RayService\n\n## Step 1: Create a Kubernetes cluster with Kind\n\n```sh\nkind create cluster --image=kindest/node:v1.23.0\n```\n\n## Step 2: Install the KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.Please note that the YAML file in this example uses `serveConfigV2` to specify a multi-application Serve config, which is supported starting from KubeRay v0.6.0.## Step 3: Install a RayService\n\n```sh\n# Step 3.1: Download `ray_v1alpha1_rayservice.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray_v1alpha1_rayservice.yaml\n\n# Step 3.2: Create a RayService\nkubectl apply -f ray_v1alpha1_rayservice.yaml\n```\n\n* Let's first take a look at the Ray Serve config (i.e.`serveConfigV2`) embedded in the RayService YAML.At a high level, there are two applications: a fruit stand app and a calculator app.Some details about the fruit stand application:\n  * The fruit stand application is contained in the `deployment_graph` variable in `fruit.py` in the [test_dag](https://github.com/ray-project/test_dag/tree/41d09119cbdf8450599f993f51318e9e27c59098) repo, so `import_path` in the config points to this variable to tell Serve from where to import the application.* It is hosted at the route prefix `/fruit`, meaning HTTP requests with routes that start with the prefix `/fruit` will be sent to the fruit stand application.* The working directory points to the [test_dag](https://github.com/ray-project/test_dag/tree/41d09119cbdf8450599f993f51318e9e27c59098) repo, which will be downloaded at runtime, and your application will be started in this directory.See {ref}`Runtime Environments <runtime-environments>`.for more details.* For more details on configuring Ray Serve deployments, see the [Ray Serve Documentation](https://docs.ray.io/en/master/serve/configure-serve-deployment.html).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3ea219e7-30a4-48b8-afa1-3abd52244608": {"__data__": {"id_": "3ea219e7-30a4-48b8-afa1-3abd52244608", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "66583d34b56b1ae5bd49516de86e24da"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md"}, "hash": "aa01bb99fca8d7ee8e6cb5c8c4a7c2b380fea8adb9e18d2b839278cd12baece4"}, "2": {"node_id": "563f9c64-b857-46d4-be44-db12cdb17e38", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "99f3b0724bc0c1024e06a1447296baae"}, "hash": "67367b947c0c1efd882610d1087c9b5fb7ae489fa7bf5d386bc5d9713d2731e4"}, "3": {"node_id": "d4abd0cd-f6dd-46d0-9662-63980fff1654", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "d62edad75e93a4a11ca017a0ad746680"}, "hash": "d02aaa7fbe879305fea966d241e353c860595347e00f67b4cc7d806d0e07482e"}}, "hash": "ca136f7c9398583583a36c94e810f79b4a23793210050b9a2fd36865f9074ff7", "text": "* Similarly, the calculator app is imported from the `conditional_dag.py` file in the same repo, and it's hosted at the route prefix `/calc`.```yaml\n  serveConfigV2: |\n    applications:\n      - name: fruit_app\n        import_path: fruit.deployment_graph\n        route_prefix: /fruit\n        runtime_env:\n          working_dir: \"https://github.com/ray-project/test_dag/archive/41d09119cbdf8450599f993f51318e9e27c59098.zip\"\n        deployments: ...\n      - name: math_app\n        import_path: conditional_dag.serve_dag\n        route_prefix: /calc\n        runtime_env:\n          working_dir: \"https://github.com/ray-project/test_dag/archive/41d09119cbdf8450599f993f51318e9e27c59098.zip\"\n        deployments: ...\n  ```\n\n## Step 4: Verify the Kubernetes cluster status \n\n```sh\n# Step 4.1: List all RayService custom resources in the `default` namespace.kubectl get rayservice\n\n# [Example output]\n# NAME                AGE\n# rayservice-sample   2m42s\n\n# Step 4.2: List all RayCluster custom resources in the `default` namespace.kubectl get raycluster\n\n# [Example output]\n# NAME                                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# rayservice-sample-raycluster-6mj28   1                 1                   ready    2m27s\n\n# Step 4.3: List all Ray Pods in the `default` namespace.kubectl get pods -l=ray.io/is-ray-node=yes\n\n# [Example output]\n# ervice-sample-raycluster-6mj28-worker-small-group-kg4v5   1/1     Running   0          3m52s\n# rayservice-sample-raycluster-6mj28-head-x77h4             1/1     Running   0          3m52s\n\n# Step 4.4: List services in the `default` namespace.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d4abd0cd-f6dd-46d0-9662-63980fff1654": {"__data__": {"id_": "d4abd0cd-f6dd-46d0-9662-63980fff1654", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "d62edad75e93a4a11ca017a0ad746680"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md"}, "hash": "aa01bb99fca8d7ee8e6cb5c8c4a7c2b380fea8adb9e18d2b839278cd12baece4"}, "2": {"node_id": "3ea219e7-30a4-48b8-afa1-3abd52244608", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "66583d34b56b1ae5bd49516de86e24da"}, "hash": "ca136f7c9398583583a36c94e810f79b4a23793210050b9a2fd36865f9074ff7"}, "3": {"node_id": "7ec5a158-8319-4c7a-8a3a-96ebc3248604", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "fd7c309c665910e85ec3d1ba4c55cad3"}, "hash": "e662857cbe3813ec4cfd30bc911cea3734e08a669cd15d6de66ddc14a92c6be6"}}, "hash": "d02aaa7fbe879305fea966d241e353c860595347e00f67b4cc7d806d0e07482e", "text": "kubectl get services\n\n# NAME                                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                                   AGE\n# ...\n# rayservice-sample-head-svc                    ClusterIP   10.96.34.90     <none>        10001/TCP,8265/TCP,52365/TCP,6379/TCP,8080/TCP,8000/TCP   4m58s\n# rayservice-sample-raycluster-6mj28-head-svc   ClusterIP   10.96.171.184   <none>        10001/TCP,8265/TCP,52365/TCP,6379/TCP,8080/TCP,8000/TCP   6m21s\n# rayservice-sample-serve-svc                   ClusterIP   10.96.161.84    <none>        8000/TCP                                                  4m58s\n```\n\nKubeRay will create a RayCluster based on `spec.rayClusterConfig` defined in the RayService YAML for a RayService custom resource.Next, after the head Pod is running and ready, KubeRay will submit a request to the head's dashboard agent port (default: 52365) to create the Ray Serve applications defined in `spec.serveConfigV2`.After the Ray Serve applications are healthy and ready, KubeRay will create a head service and a serve service for the RayService custom resource (e.g., `rayservice-sample-head-svc` and `rayservice-sample-serve-svc` in Step 4.4).Users can access the head Pod through both the head service managed by RayService (i.e.`rayservice-sample-head-svc`) and the head service managed by RayCluster (i.e.`rayservice-sample-raycluster-6mj28-head-svc`).However, during a zero downtime upgrade, a new RayCluster will be created, and a new head service will be created for the new RayCluster.If `rayservice-sample-head-svc` is not used, users will need to update their ingress configuration to point to the new head service.However, if `rayservice-sample-head-svc` is used, KubeRay will automatically update the selector to point to the new head Pod, eliminating the need for users to update their ingress configuration.\n\n\n> Note: Default ports and their definitions.| Port  | Definition          |\n|-------|---------------------|\n| 6379  | Ray GCS             |\n| 8265  | Ray Dashboard       |\n| 10001 | Ray Client          |\n| 8000  | Ray Serve           |\n| 52365 | Ray Dashboard Agent |\n\n## Step 5: Verify the status of the Serve applications\n\n```sh\n# Step 5.1: Check the status of the RayService.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7ec5a158-8319-4c7a-8a3a-96ebc3248604": {"__data__": {"id_": "7ec5a158-8319-4c7a-8a3a-96ebc3248604", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "fd7c309c665910e85ec3d1ba4c55cad3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md"}, "hash": "aa01bb99fca8d7ee8e6cb5c8c4a7c2b380fea8adb9e18d2b839278cd12baece4"}, "2": {"node_id": "d4abd0cd-f6dd-46d0-9662-63980fff1654", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "d62edad75e93a4a11ca017a0ad746680"}, "hash": "d02aaa7fbe879305fea966d241e353c860595347e00f67b4cc7d806d0e07482e"}, "3": {"node_id": "07012c0b-e785-446f-b163-fd1ad04b1f14", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "f5b4c8b4ccbf9d18b8dbc4dbe96aa2a8"}, "hash": "a090e0ded09e11ed434dcaec441c909cfaf54cf180b1cd61e315f63ff81b3feb"}}, "hash": "e662857cbe3813ec4cfd30bc911cea3734e08a669cd15d6de66ddc14a92c6be6", "text": "kubectl describe rayservices rayservice-sample\n\n# Active Service Status:\n#   Application Statuses:\n#     fruit_app:\n#       Health Last Update Time:  2023-07-11T22:21:24Z\n#       Last Update Time:         2023-07-11T22:21:24Z\n#       Serve Deployment Statuses:\n#         fruit_app_DAGDriver:\n#           Health Last Update Time:  2023-07-11T22:21:24Z\n#           Last Update Time:         2023-07-11T22:21:24Z\n#           Status:                   HEALTHY\n#         fruit_app_FruitMarket:\n#           ...\n#       Status:                       RUNNING\n#     math_app:\n#       Health Last Update Time:  2023-07-11T22:21:24Z\n#       Last Update Time:         2023-07-11T22:21:24Z\n#       Serve Deployment Statuses:\n#         math_app_Adder:\n#           Health Last Update Time:  2023-07-11T22:21:24Z\n#           Last Update Time:         2023-07-11T22:21:24Z\n#           Status:                   HEALTHY\n#         math_app_DAGDriver:\n#           ...\n#       Status:                       RUNNING\n\n# Step 5.2: Check the Serve applications in the Ray dashboard.# (1) Forward the dashboard port to localhost.# (2) Check the Serve page in the Ray dashboard at http://localhost:8265/#/serve.kubectl port-forward svc/rayservice-sample-head-svc --address 0.0.0.0 8265:8265\n```\n\n* Refer to [rayservice-troubleshooting.md](https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayservice-troubleshooting.md#observability) for more details on RayService observability.Below is a screenshot example of the Serve page in the Ray dashboard.![Ray Serve Dashboard](../images/dashboard_serve.png)\n\n## Step 6: Send requests to the Serve applications via the Kubernetes serve service\n\n```sh\n# Step 6.1: Run a curl Pod.# If you already have a curl Pod, you can use `kubectl exec -it <curl-pod> -- sh` to access the Pod.kubectl run curl --image=radial/busyboxplus:curl -i --tty\n\n# Step 6.2: Send a request to the fruit stand app.curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\n# [Expected output]: 6\n\n# Step 6.3: Send a request to the calculator app.curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/calc/ -d '[\"MUL\", 3]'\n# [Expected output]: \"15 pizzas please!\"```\n\n* `rayservice-sample-serve-svc` is HA in general.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "07012c0b-e785-446f-b163-fd1ad04b1f14": {"__data__": {"id_": "07012c0b-e785-446f-b163-fd1ad04b1f14", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "f5b4c8b4ccbf9d18b8dbc4dbe96aa2a8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md"}, "hash": "aa01bb99fca8d7ee8e6cb5c8c4a7c2b380fea8adb9e18d2b839278cd12baece4"}, "2": {"node_id": "7ec5a158-8319-4c7a-8a3a-96ebc3248604", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "fd7c309c665910e85ec3d1ba4c55cad3"}, "hash": "e662857cbe3813ec4cfd30bc911cea3734e08a669cd15d6de66ddc14a92c6be6"}, "3": {"node_id": "1cac615e-3e13-4380-a358-d489653e42ae", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "8001d34697a3c2b6114f0bf9b095a38e"}, "hash": "e3cf1b0a7de0241b0aee9b9098d3ebf8c71d0baf4b6103b3fea421cbef331a46"}}, "hash": "a090e0ded09e11ed434dcaec441c909cfaf54cf180b1cd61e315f63ff81b3feb", "text": "It will do traffic routing among all the workers which have Serve deployments and will always try to point to the healthy cluster, even during upgrading or failing cases.## Step 7: In-place update for Ray Serve applications\n\nYou can update the configurations for the applications by modifying `serveConfigV2` in the RayService config file.Re-applying the modified config with `kubectl apply` will re-apply the new configurations to the existing RayCluster instead of creating a new RayCluster.Let's try it out.Update the price of mangos from `3` to `4` for the fruit stand app in [ray_v1alpha1_rayservice.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray_v1alpha1_rayservice.yaml).This will reconfigure the existing MangoStand deployment, and future requests will use the updated Mango price.```sh\n# Step 7.1: Update the price of mangos from 3 to 4.# [ray_v1alpha1_rayservice.yaml]\n# - name: MangoStand\n#   num_replicas: 1\n#   user_config:\n#     price: 4\n\n# Step 7.2: Apply the updated RayService config.kubectl apply -f ray_v1alpha1_rayservice.yaml\n\n# Step 7.3: Check the status of the RayService.kubectl describe rayservices rayservice-sample\n# [Example output]\n# Serve Deployment Statuses:\n# - healthLastUpdateTime: \"2023-07-11T23:50:13Z\"\n#   lastUpdateTime: \"2023-07-11T23:50:13Z\"\n#   name: MangoStand\n#   status: UPDATING\n\n# Step 7.4: Send a request to the fruit stand app again after the Serve deployment status changes from UPDATING to HEALTHY.# (Execute the command in the curl Pod from Step 6)\ncurl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\n# [Expected output]: 8\n```\n\n## Step 8: Zero downtime upgrade for Ray clusters\n\nIn Step 7, modifying `serveConfigV2` will not trigger a zero downtime upgrade for Ray clusters.Instead, it will reapply the new configurations to the existing RayCluster.However, if you modify `spec.rayClusterConfig` in the RayService YAML file, it will trigger a zero downtime upgrade for Ray clusters.To elaborate, RayService will temporarily create a new RayCluster and wait for it to be ready, then switch traffic to the new RayCluster by updating the selector of the head service managed by RayService (i.e.`rayservice-sample-head-svc`) and terminate the old one.To elaborate, during the zero downtime upgrade process, RayService will create a new RayCluster temporarily and wait for it to become ready.Once the new RayCluster is ready, RayService will update the selector of the head service managed by RayService (i.e., `rayservice-sample-head-svc`) to point to the new RayCluster to switch the traffic to the new RayCluster.Finally, the old RayCluster will be terminated.There are certain exceptions that will not trigger a zero downtime upgrade.Currently, only the fields managed by Ray Autoscaler, such as `replicas` and `scaleStrategy.workersToDelete`, will not trigger a zero downtime upgrade.When you update these fields, KubeRay will not propagate the update from RayService to RayCluster custom resources, so nothing will happen.```sh\n# Step 8.1: Update `spec.rayClusterConfig.workerGroupSpecs[0].replicas` in the RayService YAML file from 1 to 2.# This field is an exception that will not trigger a zero downtime upgrade, and nothing will happen.kubectl apply -f ray_v1alpha1_rayservice.yaml\n\n# Step 8.2: Check RayService CR\nkubectl describe rayservices rayservice-sample\n# Worker Group Specs:\n#   ...\n#   Replicas:  2\n\n# Step 8.3: Check RayCluster CR.The update is not propagated to the RayCluster CR.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1cac615e-3e13-4380-a358-d489653e42ae": {"__data__": {"id_": "1cac615e-3e13-4380-a358-d489653e42ae", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "8001d34697a3c2b6114f0bf9b095a38e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md"}, "hash": "aa01bb99fca8d7ee8e6cb5c8c4a7c2b380fea8adb9e18d2b839278cd12baece4"}, "2": {"node_id": "07012c0b-e785-446f-b163-fd1ad04b1f14", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "f5b4c8b4ccbf9d18b8dbc4dbe96aa2a8"}, "hash": "a090e0ded09e11ed434dcaec441c909cfaf54cf180b1cd61e315f63ff81b3feb"}}, "hash": "e3cf1b0a7de0241b0aee9b9098d3ebf8c71d0baf4b6103b3fea421cbef331a46", "text": "kubectl describe rayclusters $YOUR_RAY_CLUSTER\n# Worker Group Specs:\n#   ...\n#   Replicas:  1\n\n# Step 8.4: Update `spec.rayClusterConfig.rayVersion` to `2.100.0`.# This field is used to determine the Autoscaler sidecar image, and it will trigger a zero downtime upgrade.kubectl apply -f ray_v1alpha1_rayservice.yaml\n\n# Step 8.5: List all RayCluster custom resources in the `default` namespace.# Note that the new RayCluster is created based on the updated RayService config, so it will have 2 workers.kubectl get raycluster\n\n# NAME                                 DESIRED WORKERS   AVAILABLE WORKERS   STATUS   AGE\n# rayservice-sample-raycluster-6mj28   1                 1                   ready    142m\n# rayservice-sample-raycluster-sjj67   2                 2                   ready    44s\n\n# Step 8.6: Wait for the old RayCluster to be terminated.# Step 8.7: Submit a request to the fruit stand app via the same serve service.curl -X POST -H 'Content-Type: application/json' rayservice-sample-serve-svc:8000/fruit/ -d '[\"MANGO\", 2]'\n# [Expected output]: 8\n```\n\n### Another two possible scenarios that will trigger a new RayCluster preparation\n\n> Note: The following behavior is for KubeRay v0.6.2 or newer.For older versions, please refer to [kuberay#1293](https://github.com/ray-project/kuberay/pull/1293) for more details.Not only will the zero downtime upgrade trigger a new RayCluster preparation, but KubeRay will also trigger it if it considers a RayCluster unhealthy.In the RayService, KubeRay can mark a RayCluster as unhealthy in two possible scenarios.* Case 1: The KubeRay operator cannot connect to the dashboard agent on the head Pod for more than the duration defined by the `deploymentUnhealthySecondThreshold` parameter.Both the default value and values in sample YAML files of `deploymentUnhealthySecondThreshold` are 300 seconds.* Case 2: The KubeRay operator will mark a RayCluster as unhealthy if the status of a serve application is `DEPLOY_FAILED` or `UNHEALTHY` for a duration exceeding the `serviceUnhealthySecondThreshold` parameter.Both the default value and values in sample YAML files of `serviceUnhealthySecondThreshold` are 900 seconds.After KubeRay marks a RayCluster as unhealthy, it initiates the creation of a new RayCluster.Once the new RayCluster is ready, KubeRay redirects network traffic to it, and subsequently deletes the old RayCluster.## Step 9: Clean up the Kubernetes cluster\n\n```sh\n# Delete the RayService.kubectl delete -f ray_v1alpha1_rayservice.yaml\n\n# Uninstall the KubeRay operator.helm uninstall kuberay-operator\n\n# Delete the curl Pod.kubectl delete pod curl\n```\n\n## Next steps\n\n* Check the [RayService troubleshooting guide](kuberay-raysvc-troubleshoot) if you encounter any issues.* Check [Examples](kuberay-examples) for more RayService examples.The [MobileNet example](kuberay-mobilenet-rayservice-example) is a good example to start with because it does not require GPUs and is easy to run on a local machine.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "480d30ed-654f-43e4-b9ef-f280bd1f24b3": {"__data__": {"id_": "480d30ed-654f-43e4-b9ef-f280bd1f24b3", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md", "text_hash": "cb95c1ad92e812dc492200dfd2fb46b0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "451575bfb68af963c2c68ec33a13fc699f1a5403", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md"}, "hash": "eaa9b4623b340bb06ab60d04a670a435359a8de9438673e14ed94ab3a398cd6c"}, "3": {"node_id": "a1528e65-4c98-4402-a654-ca94bab81724", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md", "text_hash": "ea9f02fadd838ec1c63a54fe19820f6e"}, "hash": "d0e9bbe519a7fa6d808bba348fd0434ba52d53ec52b0e93bbebfdf38294684ea"}}, "hash": "4c05edc1bfa6e10160c7112747871b9c37077ee4fa09d133f82259259d758d66", "text": "# Ray on Kubernetes\n(kuberay-index)=\n## Overview\n\nIn this section we cover how to execute your distributed Ray programs on a Kubernetes cluster.Using the [KubeRay Operator](https://github.com/ray-project/kuberay) is the\nrecommended way to do so.The operator provides a Kubernetes-native way to manage Ray clusters.Each Ray cluster consists of a head node pod and a collection of worker node pods.Optional\nautoscaling support allows the KubeRay Operator to size your Ray clusters according to the\nrequirements of your Ray workload, adding and removing Ray pods as needed.KubeRay supports\nheterogenous compute nodes (including GPUs) as well as running multiple Ray clusters with\ndifferent Ray versions in the same Kubernetes cluster.```{eval-rst}\n.. image:: images/ray_on_kubernetes.png\n    :align: center\n..\n  Find source document here: https://docs.google.com/drawings/d/1E3FQgWWLuj8y2zPdKXjoWKrfwgYXw6RV_FWRwK8dVlg/edit\n```\n\nKubeRay introduces three distinct Kubernetes Custom Resource Definitions (CRDs): **RayCluster**, **RayJob**, and **RayService**.These CRDs assist users in efficiently managing Ray clusters tailored to various use cases.See [Getting Started](kuberay-quickstart) to learn the basics of KubeRay and follow the quickstart guides to run your first Ray application on Kubernetes with KubeRay.* [RayCluster Quick Start](kuberay-raycluster-quickstart)\n* [RayJob Quick Start](kuberay-rayjob-quickstart)\n* [RayService Quick Start](kuberay-rayservice-quickstart)\n\n## Learn More\n\nThe Ray docs present all the information you need to start running Ray workloads on Kubernetes.```{eval-rst}\n.. grid:: 1 2 2 2\n    :gutter: 1\n    :class-container: container pb-3\n    \n    .. grid-item-card::\n\n        **Getting Started**\n        ^^^\n    \n        Learn how to start a Ray cluster and deploy Ray applications on Kubernetes.+++\n        .. button-ref:: kuberay-quickstart\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray on Kubernetes\n\n    .. grid-item-card::\n\n        **User Guides**\n        ^^^\n    \n        Learn best practices for configuring Ray clusters on Kubernetes.+++\n        .. button-ref:: kuberay-guides\n            :color: primary\n            :outline:\n            :expand:\n\n            Read the User Guides\n\n    .. grid-item-card::\n\n        **Examples**\n        ^^^\n    \n        Try example Ray workloads on Kubernetes.+++\n        .. button-ref:: kuberay-examples\n            :color: primary\n            :outline:\n            :expand:\n\n            Try example workloads\n\n    .. grid-item-card::\n\n        **Ecosystem**\n        ^^^\n    \n        Integrate KubeRay with third party Kubernetes ecosystem tools.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1528e65-4c98-4402-a654-ca94bab81724": {"__data__": {"id_": "a1528e65-4c98-4402-a654-ca94bab81724", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md", "text_hash": "ea9f02fadd838ec1c63a54fe19820f6e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "451575bfb68af963c2c68ec33a13fc699f1a5403", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md"}, "hash": "eaa9b4623b340bb06ab60d04a670a435359a8de9438673e14ed94ab3a398cd6c"}, "2": {"node_id": "480d30ed-654f-43e4-b9ef-f280bd1f24b3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md", "text_hash": "cb95c1ad92e812dc492200dfd2fb46b0"}, "hash": "4c05edc1bfa6e10160c7112747871b9c37077ee4fa09d133f82259259d758d66"}}, "hash": "d0e9bbe519a7fa6d808bba348fd0434ba52d53ec52b0e93bbebfdf38294684ea", "text": "+++\n        .. button-ref:: kuberay-ecosystem-integration\n            :color: primary\n            :outline:\n            :expand:\n\n            Ecosystem Guides\n\n    .. grid-item-card::\n\n        **Benchmarks**\n        ^^^\n    \n        Check the KubeRay benchmark results.+++\n        .. button-ref:: kuberay-benchmarks\n            :color: primary\n            :outline:\n            :expand:\n\n            Benchmark results\n    \n    .. grid-item-card::\n\n        **Troubleshooting**\n        ^^^\n    \n        Consult the KubeRay troubleshooting guides.+++\n        .. button-ref:: kuberay-troubleshooting\n            :color: primary\n            :outline:\n            :expand:\n\n            Troubleshooting guides\n```\n## About KubeRay\n\nRay's Kubernetes support is developed at the [KubeRay GitHub repository](https://github.com/ray-project/kuberay), under the broader [Ray project](https://github.com/ray-project/).KubeRay is used by several companies to run production Ray deployments.- Visit the [KubeRay GitHub repo](https://github.com/ray-project/kuberay) to track progress, report bugs, propose new features, or contribute to\nthe project.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ccc2da2f-5f51-4f94-9bf9-9cb622fc8c2e": {"__data__": {"id_": "ccc2da2f-5f51-4f94-9bf9-9cb622fc8c2e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem.md", "file_name": "k8s-ecosystem.md", "text_hash": "84ed11f43398f275c450b217ef386adc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e5eee13cd4ac2d3eb43d00fc342865b54e6e870b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem.md", "file_name": "k8s-ecosystem.md"}, "hash": "a00d4be1061e6ce84a4a4c748b77a9b293dffee104d486ee9311f3d02d32d4eb"}}, "hash": "a00d4be1061e6ce84a4a4c748b77a9b293dffee104d486ee9311f3d02d32d4eb", "text": "(kuberay-ecosystem-integration)=\n\n# KubeRay Ecosystem\n\n* {ref}`kuberay-ingress`\n* {ref}`kuberay-prometheus-grafana`\n* {ref}`kuberay-pyspy-integration`\n* {ref}`kuberay-kubeflow-integration`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "afe46cdf-d27f-48f4-9f57-749cb98fdc7b": {"__data__": {"id_": "afe46cdf-d27f-48f4-9f57-749cb98fdc7b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "c00f9f5f86e8353d1229299631608819"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14280205ec65214ae56e2e49e685b225991a4c47", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md"}, "hash": "b34a7b3a19891e957b8528be03879a147d91ed37bf21d8bd9739f2495e67016b"}, "3": {"node_id": "f43b28fa-01ea-4f41-a64d-4331a2461f04", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "2222426c382f5d0f5d919eb95c1aeb23"}, "hash": "a912921d5f0d56d3e51c8edd30b38a59447655e4cca4a64f70938dabb4f36d30"}}, "hash": "3c6c5a2f4a5b6cb51b7639bcab51f59d9bcebd5f42e83009409ca8b10c6e029f", "text": "(kuberay-ingress)=\n\n# Ingress\n\nTwo examples show how to use ingress to access your Ray cluster:\n\n  * [AWS Application Load Balancer (ALB) Ingress support on AWS EKS](kuberay-aws-alb)\n  * [Manually setting up NGINX Ingress on Kind](kuberay-nginx)\n\n(kuberay-aws-alb)=\n## AWS Application Load Balancer (ALB) Ingress support on AWS EKS\n\n### Prerequisite\n* Create an EKS cluster.See [Getting started with Amazon EKS \u2013 AWS Management Console and AWS CLI](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html#eks-configure-kubectl).* Set up the [AWS Load Balancer controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller), see [installation instructions](https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/deploy/installation/).Note that the repository maintains a webpage for each release.Confirm that you are using the latest installation instructions.* (Optional) Try the [echo server example](https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/main/docs/examples/echo_server.md) in the [aws-load-balancer-controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller) repository.* (Optional) Read [how-it-works.md](https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/main/docs/how-it-works.md) to understand the [aws-load-balancer-controller](https://github.com/kubernetes-sigs/aws-load-balancer-controller) mechanism.### Instructions\n```sh\n# Step 1: Install KubeRay operator and CRD\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.6.0\n\n# Step 2: Install a RayCluster\nhelm install raycluster kuberay/ray-cluster --version 0.6.0\n\n# Step 3: Edit the `ray-operator/config/samples/ray-cluster-alb-ingress.yaml`\n#\n# (1) Annotation `alb.ingress.kubernetes.io/subnets`\n#   1.Please include at least two subnets.#   2.One Availability Zone (ex: us-west-2a) can only have at most 1 subnet.#   3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f43b28fa-01ea-4f41-a64d-4331a2461f04": {"__data__": {"id_": "f43b28fa-01ea-4f41-a64d-4331a2461f04", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "2222426c382f5d0f5d919eb95c1aeb23"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14280205ec65214ae56e2e49e685b225991a4c47", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md"}, "hash": "b34a7b3a19891e957b8528be03879a147d91ed37bf21d8bd9739f2495e67016b"}, "2": {"node_id": "afe46cdf-d27f-48f4-9f57-749cb98fdc7b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "c00f9f5f86e8353d1229299631608819"}, "hash": "3c6c5a2f4a5b6cb51b7639bcab51f59d9bcebd5f42e83009409ca8b10c6e029f"}, "3": {"node_id": "319c8e5d-8e00-4ffb-baac-eb379d68ef32", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "0d66e85a2a172e48b84966d113241edb"}, "hash": "c6ed89e8e26300e1e3bcaf6bca3d664354f908a52f26f0b0137f8f96069ddc49"}}, "hash": "a912921d5f0d56d3e51c8edd30b38a59447655e4cca4a64f70938dabb4f36d30", "text": "In this example, you need to select public subnets (subnets that \"Auto-assign public IPv4 address\" is Yes on AWS dashboard)\n#\n# (2) Set the name of head pod service to `spec...backend.service.name`\neksctl get cluster ${YOUR_EKS_CLUSTER} # Check subnets on the EKS cluster\n\n# Step 4: Check ingress created by Step 4.\nkubectl describe ingress ray-cluster-ingress\n\n# [Example]\n# Name:             ray-cluster-ingress\n# Labels:           <none>\n# Namespace:        default\n# Address:          k8s-default-rayclust-....${REGION_CODE}.elb.amazonaws.com\n# Default backend:  default-http-backend:80 (<error: endpoints \"default-http-backend\" not found>)\n# Rules:\n#  Host        Path  Backends\n#  ----        ----  --------\n#  *\n#              /   ray-cluster-kuberay-head-svc:8265 (192.168.185.157:8265)\n# Annotations: alb.ingress.kubernetes.io/scheme: internet-facing\n#              alb.ingress.kubernetes.io/subnets: ${SUBNET_1},${SUBNET_2}\n#              alb.ingress.kubernetes.io/tags: Environment=dev,Team=test\n#              alb.ingress.kubernetes.io/target-type: ip\n# Events:\n#   Type    Reason                  Age   From     Message\n#   ----    ------                  ----  ----     -------\n#   Normal  SuccessfullyReconciled  39m   ingress  Successfully reconciled\n\n# Step 6: Check ALB on AWS (EC2 -> Load Balancing -> Load Balancers)\n#        The name of the ALB should be like \"k8s-default-rayclust-......\".# Step 7: Check Ray Dashboard by ALB DNS Name.The name of the DNS Name should be like\n#        \"k8s-default-rayclust-.....us-west-2.elb.amazonaws.com\"\n\n# Step 8: Delete the ingress, and AWS Load Balancer controller will remove ALB.#        Check ALB on AWS to make sure it is removed.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "319c8e5d-8e00-4ffb-baac-eb379d68ef32": {"__data__": {"id_": "319c8e5d-8e00-4ffb-baac-eb379d68ef32", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "0d66e85a2a172e48b84966d113241edb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "14280205ec65214ae56e2e49e685b225991a4c47", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md"}, "hash": "b34a7b3a19891e957b8528be03879a147d91ed37bf21d8bd9739f2495e67016b"}, "2": {"node_id": "f43b28fa-01ea-4f41-a64d-4331a2461f04", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "2222426c382f5d0f5d919eb95c1aeb23"}, "hash": "a912921d5f0d56d3e51c8edd30b38a59447655e4cca4a64f70938dabb4f36d30"}}, "hash": "c6ed89e8e26300e1e3bcaf6bca3d664354f908a52f26f0b0137f8f96069ddc49", "text": "kubectl delete ingress ray-cluster-ingress\n```\n\n(kuberay-nginx)=\n## Manually setting up NGINX Ingress on Kind\n\n```sh\n# Step 1: Create a Kind cluster with `extraPortMappings` and `node-labels`\n# Reference for the setting up of Kind cluster: https://kind.sigs.k8s.io/docs/user/ingress/\ncat <<EOF | kind create cluster --config=-\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n  kubeadmConfigPatches:\n  - |\n    kind: InitConfiguration\n    nodeRegistration:\n      kubeletExtraArgs:\n        node-labels: \"ingress-ready=true\"\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n    protocol: TCP\n  - containerPort: 443\n    hostPort: 443\n    protocol: TCP\nEOF\n\n# Step 2: Install NGINX ingress controller\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml\nsleep 10 # Wait for the Kubernetes API Server to create the related resources\nkubectl wait --namespace ingress-nginx \\\n  --for=condition=ready pod \\\n  --selector=app.kubernetes.io/component=controller \\\n  --timeout=90s\n\n# Step 3: Install KubeRay operator and CRD\nhelm repo add kuberay https://ray-project.github.io/kuberay-helm/\nhelm install kuberay-operator kuberay/kuberay-operator --version 0.6.0\n\n# Step 4: Install RayCluster and create an ingress separately.# More information about change of setting was documented in https://github.com/ray-project/kuberay/pull/699 \n# and `ray-operator/config/samples/ray-cluster.separate-ingress.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.separate-ingress.yaml\nkubectl apply -f ray-operator/config/samples/ray-cluster.separate-ingress.yaml\n\n# Step 5: Check the ingress created in Step 4.\nkubectl describe ingress raycluster-ingress-head-ingress\n\n# [Example]\n# ...\n# Rules:\n# Host        Path  Backends\n# ----        ----  --------\n# *\n#             /raycluster-ingress/(.*)   raycluster-ingress-head-svc:8265 (10.244.0.11:8265)\n# Annotations:  nginx.ingress.kubernetes.io/rewrite-target: /$1\n\n# Step 6: Check `<ip>/raycluster-ingress/` on your browser.You will see the Ray Dashboard.#        [Note] The forward slash at the end of the address is necessary.`<ip>/raycluster-ingress`\n#               will report \"404 Not Found\".```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9e832df2-03ce-48de-b8e3-624545c04a4e": {"__data__": {"id_": "9e832df2-03ce-48de-b8e3-624545c04a4e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md", "text_hash": "d0e194f1ab53bb066b619315af3bdd56"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e94ae1e47b404478422d0c5de96ec8b49bc63aa8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md"}, "hash": "f7950699d660c566b3f3632d165eeca2756bdc0267c4c70aa08550646a8d17dd"}, "3": {"node_id": "d83a5888-ad90-4410-b1dd-4498a4603a7c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md", "text_hash": "107e92da2b7d0ec40312e0f432ab951a"}, "hash": "e8b04d80a690b74a1b75f80ecc71662c5a1d160afb225b8f378cd3938c247111"}}, "hash": "e3f4f506355e5f75162664f14d73bb2030271565ee68cc0fa12966b421640ae6", "text": "(kuberay-kubeflow-integration)=\n\n# Kubeflow: an interactive development solution\n\n<!-- TODO(kevin85421): Update Ray versions and replace Ray client with the Ray Job Submission -->\n\n> Credit: This manifest refers a lot to the engineering blog [\"Building a Machine Learning Platform with Kubeflow and Ray on Google Kubernetes Engine\"](https://cloud.google.com/blog/products/ai-machine-learning/build-a-ml-platform-with-kubeflow-and-ray-on-gke) from Google Cloud.The [Kubeflow](https://www.kubeflow.org/) project is dedicated to making deployments of machine learning (ML) workflows on Kubernetes simple, portable and scalable.# Requirements\n* Dependencies\n    * `kustomize`: v3.2.0 (Kubeflow manifest is sensitive to `kustomize` version.)* `Kubernetes`: v1.23\n\n* Computing resources:\n    * 16GB RAM\n    * 8 CPUs\n\n# Example: Use Kubeflow to provide an interactive development envirzonment\n![image](../images/kubeflow-architecture.svg)\n\n## Step 1: Create a Kubernetes cluster with Kind.```sh\n# Kubeflow is sensitive to Kubernetes version and Kustomize version.kind create cluster --image=kindest/node:v1.23.0\nkustomize version --short\n# 3.2.0\n```\n\n## Step 2: Install Kubeflow v1.6-branch\n* This example installs Kubeflow with the [v1.6-branch](https://github.com/kubeflow/manifests/tree/v1.6-branch).* Install all Kubeflow official components and all common services using [one command](https://github.com/kubeflow/manifests/tree/v1.6-branch#install-with-a-single-command).* If you do not want to install all components, you can comment out **KNative**, **Katib**, **Tensorboards Controller**, **Tensorboard Web App**, **Training Operator**, and **KServe** from [example/kustomization.yaml](https://github.com/kubeflow/manifests/blob/v1.6-branch/example/kustomization.yaml).## Step 3: Install KubeRay operator\n\n* Follow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.## Step 4: Install RayCluster\n```sh\n# Create a RayCluster CR, and the KubeRay operator will reconcile a Ray cluster\n# with 1 head Pod and 1 worker Pod.helm install raycluster kuberay/ray-cluster --version 0.6.0 --set image.tag=2.2.0-py38-cpu\n\n# Check RayCluster\nkubectl get pod -l ray.io/cluster=raycluster-kuberay\n# NAME                                          READY   STATUS    RESTARTS   AGE\n# raycluster-kuberay-head-bz77b                 1/1     Running   0          64s\n# raycluster-kuberay-worker-workergroup-8gr5q   1/1     Running   0          63s\n```\n\n* This step uses `rayproject/ray:2.2.0-py38-cpu` as its image.Ray is very sensitive to the Python versions and Ray versions between the server (RayCluster) and client (JupyterLab) sides.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d83a5888-ad90-4410-b1dd-4498a4603a7c": {"__data__": {"id_": "d83a5888-ad90-4410-b1dd-4498a4603a7c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md", "text_hash": "107e92da2b7d0ec40312e0f432ab951a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e94ae1e47b404478422d0c5de96ec8b49bc63aa8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md"}, "hash": "f7950699d660c566b3f3632d165eeca2756bdc0267c4c70aa08550646a8d17dd"}, "2": {"node_id": "9e832df2-03ce-48de-b8e3-624545c04a4e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md", "text_hash": "d0e194f1ab53bb066b619315af3bdd56"}, "hash": "e3f4f506355e5f75162664f14d73bb2030271565ee68cc0fa12966b421640ae6"}}, "hash": "e8b04d80a690b74a1b75f80ecc71662c5a1d160afb225b8f378cd3938c247111", "text": "This image uses:\n    * Python 3.8.13\n    * Ray 2.2.0\n\n## Step 5: Forward the port of Istio's Ingress-Gateway\n* Follow the [instructions](https://github.com/kubeflow/manifests/tree/v1.6-branch#port-forward) to forward the port of Istio's Ingress-Gateway and log in to Kubeflow Central Dashboard.## Step 6: Create a JupyterLab via Kubeflow Central Dashboard\n* Click \"Notebooks\" icon in the left panel.* Click \"New Notebook\"\n* Select `kubeflownotebookswg/jupyter-scipy:v1.6.1` as OCI image.* Click \"Launch\"\n* Click \"CONNECT\" to connect into the JupyterLab instance.## Step 7: Use Ray client in the JupyterLab to connect to the RayCluster\n> Warning: Ray client has some known [limitations](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/ray-client.html#things-to-know) and is not actively maintained.We recommend using the [Ray Job Submission](https://docs.ray.io/en/latest/cluster/running-applications/job-submission/) instead.* As mentioned in Step 4, Ray is very sensitive to the Python versions and Ray versions between the server (RayCluster) and client (JupyterLab) sides.Open a terminal in the JupyterLab:\n    ```sh\n    # Check Python version.The version's MAJOR and MINOR should match with RayCluster (i.e.Python 3.8)\n    python --version \n    # Python 3.8.10\n    \n    # Install Ray 2.2.0\n    pip install -U ray[default]==2.2.0\n    ```\n* Connect to RayCluster via Ray client.```python\n    # Open a new .ipynb page.import ray\n    # ray://${RAYCLUSTER_HEAD_SVC}.${NAMESPACE}.svc.cluster.local:${RAY_CLIENT_PORT}\n    ray.init(address=\"ray://raycluster-kuberay-head-svc.default.svc.cluster.local:10001\")\n    print(ray.cluster_resources())\n    # {'node:10.244.0.41': 1.0, 'memory': 3000000000.0, 'node:10.244.0.40': 1.0, 'object_store_memory': 805386239.0, 'CPU': 2.0}\n\n    # Try Ray task\n    @ray.remote\n    def f(x):\n        return x * x\n\n    futures = [f.remote(i) for i in range(4)]\n    print(ray.get(futures)) # [0, 1, 4, 9]\n\n    # Try Ray actor\n    @ray.remote\n    class Counter(object):\n        def __init__(self):\n            self.n = 0\n\n        def increment(self):\n            self.n += 1\n\n        def read(self):\n            return self.n\n\n    counters = [Counter.remote() for i in range(4)]\n    [c.increment.remote() for c in counters]\n    futures = [c.read.remote() for c in counters]\n    print(ray.get(futures)) # [1, 1, 1, 1]\n    ```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5c7a8dda-c347-4886-8d95-932731e4b6fd": {"__data__": {"id_": "5c7a8dda-c347-4886-8d95-932731e4b6fd", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "3d10c34b561aeb86a85eacd9958ae32a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "3": {"node_id": "7acb202b-f342-42a0-93bb-eaf2ba917e52", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "19ee7134579ac6e707a60fe7c2c9a561"}, "hash": "e6e211db5cfb81ad542e8447d65161d5eba9299648216bd256fbf93bc215724f"}}, "hash": "70f2cf378a9501faca70ec2bf699c2e53858795a5f71c816fbfa4e998e83a3db", "text": "(kuberay-prometheus-grafana)=\n\n# Using Prometheus and Grafana\n\nThis section will describe how to monitor Ray Clusters in Kubernetes using Prometheus & Grafana.\n\nIf you do not have any experience with Prometheus and Grafana on Kubernetes, watch this [YouTube playlist](https://youtube.com/playlist?list=PLy7NrYWoggjxCF3av5JKwyG7FFF9eLeL4).\n\n## Preparation\n\nClone the [KubeRay repository](https://github.com/ray-project/kuberay) and checkout the `master` branch.\nThis tutorial requires several files in the repository.\n\n## Step 1: Create a Kubernetes cluster with Kind\n\n```sh\nkind create cluster\n```\n\n## Step 2: Install Kubernetes Prometheus Stack via Helm chart\n\n```sh\n# Path: kuberay/\n./install/prometheus/install.sh\n\n# Check the installation\nkubectl get all -n prometheus-system\n\n# (part of the output)\n# NAME                                                  READY   UP-TO-DATE   AVAILABLE   AGE\n# deployment.apps/prometheus-grafana                    1/1     1            1           46s\n# deployment.apps/prometheus-kube-prometheus-operator   1/1     1            1           46s\n# deployment.apps/prometheus-kube-state-metrics         1/1     1            1           46s\n```\n\n* KubeRay provides an [install.sh script](https://github.com/ray-project/kuberay/blob/master/install/prometheus/install.sh) to install the [kube-prometheus-stack v48.2.1](https://github.com/prometheus-community/helm-charts/tree/kube-prometheus-stack-48.2.1/charts/kube-prometheus-stack) chart and related custom resources, including **ServiceMonitor**, **PodMonitor** and **PrometheusRule**, in the namespace `prometheus-system` automatically.\n\n* We made some modifications to the original `values.yaml` in kube-prometheus-stack chart to allow embedding Grafana panels in Ray Dashboard. See [overrides.yaml](https://github.com/ray-project/kuberay/tree/master/install/prometheus/overrides.yaml) for more details.\n  ```yaml\n  grafana:\n    grafana.ini:\n      security:\n        allow_embedding: true\n      auth.anonymous:\n        enabled: true\n        org_role: Viewer\n  ```\n\n\n\n## Step 3: Install a KubeRay operator\n\n* Follow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7acb202b-f342-42a0-93bb-eaf2ba917e52": {"__data__": {"id_": "7acb202b-f342-42a0-93bb-eaf2ba917e52", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "19ee7134579ac6e707a60fe7c2c9a561"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "2": {"node_id": "5c7a8dda-c347-4886-8d95-932731e4b6fd", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "3d10c34b561aeb86a85eacd9958ae32a"}, "hash": "70f2cf378a9501faca70ec2bf699c2e53858795a5f71c816fbfa4e998e83a3db"}, "3": {"node_id": "d014be4c-9b01-4641-8c64-f95aedcd321b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "a84d568e0bbda21a138d8ff64ef59fef"}, "hash": "d003bfeb01d6112839a54d7967e8855e8c95fecb2250c36b1e4b3ea7cb3cb502"}}, "hash": "e6e211db5cfb81ad542e8447d65161d5eba9299648216bd256fbf93bc215724f", "text": "## Step 4: Install a RayCluster\n\n```sh\n# path: ray-operator/config/samples/\nkubectl apply -f ray-cluster.embed-grafana.yaml\n\n# Check ${RAYCLUSTER_HEAD_POD}\nkubectl get pod -l ray.io/node-type=head\n\n# Example output:\n# NAME                            READY   STATUS    RESTARTS   AGE\n# raycluster-kuberay-head-btwc2   1/1     Running   0          63s\n\n# Wait until all Ray Pods are running and forward the port of the Prometheus metrics endpoint in a new terminal.kubectl port-forward --address 0.0.0.0 ${RAYCLUSTER_HEAD_POD} 8080:8080\ncurl localhost:8080\n\n# Example output (Prometheus metrics format):\n# # HELP ray_spill_manager_request_total Number of {spill, restore} requests.# # TYPE ray_spill_manager_request_total gauge\n# ray_spill_manager_request_total{Component=\"raylet\",NodeAddress=\"10.244.0.13\",Type=\"Restored\",Version=\"2.0.0\"} 0.0\n\n# Ensure that the port (8080) for the metrics endpoint is also defined in the head's Kubernetes service.kubectl get service\n\n# NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE\n# raycluster-kuberay-head-svc   ClusterIP   10.96.201.142   <none>        6379/TCP,8265/TCP,8080/TCP,8000/TCP,10001/TCP   106m\n```\n\n* KubeRay exposes a Prometheus metrics endpoint in port **8080** via a built-in exporter by default.Hence, we do not need to install any external exporter.* If you want to configure the metrics endpoint to a different port, see [kuberay/#954](https://github.com/ray-project/kuberay/pull/954) for more details.* Prometheus metrics format:\n  * `# HELP`: Describe the meaning of this metric.* `# TYPE`: See [this document](https://prometheus.io/docs/concepts/metric_types/) for more details.* Three required environment variables are defined in [ray-cluster.embed-grafana.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.embed-grafana.yaml).See [Configuring and Managing Ray Dashboard](https://docs.ray.io/en/latest/cluster/configure-manage-dashboard.html) for more details about these environment variables.```yaml\n  env:\n    - name: RAY_GRAFANA_IFRAME_HOST\n      value: http://127.0.0.1:3000\n    - name: RAY_GRAFANA_HOST\n      value: http://prometheus-grafana.prometheus-system.svc:80\n    - name: RAY_PROMETHEUS_HOST\n      value: http://prometheus-kube-prometheus-prometheus.prometheus-system.svc:9090\n  ```\n  * Note that we do not deploy Grafana in the head Pod, so we need to set both `RAY_GRAFANA_IFRAME_HOST` and `RAY_GRAFANA_HOST`.`RAY_GRAFANA_HOST` is used by the head Pod to send health-check requests to Grafana in the backend.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d014be4c-9b01-4641-8c64-f95aedcd321b": {"__data__": {"id_": "d014be4c-9b01-4641-8c64-f95aedcd321b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "a84d568e0bbda21a138d8ff64ef59fef"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "2": {"node_id": "7acb202b-f342-42a0-93bb-eaf2ba917e52", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "19ee7134579ac6e707a60fe7c2c9a561"}, "hash": "e6e211db5cfb81ad542e8447d65161d5eba9299648216bd256fbf93bc215724f"}, "3": {"node_id": "5830ebfa-4880-41c3-adc9-08303f4f371a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "e46e71b70303f5800eca9225ca064b43"}, "hash": "43113a09e8a3b91592b5094c244aa734ed2774d990b470f9935332daa82cb33f"}}, "hash": "d003bfeb01d6112839a54d7967e8855e8c95fecb2250c36b1e4b3ea7cb3cb502", "text": "`RAY_GRAFANA_IFRAME_HOST` is used by your browser to fetch the Grafana panels from the Grafana server rather than from the head Pod.Because we forward the port of Grafana to `127.0.0.1:3000` in this example, we set `RAY_GRAFANA_IFRAME_HOST` to `http://127.0.0.1:3000`.* `http://` is required.## Step 5: Collect Head Node metrics with a ServiceMonitor\n\n```yaml\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: ray-head-monitor\n  namespace: prometheus-system\n  labels:\n    # `release: $HELM_RELEASE`: Prometheus can only detect ServiceMonitor with this label.release: prometheus\nspec:\n  jobLabel: ray-head\n  # Only select Kubernetes Services in the \"default\" namespace.namespaceSelector:\n    matchNames:\n      - default\n  # Only select Kubernetes Services with \"matchLabels\".selector:\n    matchLabels:\n      ray.io/node-type: head\n  # A list of endpoints allowed as part of this ServiceMonitor.endpoints:\n    - port: metrics\n  targetLabels:\n  - ray.io/cluster\n```\n\n* The YAML example above is [serviceMonitor.yaml](https://github.com/ray-project/kuberay/blob/master/config/prometheus/serviceMonitor.yaml), and it is created by **install.sh**.Hence, no need to create anything here.* See [ServiceMonitor official document](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#servicemonitor) for more details about the configurations.* `release: $HELM_RELEASE`: Prometheus can only detect ServiceMonitor with this label.<div id=\"prometheus-can-only-detect-this-label\" ></div>\n\n  ```sh\n  helm ls -n prometheus-system\n  # ($HELM_RELEASE is \"prometheus\".)# NAME            NAMESPACE               REVISION        UPDATED                                 STATUS          CHART                           APP VERSION\n  # prometheus      prometheus-system       1               2023-02-06 06:27:05.530950815 +0000 UTC deployed        kube-prometheus-stack-44.3.1    v0.62.0\n\n  kubectl get prometheuses.monitoring.coreos.com -n prometheus-system -oyaml\n  # serviceMonitorSelector:\n  #   matchLabels:\n  #     release: prometheus\n  # podMonitorSelector:\n  #   matchLabels:\n  #     release: prometheus\n  # ruleSelector:\n  #   matchLabels:\n  #     release: prometheus\n  ```\n\n* `namespaceSelector` and `seletor` are used to select exporter's Kubernetes service.Because Ray uses a built-in exporter, the **ServiceMonitor** selects Ray's head service which exposes the metrics endpoint (i.e.port 8080 here).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "5830ebfa-4880-41c3-adc9-08303f4f371a": {"__data__": {"id_": "5830ebfa-4880-41c3-adc9-08303f4f371a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "e46e71b70303f5800eca9225ca064b43"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "2": {"node_id": "d014be4c-9b01-4641-8c64-f95aedcd321b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "a84d568e0bbda21a138d8ff64ef59fef"}, "hash": "d003bfeb01d6112839a54d7967e8855e8c95fecb2250c36b1e4b3ea7cb3cb502"}, "3": {"node_id": "0d088cc4-f973-4399-82ec-c747dc16ac21", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "53de1d075e92a2d1fd1d08fefae02a5b"}, "hash": "cdb32aaf4ebc078b1bb8f38a7a4b0c7d4873dff91e547f0d8440312678d071c7"}}, "hash": "43113a09e8a3b91592b5094c244aa734ed2774d990b470f9935332daa82cb33f", "text": "```sh\n  kubectl get service -n default -l ray.io/node-type=head\n  # NAME                          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                                         AGE\n  # raycluster-kuberay-head-svc   ClusterIP   10.96.201.142   <none>        6379/TCP,8265/TCP,8080/TCP,8000/TCP,10001/TCP   153m\n  ```\n\n* `targetLabels`: We added `spec.targetLabels[0].ray.io/cluster` because we want to include the name of the RayCluster in the metrics that will be generated by this ServiceMonitor.The `ray.io/cluster` label is part of the Ray head node service and it will be transformed into a `ray_io_cluster` metric label.That is, any metric that will be imported, will also contain the following label `ray_io_cluster=<ray-cluster-name>`.This may seem optional but it becomes mandatory if you deploy multiple RayClusters.## Step 6: Collect Worker Node metrics with PodMonitors\n\nKubeRay operator does not create a Kubernetes service for the Ray worker Pods, therefore we cannot use a Prometheus ServiceMonitor to scrape the metrics from the worker Pods.To collect worker metrics, we can use `Prometheus PodMonitors CRD` instead.**Note**: We could create a Kubernetes service with selectors a common label subset from our worker pods, however, this is not ideal because our workers are independent from each other, that is, they are not a collection of replicas spawned by replicaset controller.Due to that, we should avoid using a Kubernetes service for grouping them together.```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PodMonitor\nmetadata:\n  name: ray-workers-monitor\n  namespace: prometheus-system\n  labels:\n    # `release: $HELM_RELEASE`: Prometheus can only detect PodMonitor with this label.release: prometheus\n    ray.io/cluster: raycluster-kuberay # $RAY_CLUSTER_NAME: \"kubectl get rayclusters.ray.io\"\nspec:\n  jobLabel: ray-workers\n  # Only select Kubernetes Pods in the \"default\" namespace.namespaceSelector:\n    matchNames:\n      - default\n  # Only select Kubernetes Pods with \"matchLabels\".selector:\n    matchLabels:\n      ray.io/node-type: worker\n  # A list of endpoints allowed as part of this PodMonitor.podMetricsEndpoints:\n  - port: metrics\n```\n\n* `release: $HELM_RELEASE`: Prometheus can only detect PodMonitor with this label.See [here](#prometheus-can-only-detect-this-label) for more details.* **PodMonitor** in `namespaceSelector` and `selector` are used to select Kubernetes Pods.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d088cc4-f973-4399-82ec-c747dc16ac21": {"__data__": {"id_": "0d088cc4-f973-4399-82ec-c747dc16ac21", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "53de1d075e92a2d1fd1d08fefae02a5b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "2": {"node_id": "5830ebfa-4880-41c3-adc9-08303f4f371a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "e46e71b70303f5800eca9225ca064b43"}, "hash": "43113a09e8a3b91592b5094c244aa734ed2774d990b470f9935332daa82cb33f"}, "3": {"node_id": "385ac200-4b03-41ae-81d5-acdb37ca0449", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "124065b365afec1d83f7951daacf9402"}, "hash": "ef3acb7e9a8bc997a7cc3088af250782019e9ec6e2cc0331cb21ac10fcf52f73"}}, "hash": "cdb32aaf4ebc078b1bb8f38a7a4b0c7d4873dff91e547f0d8440312678d071c7", "text": "```sh\n  kubectl get pod -n default -l ray.io/node-type=worker\n  # NAME                                          READY   STATUS    RESTARTS   AGE\n  # raycluster-kuberay-worker-workergroup-5stpm   1/1     Running   0          3h16m\n  ```\n\n* `ray.io/cluster: $RAY_CLUSTER_NAME`: We also define `metadata.labels` by manually adding `ray.io/cluster: <ray-cluster-name>` and then instructing the PodMonitors resource to add that label in the scraped metrics via `spec.podTargetLabels[0].ray.io/cluster`.## Step 7: Collect custom metrics with Recording Rules\n\n[Recording Rules](https://prometheus.io/docs/prometheus/latest/configuration/recording_rules/) allow us to precompute frequently needed or computationally expensive [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) expressions and save their result as custom metrics.Note this is different from [Custom Application-level Metrics](application-level-metrics) which aim for the visibility of ray applications.```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: ray-cluster-gcs-rules\n  namespace: prometheus-system\n  labels:\n    # `release: $HELM_RELEASE`: Prometheus can only detect Recording Rules with this label.release: prometheus\nspec:\n  groups:\n- #  Rules within a group are run periodically with the same evaluation interval(30s in this example).name: ray-cluster-main-staging-gcs.rules\n    # How often rules in the group are evaluated.interval: 30s\n    rules:\n    - # The name of the custom metric.# Also see best practices for naming metrics created by recording rules:\n      # https://prometheus.io/docs/practices/rules/#recording-rules\n      record: ray_gcs_availability_30d\n      # PromQL expression.expr: |\n      (\n        100 * (\n          sum(rate(ray_gcs_update_resource_usage_time_bucket{container=\"ray-head\", le=\"20.0\"}[30d]))\n          /\n          sum(rate(ray_gcs_update_resource_usage_time_count{container=\"ray-head\"}[30d]))\n        )\n      )\n```\n\n* The PromQL expression above is: \n$$\\frac{ number\\ of\\ update\\ resource\\ usage\\ RPCs\\ that\\ have\\ RTT\\ smaller\\ then\\ 20ms\\ in\\ last\\ 30\\ days\\ }{total\\ number\\ of\\ update\\ resource\\ usage\\ RPCs\\ in\\ last\\ 30\\ days\\ }   \\times 100 $$\n\n\n* The recording rule above is one of rules defined in [prometheusRules.yaml](https://github.com/ray-project/kuberay/blob/master/config/prometheus/rules/prometheusRules.yaml), and it is created by **install.sh**. Hence, no need to create anything here.\n\n* See [PrometheusRule official document](https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusrule) for more details about the configurations.\n\n* `release: $HELM_RELEASE`: Prometheus can only detect PrometheusRule with this label. See [here](#prometheus-can-only-detect-this-label) for more details.\n\n* PrometheusRule can be reloaded at runtime. Use `kubectl apply {modified prometheusRules.yaml}` to reconfigure the rules if needed.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "385ac200-4b03-41ae-81d5-acdb37ca0449": {"__data__": {"id_": "385ac200-4b03-41ae-81d5-acdb37ca0449", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "124065b365afec1d83f7951daacf9402"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "2": {"node_id": "0d088cc4-f973-4399-82ec-c747dc16ac21", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "53de1d075e92a2d1fd1d08fefae02a5b"}, "hash": "cdb32aaf4ebc078b1bb8f38a7a4b0c7d4873dff91e547f0d8440312678d071c7"}, "3": {"node_id": "30b5bb31-3345-49bd-b5a1-5ed0976f57b2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "dabf7754ef509870c20e7e59180f9d14"}, "hash": "88c637aefddaf3d93aa4607d9d16b5d86030c77b0f307a98f2f7b3ab472cc347"}}, "hash": "ef3acb7e9a8bc997a7cc3088af250782019e9ec6e2cc0331cb21ac10fcf52f73", "text": "## Step 8: Define Alert Conditions with Alerting Rules\n\n[Alerting rules](https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/) allow us to define alert conditions based on [PromQL](https://prometheus.io/docs/prometheus/latest/querying/basics/) expressions and to send notifications about firing alerts to [Alertmanager](https://prometheus.io/docs/alerting/latest/alertmanager) which adds summarization, notification rate limiting, silencing and alert dependencies on top of the simple alert definitions.```yaml\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: ray-cluster-gcs-rules\n  namespace: prometheus-system\n  labels:\n    # `release: $HELM_RELEASE`: Prometheus can only detect Alerting Rules with this label.release: prometheus\nspec:\n  groups:\n  - name: ray-cluster-main-staging-gcs.rules\n    # How often rules in the group are evaluated.interval: 30s\n    rules:\n    - alert: MissingMetricRayGlobalControlStore\n      # A set of informational labels.Annotations can be used to store longer additional information compared to rules.0.labels.annotations:\n        description: Ray GCS is not emitting any metrics for Resource Update requests\n        summary: Ray GCS is not emitting metrics anymore\n      # PromQL expression.expr: |\n                      (\n                       absent(ray_gcs_update_resource_usage_time_bucket) == 1\n                      )\n      # Time that Prometheus will wait and check if the alert continues to be active during each evaluation before firing the alert.# firing alerts may be due to false positives or noise if the setting value is too small.# On the other hand, if the value is too big, the alerts may not be handled in time.for: 5m\n      # A set of additional labels to be attached to the alert.# It is possible to overwrite the labels in metadata.labels, so make sure one of the labels match the label in ruleSelector.matchLabels.labels:\n        severity: critical\n```\n\n* The PromQL expression above checks if there is no time series exist for `ray_gcs_update_resource_usage_time_bucket` metric.See [absent()](https://prometheus.io/docs/prometheus/latest/querying/functions/#absent) for more detail.* The alerting rule above is one of rules defined in [prometheusRules.yaml](https://github.com/ray-project/kuberay/blob/master/config/prometheus/rules/prometheusRules.yaml), and it is created by **install.sh**.Hence, no need to create anything here.* Alerting rules are configured in the same way as recording rules.## Step 9: Access Prometheus Web UI\n```sh\n# Forward the port of Prometheus Web UI in the Prometheus server Pod.kubectl port-forward --address 0.0.0.0 prometheus-prometheus-kube-prometheus-prometheus-0 -n prometheus-system 9090:9090\n```\n\n- Go to `${YOUR_IP}:9090/targets` (e.g.`127.0.0.1:9090/targets`).You should be able to see:\n  - `podMonitor/prometheus-system/ray-workers-monitor/0 (1/1 up)`\n  - `serviceMonitor/prometheus-system/ray-head-monitor/0 (1/1 up)`\n\n![Prometheus Web UI](../images/prometheus_web_ui.png)\n\n- Go to `${YOUR_IP}:9090/graph`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30b5bb31-3345-49bd-b5a1-5ed0976f57b2": {"__data__": {"id_": "30b5bb31-3345-49bd-b5a1-5ed0976f57b2", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "dabf7754ef509870c20e7e59180f9d14"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md"}, "hash": "bb2e621b9738db42d3928fc968a6ef54692325d96b36c8b2fbd091162270a00e"}, "2": {"node_id": "385ac200-4b03-41ae-81d5-acdb37ca0449", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "124065b365afec1d83f7951daacf9402"}, "hash": "ef3acb7e9a8bc997a7cc3088af250782019e9ec6e2cc0331cb21ac10fcf52f73"}}, "hash": "88c637aefddaf3d93aa4607d9d16b5d86030c77b0f307a98f2f7b3ab472cc347", "text": "You should be able to query:\n  - [System Metrics](https://docs.ray.io/en/latest/ray-observability/ray-metrics.html#system-metrics)\n  - [Application Level Metrics](https://docs.ray.io/en/latest/ray-observability/ray-metrics.html#application-level-metrics)\n  - Custom Metrics defined in Recording Rules (e.g.`ray_gcs_availability_30d`)\n\n- Go to `${YOUR_IP}:9090/alerts`.You should be able to see:\n  - Alerting Rules (e.g.`MissingMetricRayGlobalControlStore`).## Step 10: Access Grafana\n\n```sh\n# Forward the port of Grafana\nkubectl port-forward --address 0.0.0.0 deployment/prometheus-grafana -n prometheus-system 3000:3000\n# Note: You need to update `RAY_GRAFANA_IFRAME_HOST` if you expose Grafana to a different port.# Check ${YOUR_IP}:3000/login for the Grafana login page (e.g.127.0.0.1:3000/login).# The default username is \"admin\" and the password is \"prom-operator\".```\n\n> Note: `kubectl port-forward` is not recommended for production use.Refer to [this Grafana document](https://grafana.com/tutorials/run-grafana-behind-a-proxy/) for exposing Grafana behind a reverse proxy.* The default password is defined by `grafana.adminPassword` in the [values.yaml](https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-prometheus-stack/values.yaml) of the kube-prometheus-stack chart.* After logging in to Grafana successfully, we can import Ray Dashboard into Grafana via **dashboard_default.json**.* Click \"Dashboards\" icon in the left panel.* Click \"New\".* Click \"Import\".* Click \"Upload JSON file\".* Choose a JSON file.* Case 1: If you are using Ray 2.5.0, you can use [config/grafana/default_grafana_dashboard.json](https://github.com/ray-project/kuberay/blob/master/config/grafana/default_grafana_dashboard.json).* Case 2: Otherwise, you should import the `default_grafana_dashboard.json` file from `/tmp/ray/session_latest/metrics/grafana/dashboards/` in the head Pod.You can use `kubectl cp` to copy the file from the head Pod to your local machine.* Click \"Import\".* TODO: Note that importing the dashboard manually is not ideal.We should find a way to import the dashboard automatically.![Grafana Ray Dashboard](../images/grafana_ray_dashboard.png)\n\n## Step 11: Embed Grafana panels in Ray Dashboard\n\n```sh\nkubectl port-forward --address 0.0.0.0 svc/raycluster-embed-grafana-head-svc 8265:8265\n# Visit http://127.0.0.1:8265/#/metrics in your browser.```\n\n![Ray Dashboard with Grafana panels](../images/ray_dashboard_embed_grafana.png)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bd154635-df9d-4866-bd96-2ad3cd190dad": {"__data__": {"id_": "bd154635-df9d-4866-bd96-2ad3cd190dad", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md", "file_name": "pyspy.md", "text_hash": "e64a17c789493f9199670c38f061678e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "38a22c7537e5c6f4028cd701b950c8a6cb05cb61", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md", "file_name": "pyspy.md"}, "hash": "a6b3860508c8ada4284a9b3fbcc9672c2fe862f7a2574c79d427d9e13d9e70bf"}}, "hash": "24ef4a79094dda56aa84fec226cde522c2aeaf773c7525dc37700555c197a01d", "text": "(kuberay-pyspy-integration)=\n\n# Profiling with py-spy\n\n## Stack trace and CPU profiling\n[py-spy](https://github.com/benfred/py-spy/tree/master) is a sampling profiler for Python programs. It lets you visualize what your Python program is spending time on without restarting the program or modifying the code in any way. This section describes how to configure RayCluster YAML file to enable py-spy and see Stack Trace and CPU Flame Graph via Ray Dashboard.\n\n## Prerequisite\npy-spy requires the `SYS_PTRACE` capability to read process memory. However, Kubernetes omits this capability by default. To enable profiling, add the following to the `template.spec.containers` for both the head and worker Pods.\n\n```bash\nsecurityContext:\n  capabilities:\n    add:\n    - SYS_PTRACE\n```\n**Notes:**\n- Adding `SYS_PTRACE` is forbidden under `baseline` and `restricted` Pod Security Standards. See [Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/) for more details.\n\n## Check CPU flame graph and stack trace via Ray Dashboard\n\n### Step 1: Create a Kind cluster\n\n```bash\nkind create cluster\n```\n\n### Step 2: Install the KubeRay operator\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.\n\n### Step 3: Create a RayCluster with `SYS_PTRACE` capability\n\n```bash\n# Download `ray-cluster.py-spy.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.py-spy.yaml\n\n# Create a RayCluster\nkubectl apply -f ray-cluster.py-spy.yaml\n```\n\n### Step 4: Forward the dashboard port\n\n```bash\nkubectl port-forward --address 0.0.0.0 svc/raycluster-py-spy-head-svc 8265:8265\n```\n\n### Step 5: Run a sample job within the head Pod\n\n```bash\n# Log in to the head Pod\nkubectl exec -it ${YOUR_HEAD_POD} -- bash\n\n# (Head Pod) Run a sample job in the Pod\n# `long_running_task` includes a `while True` loop to ensure the task remains actively running indefinitely. \n# This allows you ample time to view the Stack Trace and CPU Flame Graph via Ray Dashboard.\npython3 samples/long_running_task.py\n```\n\n**Notes:**\n- If you're running your own examples and encounter the error `Failed to write flamegraph: I/O error: No stack counts found` when viewing CPU Flame Graph, it might be due to the process being idle. Notably, using the `sleep` function can lead to this state. In such situations, py-spy filters out the idle stack traces. Refer to this [issue](https://github.com/benfred/py-spy/issues/321#issuecomment-731848950) for more information.\n\n### Step 6: Profile using Ray Dashboard\n\n- Visit http://localhost:8265/#/cluster.\n- Click `Stack Trace` for `ray::long_running_task`.\n    ![StackTrace](../images/stack_trace.png)\n- Click `CPU Flame Graph` for `ray::long_running_task`.\n    ![FlameGraph](../images/cpu_flame_graph.png)\n- For additional details on using the profiler, See [Python CPU profiling in the Dashboard](https://docs.ray.io/en/latest/ray-observability/user-guides/debug-apps/optimize-performance.html#python-cpu-profiling-in-the-dashboard).\n\n### Step 7: Clean up\n\n```bash\nkubectl delete -f ray-cluster.py-spy.yaml\nhelm uninstall kuberay-operator\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b5beb349-8735-4ad9-bcac-9b8a73f7aba1": {"__data__": {"id_": "b5beb349-8735-4ad9-bcac-9b8a73f7aba1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/references.md", "file_name": "references.md", "text_hash": "d638cca45bcae35f294803ceb0d50943"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "e43761527e1583a7e63955b3c7f439bf71d3a789", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/references.md", "file_name": "references.md"}, "hash": "f5e4e5c9030d91af68f85060b141faf98e810466eaf55381ceb019619ac2a3bb"}}, "hash": "d7ff9c28b5926fffc5759262cbfd17585e2dfe983760cd8e2dab4a7b810ae256", "text": "(kuberay-api-reference)=\n# API Reference\n\nTo learn about RayCluster configuration, we recommend taking a look at\nthe {ref}`configuration guide <kuberay-config>`.\n\nFor comprehensive coverage of all supported RayCluster fields,\nrefer to the [Golang structs][RayClusterDef] used to generate the RayCluster CRD.\n\n[RayClusterDef]: https://github.com/ray-project/kuberay/blob/release-0.5/ray-operator/apis/ray/v1alpha1/raycluster_types.go#L12", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f64a2270-bf42-4a9f-8cb9-722e807d9cab": {"__data__": {"id_": "f64a2270-bf42-4a9f-8cb9-722e807d9cab", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "facb095ea6e3066357cb9f02b125cece"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f0d3d82a4b9bd26d149b36619cce5c7a9b20add", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting.md", "file_name": "troubleshooting.md"}, "hash": "e5d7448d4916d390d551b1e6dce8a2704323ac12154affe7cfbb897c18e77c0b"}}, "hash": "e5d7448d4916d390d551b1e6dce8a2704323ac12154affe7cfbb897c18e77c0b", "text": "(kuberay-troubleshooting)=\n\n# KubeRay Troubleshooting\n\n- {ref}`kuberay-troubleshootin-guides`\n- {ref}`kuberay-raysvc-troubleshoot`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "86381715-01dd-48c2-8163-00481f593b46": {"__data__": {"id_": "86381715-01dd-48c2-8163-00481f593b46", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "945a2d29575ced57a2782b476338edd0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "3": {"node_id": "238ecbca-fa3c-4ee7-8b67-ee247e560cc6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "e402e6c6f89fbb203e6be6ee1dc3b8c4"}, "hash": "c940f5ad2426ddd34da5e79981a2af36bff0141cb28d42827dad569c7c5dd941"}}, "hash": "35e055ff41ad8526d25af1e4e7a65dcaab2185434c552edb5615555c2ff4176e", "text": "(kuberay-raysvc-troubleshoot)=\n\n# RayService troubleshooting\n\nRayService is a Custom Resource Definition (CRD) designed for Ray Serve. In KubeRay, creating a RayService will first create a RayCluster and then\ncreate Ray Serve applications once the RayCluster is ready. If the issue pertains to the data plane, specifically your Ray Serve scripts \nor Ray Serve configurations (`serveConfigV2`), troubleshooting may be challenging. This section provides some tips to help you debug these issues.\n\n## Observability\n\n### Method 1: Check KubeRay operator's logs for errors\n\n```bash\nkubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n```\n\nThe above command will redirect the operator's logs to a file called `operator-log`. You can then search for errors in the file.\n\n### Method 2: Check RayService CR status\n\n```bash\nkubectl describe rayservice $RAYSERVICE_NAME -n $YOUR_NAMESPACE\n```\n\nYou can check the status and events of the RayService CR to see if there are any errors.\n\n### Method 3: Check logs of Ray Pods\n\nYou can also check the Ray Serve logs directly by accessing the log files on the pods. These log files contain system level logs from the Serve controller and HTTP proxy as well as access logs and user-level logs. See [Ray Serve Logging](serve-logging) and [Ray Logging](configure-logging) for more details.\n\n```bash\nkubectl exec -it $RAY_POD -n $YOUR_NAMESPACE -- bash\n# Check the logs under /tmp/ray/session_latest/logs/serve/\n```\n\n### Method 4: Check Dashboard\n\n```bash\nkubectl port-forward $RAY_POD -n $YOUR_NAMESPACE --address 0.0.0.0 8265:8265\n# Check $YOUR_IP:8265 in your browser\n```\n\nFor more details about Ray Serve observability on the dashboard, you can refer to [the documentation](dash-serve-view) and [the YouTube video](https://youtu.be/eqXfwM641a4).\n\n### Method 5: Ray State CLI\n\nYou can use the [Ray State CLI](state-api-cli-ref) on the head Pod to check the status of Ray Serve applications.\n\n```bash\n# Log into the head Pod\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- ray summary actors\n\n# [Example output]:\n# ======== Actors Summary: 2023-07-11 17:58:24.625032 ========\n# Stats:\n# ------------------------------------\n# total_actors: 14", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "238ecbca-fa3c-4ee7-8b67-ee247e560cc6": {"__data__": {"id_": "238ecbca-fa3c-4ee7-8b67-ee247e560cc6", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "e402e6c6f89fbb203e6be6ee1dc3b8c4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "2": {"node_id": "86381715-01dd-48c2-8163-00481f593b46", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "945a2d29575ced57a2782b476338edd0"}, "hash": "35e055ff41ad8526d25af1e4e7a65dcaab2185434c552edb5615555c2ff4176e"}, "3": {"node_id": "54605cca-3f23-4a66-bb5a-ddfc88fd84e9", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "db7504e6c10d89fb5daf5510ed2d0e7b"}, "hash": "b3562f88155271c0a368afd0451d4ad4261e7803318dd40ed3e70d061876ca1e"}}, "hash": "c940f5ad2426ddd34da5e79981a2af36bff0141cb28d42827dad569c7c5dd941", "text": "# Table (group by class):\n# ------------------------------------\n#     CLASS_NAME                          STATE_COUNTS\n# 0   ServeController                     ALIVE: 1\n# 1   ServeReplica:fruit_app_OrangeStand  ALIVE: 1\n# 2   HTTPProxyActor                      ALIVE: 3\n# 3   ServeReplica:math_app_DAGDriver     ALIVE: 1\n# 4   ServeReplica:math_app_Multiplier    ALIVE: 1\n# 5   ServeReplica:math_app_create_order  ALIVE: 1\n# 6   ServeReplica:fruit_app_DAGDriver    ALIVE: 1\n# 7   ServeReplica:fruit_app_FruitMarket  ALIVE: 1\n# 8   ServeReplica:math_app_Adder         ALIVE: 1\n# 9   ServeReplica:math_app_Router        ALIVE: 1\n# 10  ServeReplica:fruit_app_MangoStand   ALIVE: 1\n# 11  ServeReplica:fruit_app_PearStand    ALIVE: 1\n```\n\n## Common issues\n\n* {ref}`kuberay-raysvc-issue1`\n* {ref}`kuberay-raysvc-issue2`\n* {ref}`kuberay-raysvc-issue3-1`\n* {ref}`kuberay-raysvc-issue3-2`\n* {ref}`kuberay-raysvc-issue4`\n* {ref}`kuberay-raysvc-issue5`\n* {ref}`kuberay-raysvc-issue6`\n* {ref}`kuberay-raysvc-issue7`\n* {ref}`kuberay-raysvc-issue8`\n* {ref}`kuberay-raysvc-issue9`\n\n(kuberay-raysvc-issue1)=\n### Issue 1: Ray Serve script is incorrect.We strongly recommend that you test your Ray Serve script locally or in a RayCluster before\ndeploying it to a RayService.Please refer to [rayserve-dev-doc.md](https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayserve-dev-doc.md) for more details.(kuberay-raysvc-issue2)=\n### Issue 2: `serveConfigV2` is incorrect.For the sake of flexibility, we have set `serveConfigV2` as a YAML multi-line string in the RayService CR.This implies that there is no strict type checking for the Ray Serve configurations in `serveConfigV2` field.Some tips to help you debug the `serveConfigV2` field:\n\n* Check [the documentation](serve-api) for the schema about\nthe Ray Serve Multi-application API `PUT \"/api/serve/applications/\"`.* Unlike `serveConfig`, `serveConfigV2` adheres to the snake case naming convention.For example, `numReplicas` is used in `serveConfig`, while `num_replicas` is used in `serveConfigV2`.(kuberay-raysvc-issue3-1)=\n### Issue 3-1: The Ray image does not include the required dependencies.You have two options to resolve this issue:\n\n* Build your own Ray image with the required dependencies.* Specify the required dependencies via `runtime_env` in `serveConfigV2` field.* For example, the MobileNet example requires `python-multipart`, which is not included in the Ray image `rayproject/ray-ml:2.5.0`.Therefore, the YAML file includes `python-multipart` in the runtime environment.For more details, refer to [the MobileNet example](kuberay-mobilenet-rayservice-example).(kuberay-raysvc-issue3-2)=\n### Issue 3-2: Examples for troubleshooting dependency issues.> Note: We highly recommend testing your Ray Serve script locally or in a RayCluster before deploying it to a RayService.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "54605cca-3f23-4a66-bb5a-ddfc88fd84e9": {"__data__": {"id_": "54605cca-3f23-4a66-bb5a-ddfc88fd84e9", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "db7504e6c10d89fb5daf5510ed2d0e7b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "2": {"node_id": "238ecbca-fa3c-4ee7-8b67-ee247e560cc6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "e402e6c6f89fbb203e6be6ee1dc3b8c4"}, "hash": "c940f5ad2426ddd34da5e79981a2af36bff0141cb28d42827dad569c7c5dd941"}, "3": {"node_id": "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "ef2674c0a0cbd6f5aecde320c0db0090"}, "hash": "01f1ab52bc48636f01c57901bd722349a51d6cfb148e751ba552d06a61e83065"}}, "hash": "b3562f88155271c0a368afd0451d4ad4261e7803318dd40ed3e70d061876ca1e", "text": "This helps identify any dependency issues in the early stages.Please refer to [rayserve-dev-doc.md](https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayserve-dev-doc.md) for more details.In the [MobileNet example](kuberay-mobilenet-rayservice-example), the [mobilenet.py](https://github.com/ray-project/serve_config_examples/blob/master/mobilenet/mobilenet.py) consists of two functions: `__init__()` and `__call__()`.The function `__call__()` will only be called when the Serve application receives a request.* Example 1: Remove `python-multipart` from the runtime environment in [the MobileNet YAML](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-service.mobilenet.yaml).* The `python-multipart` library is only required for the `__call__` method.Therefore, we can only observe the dependency issue when we send a request to the application.* Example error message:\n    ```bash\n    Unexpected error, traceback: ray::ServeReplica:mobilenet_ImageClassifier.handle_request() (pid=226, ip=10.244.0.9)\n      ...File \"...\", line 24, in __call__\n        request = await http_request.form()\n      File \"/home/ray/anaconda3/lib/python3.7/site-packages/starlette/requests.py\", line 256, in _get_form\n        ), \"The `python-multipart` library must be installed to use form parsing.\"AssertionError: The `python-multipart` library must be installed to use form parsing..\n    ```\n\n* Example 2: Update the image from `rayproject/ray-ml:2.5.0` to `rayproject/ray:2.5.0` in [the MobileNet YAML](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-service.mobilenet.yaml).The latter image does not include `tensorflow`.* The `tensorflow` library is imported in the [mobilenet.py](https://github.com/ray-project/serve_config_examples/blob/master/mobilenet/mobilenet.py).* Example error message:\n    ```bash\n    kubectl describe rayservices.ray.io rayservice-mobilenet\n\n    # Example error message:\n    Pending Service Status:\n      Application Statuses:\n        Mobilenet:\n          ...Message:                  Deploying app 'mobilenet' failed:\n            ray::deploy_serve_application() (pid=279, ip=10.244.0.12)\n                ...File \".../mobilenet/mobilenet.py\", line 4, in <module>\n                from tensorflow.keras.preprocessing import image\n            ModuleNotFoundError: No module named 'tensorflow'\n    ```\n\n(kuberay-raysvc-issue4)=\n### Issue 4: Incorrect `import_path`.You can refer to [the documentation](https://docs.ray.io/en/latest/serve/api/doc/ray.serve.schema.ServeApplicationSchema.html#ray.serve.schema.ServeApplicationSchema.import_path) for more details about the format of `import_path`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c": {"__data__": {"id_": "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "ef2674c0a0cbd6f5aecde320c0db0090"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "2": {"node_id": "54605cca-3f23-4a66-bb5a-ddfc88fd84e9", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "db7504e6c10d89fb5daf5510ed2d0e7b"}, "hash": "b3562f88155271c0a368afd0451d4ad4261e7803318dd40ed3e70d061876ca1e"}, "3": {"node_id": "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "8f88616a866123c3e0e1937a5d5cc487"}, "hash": "efb4d3bca72d3c0cd8a5bf02b1fec954b5e0be02871144d8d4370f0c6c7624e5"}}, "hash": "01f1ab52bc48636f01c57901bd722349a51d6cfb148e751ba552d06a61e83065", "text": "Taking [the MobileNet YAML file](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-service.mobilenet.yaml) as an example,\nthe `import_path` is `mobilenet.mobilenet:app`.The first `mobilenet` is the name of the directory in the `working_dir`,\nthe second `mobilenet` is the name of the Python file in the directory `mobilenet/`,\nand `app` is the name of the variable representing Ray Serve application within the Python file.```yaml\n  serveConfigV2: |\n    applications:\n      - name: mobilenet\n        import_path: mobilenet.mobilenet:app\n        runtime_env:\n          working_dir: \"https://github.com/ray-project/serve_config_examples/archive/b393e77bbd6aba0881e3d94c05f968f05a387b96.zip\"\n          pip: [\"python-multipart==0.0.6\"]\n```\n\n(kuberay-raysvc-issue5)=\n### Issue 5: Fail to create / update Serve applications.You may encounter the following error messages when KubeRay tries to create / update Serve applications:\n\n#### Error message 1: `connect: connection refused`\n\n```\nPut \"http://${HEAD_SVC_FQDN}:52365/api/serve/applications/\": dial tcp $HEAD_IP:52365: connect: connection refused\n```\n\nFor RayService, the KubeRay operator submits a request to the RayCluster for creating Serve applications once the head Pod is ready.It's important to note that the Dashboard, Dashboard Agent and GCS may take a few seconds to start up after the head Pod is ready.As a result, the request may fail a few times initially before the necessary components are fully operational.If you continue to encounter this issue after waiting for 1 minute, it's possible that the dashboard or dashboard agent may have failed to start.For more information, you can check the `dashboard.log` and `dashboard_agent.log` files located at `/tmp/ray/session_latest/logs/` on the head Pod.#### Error message 2: `i/o timeout`\n\n```\nPut \"http://${HEAD_SVC_FQDN}:52365/api/serve/applications/\": dial tcp $HEAD_IP:52365: i/o timeout\"\n```\n\nOne possible cause of this issue could be a Kubernetes NetworkPolicy blocking the traffic between the Ray Pods and the dashboard agent's port (i.e., 52365).(kuberay-raysvc-issue6)=\n### Issue 6: `runtime_env`\n\nIn `serveConfigV2`, you can specify the runtime environment for the Ray Serve applications via `runtime_env`.Some common issues related to `runtime_env`:\n\n* The `working_dir` points to a private AWS S3 bucket, but the Ray Pods do not have the necessary permissions to access the bucket.* The NetworkPolicy blocks the traffic between the Ray Pods and the external URLs specified in `runtime_env`.(kuberay-raysvc-issue7)=\n### Issue 7: Failed to get Serve application statuses.You may encounter the following error message when KubeRay tries to get Serve application statuses:\n\n```\nGet \"http://${HEAD_SVC_FQDN}:52365/api/serve/applications/\": dial tcp $HEAD_IP:52365: connect: connection refused\"\n```\n\nAs mentioned in [Issue 5](#issue-5-fail-to-create--update-serve-applications), the KubeRay operator submits a `Put` request to the RayCluster for creating Serve applications once the head Pod is ready.After the successful submission of the `Put` request to the dashboard agent, a `Get` request is sent to the dashboard agent port (i.e., 52365).The successful submission indicates that all the necessary components, including the dashboard agent, are fully operational.Therefore, unlike Issue 5, the failure of the `Get` request is not expected.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a": {"__data__": {"id_": "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "8f88616a866123c3e0e1937a5d5cc487"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "2": {"node_id": "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "ef2674c0a0cbd6f5aecde320c0db0090"}, "hash": "01f1ab52bc48636f01c57901bd722349a51d6cfb148e751ba552d06a61e83065"}, "3": {"node_id": "0f3235ff-6850-417c-8ff2-4f101ce4a94b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "cd569688915b78ee3a8b4bd707b20f32"}, "hash": "c566cf2e1c27bbc7d723576b05bd846ea0111963f57573b7921fff95358495c4"}}, "hash": "efb4d3bca72d3c0cd8a5bf02b1fec954b5e0be02871144d8d4370f0c6c7624e5", "text": "If you consistently encounter this issue, there are several possible causes:\n\n* The dashboard agent process on the head Pod is not running.You can check the `dashboard_agent.log` file located at `/tmp/ray/session_latest/logs/` on the head Pod for more information.In addition, you can also perform an experiment to reproduce this cause by manually killing the dashboard agent process on the head Pod.```bash\n  # Step 1: Log in to the head Pod\n  kubectl exec -it $HEAD_POD -n $YOUR_NAMESPACE -- bash\n\n  # Step 2: Check the PID of the dashboard agent process\n  ps aux\n  # [Example output]\n  # ray          156 ... 0:03 /.../python -u /.../ray/dashboard/agent.py --\n\n  # Step 3: Kill the dashboard agent process\n  kill 156\n\n  # Step 4: Check the logs\n  cat /tmp/ray/session_latest/logs/dashboard_agent.log\n\n  # [Example output]\n  # 2023-07-10 11:24:31,962 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:31 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:34,001 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:33 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:36,043 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:36 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:38,082 INFO web_log.py:206 -- 10.244.0.5 [10/Jul/2023:18:24:38 +0000] \"GET /api/serve/applications/ HTTP/1.1\" 200 13940 \"-\" \"Go-http-client/1.1\"\n  # 2023-07-10 11:24:38,590 WARNING agent.py:531 -- Exiting with SIGTERM immediately...\n\n  # Step 5: Open a new terminal and check the logs of the KubeRay operator\n  kubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n\n  # [Example output]\n  # Get \\\"http://rayservice-sample-raycluster-rqlsl-head-svc.default.svc.cluster.local:52365/api/serve/applications/\\\": dial tcp 10.96.7.154:52365: connect: connection refused\n  ```\n\n(kuberay-raysvc-issue8)=\n### Issue 8: A loop of restarting the RayCluster occurs when the Kubernetes cluster runs out of resources.(KubeRay v0.6.1 or earlier)\n\n> Note: Currently, the KubeRay operator does not have a clear plan to handle situations where the Kubernetes cluster runs out of resources.Therefore, we recommend ensuring that the Kubernetes cluster has sufficient resources to accommodate the serve application.If the status of a serve application remains non-`RUNNING` for more than `serviceUnhealthySecondThreshold` seconds,\nthe KubeRay operator will consider the RayCluster as unhealthy and initiate the preparation of a new RayCluster.A common cause of this issue is that the Kubernetes cluster does not have enough resources to accommodate the serve application.In such cases, the KubeRay operator may continue to restart the RayCluster, leading to a loop of restarts.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0f3235ff-6850-417c-8ff2-4f101ce4a94b": {"__data__": {"id_": "0f3235ff-6850-417c-8ff2-4f101ce4a94b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "cd569688915b78ee3a8b4bd707b20f32"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "2": {"node_id": "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "8f88616a866123c3e0e1937a5d5cc487"}, "hash": "efb4d3bca72d3c0cd8a5bf02b1fec954b5e0be02871144d8d4370f0c6c7624e5"}, "3": {"node_id": "e47b8ac9-0bff-4fff-842d-c78e9e60a3a1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "8e87c66eaff60eb6bc0a8fa3090736d2"}, "hash": "deea502e9e0eda9552f9348caa2c94fb0fd14e81d52ca2cf6074c65c0e5d0346"}}, "hash": "c566cf2e1c27bbc7d723576b05bd846ea0111963f57573b7921fff95358495c4", "text": "We can also perform an experiment to reproduce this situation:\n\n* A Kubernetes cluster with an 8-CPUs node\n* [ray-service.insufficient-resources.yaml](https://gist.github.com/kevin85421/6a7779308aa45b197db8015aca0c1faf)\n  * RayCluster:\n    * The cluster has 1 head Pod with 4 physical CPUs, but `num-cpus` is set to 0 in `rayStartParams` to prevent any serve replicas from being scheduled on the head Pod.* The cluster also has 1 worker Pod with 1 CPU by default.* `serveConfigV2` specifies 5 serve deployments, each with 1 replica and a requirement of 1 CPU.```bash\n# Step 1: Get the number of CPUs available on the node\nkubectl get nodes -o custom-columns=NODE:.metadata.name,ALLOCATABLE_CPU:.status.allocatable.cpu\n\n# [Example output]\n# NODE                 ALLOCATABLE_CPU\n# kind-control-plane   8\n\n# Step 2: Install a KubeRay operator.# Step 3: Create a RayService with autoscaling enabled.kubectl apply -f ray-service.insufficient-resources.yaml\n\n# Step 4: The Kubernetes cluster will not have enough resources to accommodate the serve application.kubectl describe rayservices.ray.io rayservice-sample -n $YOUR_NAMESPACE\n\n# [Example output]\n# fruit_app_DAGDriver:\n#   Health Last Update Time:  2023-07-11T02:10:02Z\n#   Last Update Time:         2023-07-11T02:10:35Z\n#   Message:                  Deployment \"fruit_app_DAGDriver\" has 1 replicas that have taken more than 30s to be scheduled.This may be caused by waiting for the cluster to auto-scale, or waiting for a runtime environment to install.Resources required for each replica: {\"CPU\": 1.0}, resources available: {}.#   Status:                   UPDATING\n\n# Step 5: A new RayCluster will be created after `serviceUnhealthySecondThreshold` (300s here) seconds.# Check the logs of the KubeRay operator to find the reason for restarting the RayCluster.kubectl logs $KUBERAY_OPERATOR_POD -n $YOUR_NAMESPACE | tee operator-log\n\n# [Example output]\n# 2023-07-11T02:14:58.109Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"appName\": \"fruit_app\", \"restart reason\": \"The status of the serve application fruit_app has not been RUNNING for more than 300.000000 seconds.Hence, KubeRay operator labels the RayCluster unhealthy and will prepare a new RayCluster.\"}# 2023-07-11T02:14:58.109Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"deploymentName\": \"fruit_app_FruitMarket\", \"appName\": \"fruit_app\", \"restart reason\": \"The status of the serve deployment fruit_app_FruitMarket or the serve application fruit_app has not been HEALTHY/RUNNING for more than 300.000000 seconds.Hence, KubeRay operator labels the RayCluster unhealthy and will prepare a new RayCluster.The message of the serve deployment is: Deployment \\\"fruit_app_FruitMarket\\\" has 1 replicas that have taken more than 30s to be scheduled.This may be caused by waiting for the cluster to auto-scale, or waiting for a runtime environment to install.Resources required for each replica: {\\\"CPU\\\": 1.0}, resources available: {}.\"}# .# .# .# 2023-07-11T02:14:58.122Z\tINFO\tcontrollers.RayService\tRestart RayCluster\t{\"ServiceName\": \"default/rayservice-sample\", \"AvailableWorkerReplicas\": 1, \"DesiredWorkerReplicas\": 5, \"restart reason\": \"The serve application is unhealthy, restarting the cluster.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e47b8ac9-0bff-4fff-842d-c78e9e60a3a1": {"__data__": {"id_": "e47b8ac9-0bff-4fff-842d-c78e9e60a3a1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "8e87c66eaff60eb6bc0a8fa3090736d2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md"}, "hash": "ec0fb3ee9f397996838643edca17d98a354c2ca2ffeecbe2b10b391dd9622029"}, "2": {"node_id": "0f3235ff-6850-417c-8ff2-4f101ce4a94b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "cd569688915b78ee3a8b4bd707b20f32"}, "hash": "c566cf2e1c27bbc7d723576b05bd846ea0111963f57573b7921fff95358495c4"}}, "hash": "deea502e9e0eda9552f9348caa2c94fb0fd14e81d52ca2cf6074c65c0e5d0346", "text": "If the AvailableWorkerReplicas is not equal to DesiredWorkerReplicas, this may imply that the Autoscaler does not have enough resources to scale up the cluster.Hence, the serve application does not have enough resources to run.Please check https://github.com/ray-project/kuberay/blob/master/docs/guidance/rayservice-troubleshooting.md for more details.\", \"RayCluster\": {\"apiVersion\": \"ray.io/v1alpha1\", \"kind\": \"RayCluster\", \"namespace\": \"default\", \"name\": \"rayservice-sample-raycluster-hvd9f\"}}\n```\n\n(kuberay-raysvc-issue9)=\n### Issue 9: Upgrade from Ray Serve's single-application API to its multi-application API without downtime\n\nKubeRay v0.6.0 has begun supporting Ray Serve API V2 (multi-application) by exposing `serveConfigV2` in the RayService CRD.However, Ray Serve does not support deploying both API V1 and API V2 in the cluster simultaneously.Hence, if users want to perform in-place upgrades by replacing `serveConfig` with `serveConfigV2`, they may encounter the following error message:\n\n```\nray.serve.exceptions.RayServeException: You are trying to deploy a multi-application config, however a single-application \nconfig has been deployed to the current Serve instance already.Mixing single-app and multi-app is not allowed.Please either\nredeploy using the single-application config format `ServeApplicationSchema`, or shutdown and restart Serve to submit a\nmulti-app config of format `ServeDeploySchema`.If you are using the REST API, you can submit a multi-app config to the\nthe multi-app API endpoint `/api/serve/applications/`.```\n\nTo resolve this issue, you can replace `serveConfig` with `serveConfigV2` and modify `rayVersion` which has no effect when the Ray version is 2.0.0 or later to 2.100.0.This will trigger a new RayCluster preparation instead of an in-place update.If, after following the steps above, you still see the error message and GCS fault tolerance is enabled, it may be due to the `ray.io/external-storage-namespace` annotatoin being the same for both old and new RayClusters.You can remove the annotation and KubeRay will automatically generate a unique key for each RayCluster custom resource.You can refer to [kuberay#1297](https://github.com/ray-project/kuberay/issues/1297) for more details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c808a3ee-24c5-4f4d-836b-072d2d25b86b": {"__data__": {"id_": "c808a3ee-24c5-4f4d-836b-072d2d25b86b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "4e493a992e31411a8b58e9b811cf0a82"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91fb2685a01f700f1014320fe55a2bec9f38453a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md"}, "hash": "1d81d51e155836af5c2200ef0309c3f7b1a0565c92429663aad7eb7ed4c1ea62"}, "3": {"node_id": "94dc960a-7081-48f4-8960-4dfa0b74d134", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "8a4b0246c84e761925f5b989273ea23f"}, "hash": "6a6fc9fc30ae81d52dc62f40da29fa752c98ffd196fb859dc1aa7d762bbb3614"}}, "hash": "abdb3a1406b20d91183b49232ca440bb6a2a9dd37f34dda6d0882cfa2a534131", "text": "(kuberay-troubleshootin-guides)=\n\n# Troubleshooting guide\n\nThis document addresses common inquiries.If you don't find an answer to your question here, please don't hesitate to connect with us via our [community channels](https://github.com/ray-project/kuberay#getting-involved).# Contents\n\n- [Worker init container](#worker-init-container)\n- [Cluster domain](#cluster-domain)\n- [RayService](#rayservice)\n- [Other questions](#questions)\n\n## Worker init container\n\nThe KubeRay operator will inject a default [init container](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/) into every worker Pod.This init container is responsible for waiting until the Global Control Service (GCS) on the head Pod is ready before establishing a connection to the head.The init container will use `ray health-check` to check the GCS server status continuously.The default worker init container may not work for all use cases, or users may want to customize the init container.### 1.Init container troubleshooting\n\nSome common causes for the worker init container to stuck in `Init:0/1` status are:\n\n* The GCS server process has failed in the head Pod.Please inspect the log directory `/tmp/ray/session_latest/logs/` in the head Pod for errors related to the GCS server.* The `ray` executable is not included in the `$PATH` for the image, so the init container will fail to run `ray health-check`.* The `CLUSTER_DOMAIN` environment variable is not set correctly.See the section [cluster domain](#cluster-domain) for more details.* The worker init container shares the same ***ImagePullPolicy***, ***SecurityContext***, ***Env***, ***VolumeMounts***, and ***Resources*** as the worker Pod template.Sharing these settings is possible to cause a deadlock.See [#1130](https://github.com/ray-project/kuberay/issues/1130) for more details.If the init container remains stuck in `Init:0/1` status for 2 minutes, we will stop redirecting the output messages to `/dev/null` and instead print them to the worker Pod logs.To troubleshoot further, you can inspect the logs using `kubectl logs`.### 2.Disable the init container injection\n\nIf you want to customize the worker init container, you can disable the init container injection and add your own.To disable the injection, set the `ENABLE_INIT_CONTAINER_INJECTION` environment variable in the KubeRay operator to `false` (applicable from KubeRay v0.5.2).Please refer to [#1069](https://github.com/ray-project/kuberay/pull/1069) and the [KubeRay Helm chart](https://github.com/ray-project/kuberay/blob/ddb5e528c29c2e1fb80994f05b1bd162ecbaf9f2/helm-chart/kuberay-operator/values.yaml#L83-L87) for instructions on how to set the environment variable.Once disabled, you can add your custom init container to the worker Pod template.## Cluster domain\n\nIn KubeRay, we use Fully Qualified Domain Names (FQDNs) to establish connections between workers and the head.The FQDN of the head service is `${HEAD_SVC}.${NAMESPACE}.svc.${CLUSTER_DOMAIN}`.The default [cluster domain](https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#introduction) is `cluster.local`, which works for most Kubernetes clusters.However, it's important to note that some clusters may have a different cluster domain.You can check the cluster domain of your Kubernetes cluster by checking `/etc/resolv.conf` in a Pod.To set a custom cluster domain, adjust the `CLUSTER_DOMAIN` environment variable in the KubeRay operator.Helm chart users can make this modification [here](https://github.com/ray-project/kuberay/blob/ddb5e528c29c2e1fb80994f05b1bd162ecbaf9f2/helm-chart/kuberay-operator/values.yaml#L88-L91).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "94dc960a-7081-48f4-8960-4dfa0b74d134": {"__data__": {"id_": "94dc960a-7081-48f4-8960-4dfa0b74d134", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "8a4b0246c84e761925f5b989273ea23f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "91fb2685a01f700f1014320fe55a2bec9f38453a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md"}, "hash": "1d81d51e155836af5c2200ef0309c3f7b1a0565c92429663aad7eb7ed4c1ea62"}, "2": {"node_id": "c808a3ee-24c5-4f4d-836b-072d2d25b86b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "4e493a992e31411a8b58e9b811cf0a82"}, "hash": "abdb3a1406b20d91183b49232ca440bb6a2a9dd37f34dda6d0882cfa2a534131"}}, "hash": "6a6fc9fc30ae81d52dc62f40da29fa752c98ffd196fb859dc1aa7d762bbb3614", "text": "For more information, please refer to [#951](https://github.com/ray-project/kuberay/pull/951) and [#938](https://github.com/ray-project/kuberay/pull/938) for more details.## RayService\n\nRayService is a Custom Resource Definition (CRD) designed for Ray Serve.In KubeRay, creating a RayService will first create a RayCluster and then\ncreate Ray Serve applications once the RayCluster is ready.If the issue pertains to the data plane, specifically your Ray Serve scripts \nor Ray Serve configurations (`serveConfigV2`), troubleshooting may be challenging.See [rayservice-troubleshooting](kuberay-raysvc-troubleshoot) for more details.## Questions\n\n### Why are my changes to RayCluster/RayJob CR not taking effect?Currently, only modifications to the `replicas` field in `RayCluster/RayJob` CR are supported.Changes to other fields may not take effect or could lead to unexpected results.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "acdb9e12-24f2-4e76-9b8b-4f17d57be912": {"__data__": {"id_": "acdb9e12-24f2-4e76-9b8b-4f17d57be912", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides.md", "file_name": "user-guides.md", "text_hash": "27878cfd97b68513207dd16000610c2c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "db32fd9ae4dbd777a9e04c19455dc94c2da0a76f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides.md", "file_name": "user-guides.md"}, "hash": "5adfed16f510a59659ccf06efc82bd35f46bf25bf626a7d168ae48083d96af69"}}, "hash": "2e9e2a6478815d5ef3bda3960276c072784b72c36ce61f34530775ae2a97875e", "text": "(kuberay-guides)=\n\n# User Guides\n\n:::{note}\nTo learn the basics of Ray on Kubernetes, we recommend taking a look\nat the {ref}`introductory guide <kuberay-quickstart>` first.\n:::\n\nIn these guides, we go into further depth on several topics related to\ndeployments of Ray on Kubernetes.\n\n* {ref}`kuberay-k8s-setup`\n* {ref}`kuberay-config`\n* {ref}`kuberay-autoscaling`\n* {ref}`kuberay-gpu`\n* {ref}`kuberay-logging`\n* {ref}`kuberay-dev-serve`\n* {ref}`kuberay-experimental`\n* {ref}`kuberay-pod-command`\n* {ref}`kuberay-pod-security`\n* {ref}`kuberay-tls`\n* {ref}`deploy-a-static-ray-cluster-without-kuberay`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4aa497ba-7b26-4649-9ba7-489e4b0fe4d1": {"__data__": {"id_": "4aa497ba-7b26-4649-9ba7-489e4b0fe4d1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md", "text_hash": "a0c72012a478692955bf3996b391b77d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90359a47da74d7df69371b361d0d214655b93769", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md"}, "hash": "5a096ac9ea92e780d4c9b4f4863416c655e8f8d6acf1e73e0436d713ace09fcc"}, "3": {"node_id": "67da064b-5cd5-4f0a-809a-5569c18b5696", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md", "text_hash": "8370f4748e757feba049660d53efcb06"}, "hash": "03b910dc4b0183ec88fd8be893e06865b33ef75aab2be8950a51bb9e3d63293c"}}, "hash": "e5a7a3a43b8c537aa220f28adfac4cb4b76a22bcf439f84b119d0ace312524cf", "text": "(kuberay-eks-gpu-cluster-setup)=\n\n# Start Amazon EKS Cluster with GPUs for KubeRay\n\nThis guide walks you through the steps to create an Amazon EKS cluster with GPU nodes specifically for KubeRay.The configuration outlined here can be applied to most KubeRay examples found in the documentation.## Step 1: Create a Kubernetes cluster on Amazon EKS\n\nFollow the first two steps in [this AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html#) to: \n(1) create your Amazon EKS cluster and (2) configure your computer to communicate with your cluster.## Step 2: Create node groups for the Amazon EKS cluster\n\nFollow \"Step 3: Create nodes\" in [this AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html#) to create node groups.The following section provides more detailed information.### Create a CPU node group\n\nTypically, avoid running GPU workloads on the Ray head.Create a CPU node group for all Pods except Ray GPU \nworkers, such as the KubeRay operator, Ray head, and CoreDNS Pods.Here's a common configuration that works for most KubeRay examples in the docs:\n  * Instance type: [**m5.xlarge**](https://aws.amazon.com/ec2/instance-types/m5/) (4 vCPU; 16 GB RAM)\n  * Disk size: 256 GB\n  * Desired size: 1, Min size: 0, Max size: 1\n\n### Create a GPU node group\n\nCreate a GPU node group for Ray GPU workers.1.Here's a common configuration that works for most KubeRay examples in the docs:\n   * AMI type: Bottlerocket NVIDIA (BOTTLEROCKET_x86_64_NVIDIA)\n   * Instance type: [**g5.xlarge**](https://aws.amazon.com/ec2/instance-types/g5/) (1 GPU; 24 GB GPU Memory; 4 vCPUs; 16 GB RAM)\n   * Disk size: 1024 GB\n   * Desired size: 1, Min size: 0, Max size: 1\n\n> **Note:** If you encounter permission issues with `kubectl`, follow \"Step 2: Configure your computer to communicate with your cluster\"\nin the [AWS documentation](https://docs.aws.amazon.com/eks/latest/userguide/getting-started-console.html#).2.Please install the NVIDIA device plugin.(Note: You can skip this step if you used the `BOTTLEROCKET_x86_64_NVIDIA` AMI in the step above.)* Install the DaemonSet for NVIDIA device plugin to run GPU enabled containers in your Amazon EKS cluster.You can refer to the [Amazon EKS optimized accelerated Amazon Linux AMIs](https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami)\n   or [NVIDIA/k8s-device-plugin](https://github.com/NVIDIA/k8s-device-plugin) repository for more details.* If the GPU nodes have taints, add `tolerations` to `nvidia-device-plugin.yml` to enable the DaemonSet to schedule Pods on the GPU nodes.```sh\n# Install the DaemonSet\nkubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.9.0/nvidia-device-plugin.yml\n\n# Verify that your nodes have allocatable GPUs.If the GPU node fails to detect GPUs,\n# please verify whether the DaemonSet schedules the Pod on the GPU node.kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                GPU\n# ip-....us-west-2.compute.internal   4\n# ip-....us-west-2.compute.internal   <none>\n```\n\n3.Add a Kubernetes taint to prevent scheduling CPU Pods on this GPU node group.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "67da064b-5cd5-4f0a-809a-5569c18b5696": {"__data__": {"id_": "67da064b-5cd5-4f0a-809a-5569c18b5696", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md", "text_hash": "8370f4748e757feba049660d53efcb06"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "90359a47da74d7df69371b361d0d214655b93769", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md"}, "hash": "5a096ac9ea92e780d4c9b4f4863416c655e8f8d6acf1e73e0436d713ace09fcc"}, "2": {"node_id": "4aa497ba-7b26-4649-9ba7-489e4b0fe4d1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md", "text_hash": "a0c72012a478692955bf3996b391b77d"}, "hash": "e5a7a3a43b8c537aa220f28adfac4cb4b76a22bcf439f84b119d0ace312524cf"}}, "hash": "03b910dc4b0183ec88fd8be893e06865b33ef75aab2be8950a51bb9e3d63293c", "text": "For KubeRay examples, add the following taint to the GPU nodes: `Key: ray.io/node-type, Value: worker, Effect: NoSchedule`, and include the corresponding `tolerations` for GPU Ray worker Pods.> Warning: GPU nodes are extremely expensive.Please remember to delete the cluster if you no longer need it.## Step 3: Verify the node groups\n\n> **Note:** If you encounter permission issues with `eksctl`, navigate to your AWS account's webpage and copy the\ncredential environment variables, including `AWS_ACCESS_KEY_ID`, `AWS_SECRET_ACCESS_KEY`, and `AWS_SESSION_TOKEN`,\nfrom the \"Command line or programmatic access\" page.```sh\neksctl get nodegroup --cluster ${YOUR_EKS_NAME}\n\n# CLUSTER         NODEGROUP       STATUS  CREATED                 MIN SIZE        MAX SIZE        DESIRED CAPACITY        INSTANCE TYPE   IMAGE ID                        ASG NAME                           TYPE\n# ${YOUR_EKS_NAME}     cpu-node-group  ACTIVE  2023-06-05T21:31:49Z    0               1               1                       m5.xlarge       AL2_x86_64                      eks-cpu-node-group-...     managed\n# ${YOUR_EKS_NAME}     gpu-node-group  ACTIVE  2023-06-05T22:01:44Z    0               1               1                       g5.12xlarge     BOTTLEROCKET_x86_64_NVIDIA      eks-gpu-node-group-...     managed\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "501a0fe0-a994-4240-9fc0-d0fb6f9d17b6": {"__data__": {"id_": "501a0fe0-a994-4240-9fc0-d0fb6f9d17b6", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "f6b2ac2406bc0d88d52b987c1bc38e16"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md"}, "hash": "816e4dc50386bca588a6d4011ba36de5e65a17eb5db156c16d38079a34b7c18e"}, "3": {"node_id": "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "63a3ba3710ad502c6144448a3647cd63"}, "hash": "b8e685da4862a05b545916cd59781787506783f1b29980c5de66344a690b4431"}}, "hash": "5f8c92f0121952f79b36dfd015aef2718a38e0b57b0a08adccca552ccb515f8b", "text": "(kuberay-config)=\n\n# RayCluster Configuration\n\nThis guide covers the key aspects of Ray cluster configuration on Kubernetes.## Introduction\n\nDeployments of Ray on Kubernetes follow the [operator pattern](https://kubernetes.io/docs/concepts/extend-kubernetes/operator/).The key players are\n- A [custom resource](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/)\n    called a `RayCluster` describing the desired state of a Ray cluster.- A [custom controller](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#custom-controllers),\n    the KubeRay operator, which manages Ray pods in order to match the `RayCluster`'s spec.To deploy a Ray cluster, one creates a `RayCluster` custom resource (CR):\n```shell\nkubectl apply -f raycluster.yaml\n```\n\nThis guide covers the salient features of `RayCluster` CR configuration.For reference, here is a condensed example of a `RayCluster` CR in yaml format.```yaml\napiVersion: ray.io/v1alpha1\nkind: RayCluster\nmetadata:\n  name: raycluster-complete\nspec:\n  rayVersion: \"2.3.0\"\n  enableInTreeAutoscaling: true\n  autoscalerOptions:\n     ...\n  headGroupSpec:\n    serviceType: ClusterIP # Options are ClusterIP, NodePort, and LoadBalancer\n    rayStartParams:\n      dashboard-host: \"0.0.0.0\"\n      ...\n    template: # Pod template\n        metadata: # Pod metadata\n        spec: # Pod spec\n            containers:\n            - name: ray-head\n              image: rayproject/ray-ml:2.3.0\n              resources:\n                limits:\n                  cpu: 14\n                  memory: 54Gi\n                requests:\n                  cpu: 14\n                  memory: 54Gi\n              # Keep this preStop hook in each Ray container config.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1": {"__data__": {"id_": "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "63a3ba3710ad502c6144448a3647cd63"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md"}, "hash": "816e4dc50386bca588a6d4011ba36de5e65a17eb5db156c16d38079a34b7c18e"}, "2": {"node_id": "501a0fe0-a994-4240-9fc0-d0fb6f9d17b6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "f6b2ac2406bc0d88d52b987c1bc38e16"}, "hash": "5f8c92f0121952f79b36dfd015aef2718a38e0b57b0a08adccca552ccb515f8b"}, "3": {"node_id": "3b9b406e-9259-4f36-823b-913504fc5c67", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "62367d8119eb9bd49565cc9031ce8c8d"}, "hash": "fb368580f0bee3238dcc75f71d5bfba871813a183143520e6f7a2e97b0a0dd54"}}, "hash": "b8e685da4862a05b545916cd59781787506783f1b29980c5de66344a690b4431", "text": "lifecycle:\n                preStop:\n                  exec:\n                    command: [\"/bin/sh\",\"-c\",\"ray stop\"]\n              ports: # Optional service port overrides\n              - containerPort: 6379\n                name: gcs\n              - containerPort: 8265\n                name: dashboard\n              - containerPort: 10001\n                name: client\n              - containerPort: 8000\n                name: serve\n                ...\n  workerGroupSpecs:\n  - groupName: small-group\n    replicas: 1\n    minReplicas: 1\n    maxReplicas: 5\n    rayStartParams:\n        ...\n    template: # Pod template\n      spec:\n        ...\n  # Another workerGroup\n  - groupName: medium-group\n    ...\n  # Yet another workerGroup, with access to special hardware perhaps.- groupName: gpu-group\n    ...\n```\n\nThe rest of this guide will discuss the `RayCluster` CR's config fields.See also the [guide](kuberay-autoscaling-config) on configuring Ray autoscaling with KubeRay.(kuberay-config-ray-version)=\n## The Ray Version\nThe field `rayVersion` specifies the version of Ray used in the Ray cluster.The `rayVersion` is used to fill default values for certain config fields.The Ray container images specified in the RayCluster CR should carry\nthe same Ray version as the CR's `rayVersion`.If you are using a nightly or development\nRay image, it is fine to set `rayVersion` to the latest release version of Ray.## Pod configuration: headGroupSpec and workerGroupSpecs\n\nAt a high level, a RayCluster is a collection of Kubernetes pods, similar to a Kubernetes Deployment or StatefulSet.Just as with the Kubernetes built-ins, the key pieces of configuration are\n* Pod specification\n* Scale information (how many pods are desired)\n\nThe key difference between a Deployment and a `RayCluster` is that a `RayCluster` is\nspecialized for running Ray applications.A Ray cluster consists of\n\n* One **head pod** which hosts global control processes for the Ray cluster.The head pod can also run Ray tasks and actors.* Any number of **worker pods**, which run Ray tasks and actors.Workers come in **worker groups** of identically configured pods.For each worker group, we must specify **replicas**, the number of\n  pods we want of that group.The head pod\u2019s configuration is\nspecified under `headGroupSpec`, while configuration for worker pods is\nspecified under `workerGroupSpecs`.There may be multiple worker groups,\neach group with its own configuration.The `replicas` field\nof a `workerGroupSpec` specifies the number of worker pods of that group to\nkeep in the cluster.Each `workerGroupSpec` also has optional `minReplicas` and\n`maxReplicas` fields; these fields are important if you wish to enable {ref}`autoscaling <kuberay-autoscaling-config>`.### Pod templates\nThe bulk of the configuration for a `headGroupSpec` or\n`workerGroupSpec` goes in the `template` field.The `template` is a Kubernetes Pod\ntemplate which determines the configuration for the pods in the group.Here are some of the subfields of the pod `template` to pay attention to:\n\n#### containers\nA Ray pod template specifies at minimum one container, namely the container\nthat runs the Ray processes.A Ray pod template may also specify additional sidecar\ncontainers, for purposes such as {ref}`log processing <kuberay-logging>`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3b9b406e-9259-4f36-823b-913504fc5c67": {"__data__": {"id_": "3b9b406e-9259-4f36-823b-913504fc5c67", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "62367d8119eb9bd49565cc9031ce8c8d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md"}, "hash": "816e4dc50386bca588a6d4011ba36de5e65a17eb5db156c16d38079a34b7c18e"}, "2": {"node_id": "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "63a3ba3710ad502c6144448a3647cd63"}, "hash": "b8e685da4862a05b545916cd59781787506783f1b29980c5de66344a690b4431"}, "3": {"node_id": "617b32f1-cd1f-4683-a4b7-8d9abb607760", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "93eb44d5d812a96c815520b2a5b66094"}, "hash": "6bb8a9e5490668be2ce91224294160cb1193ba5c62e443cc96393768e4805ffa"}}, "hash": "fb368580f0bee3238dcc75f71d5bfba871813a183143520e6f7a2e97b0a0dd54", "text": "However, the KubeRay operator assumes that\nthe first container in the containers list is the main Ray container.Therefore, make sure to specify any sidecar containers\n**after** the main Ray container.In other words, the Ray container should be the **first**\nin the `containers` list.#### resources\nIt\u2019s important to specify container CPU and memory requests and limits for\neach group spec.For GPU workloads, you may also wish to specify GPU\nlimits.For example, set `nvidia.com/gpu:2` if using an Nvidia GPU device plugin\nand you wish to specify a pod with access to 2 GPUs.See {ref}`this guide <kuberay-gpu>` for more details on GPU support.It's ideal to size each Ray pod to take up the\nentire Kubernetes node on which it is scheduled.In other words, it\u2019s\nbest to run one large Ray pod per Kubernetes node.In general, it is more efficient to use a few large Ray pods than many small ones.The pattern of fewer large Ray pods has the following advantages:\n- more efficient use of each Ray pod's shared memory object store\n- reduced communication overhead between Ray pods\n- reduced redundancy of per-pod Ray control structures such as Raylets\n\nThe CPU, GPU, and memory **limits** specified in the Ray container config\nwill be automatically advertised to Ray.These values will be used as\nthe logical resource capacities of Ray pods in the head or worker group.Note that CPU quantities will be rounded up to the nearest integer\nbefore being relayed to Ray.The resource capacities advertised to Ray may be overridden in the {ref}`rayStartParams`.On the other hand CPU, GPU, and memory **requests** will be ignored by Ray.For this reason, it is best when possible to **set resource requests equal to resource limits**.#### nodeSelector and tolerations\nYou can control the scheduling of worker groups' Ray pods by setting the `nodeSelector` and\n`tolerations` fields of the pod spec.Specifically, these fields determine on which Kubernetes\nnodes the pods may be scheduled.See the [Kubernetes docs](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/)\nfor more about Pod-to-Node assignment.#### image\nThe Ray container images specified in the `RayCluster` CR should carry\nthe same Ray version as the CR's `spec.rayVersion`.If you are using a nightly or development Ray image, it is fine to specify Ray's\nlatest release version under `spec.rayVersion`.Code dependencies for a given Ray task or actor must be installed on each Ray node that\nmight run the task or actor.To achieve this, it is simplest to use the same Ray image for the Ray head and all worker groups.In any case, do make sure that all Ray images in your CR carry the same Ray version and\nPython version.To distribute custom code dependencies across your cluster, you can build a custom container image,\nusing one of the [official Ray images](https://hub.docker.com/r/rayproject/ray>) as the base.See {ref}`this guide <docker-images>` to learn more about the official Ray images.For dynamic dependency management geared towards iteration and developement,\nyou can also use {ref}`Runtime Environments <runtime-environments>`.#### metadata.name and metadata.generateName\nThe KubeRay operator will ignore the values of `metadata.name` and `metadata.generateName` set by users.The KubeRay operator will generate a `generateName` automatically to avoid name conflicts.See [KubeRay issue #587](https://github.com/ray-project/kuberay/pull/587) for more details.(rayStartParams)=\n## Ray Start Parameters\nThe ``rayStartParams`` field of each group spec is a string-string map of arguments to the Ray\ncontainer\u2019s `ray start` entrypoint.For the full list of arguments, refer to\nthe documentation for {ref}`ray start <ray-start-doc>`.We make special note of the following arguments:\n\n### dashboard-host\nFor most use-cases, this field should be set to \"0.0.0.0\" for the Ray head pod.This is required to expose the Ray dashboard outside the Ray cluster.(Future versions might set\nthis parameter by default.)(kuberay-num-cpus)=\n### num-cpus\nThis optional field tells the Ray scheduler and autoscaler how many CPUs are\navailable to the Ray pod.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "617b32f1-cd1f-4683-a4b7-8d9abb607760": {"__data__": {"id_": "617b32f1-cd1f-4683-a4b7-8d9abb607760", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "93eb44d5d812a96c815520b2a5b66094"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md"}, "hash": "816e4dc50386bca588a6d4011ba36de5e65a17eb5db156c16d38079a34b7c18e"}, "2": {"node_id": "3b9b406e-9259-4f36-823b-913504fc5c67", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "62367d8119eb9bd49565cc9031ce8c8d"}, "hash": "fb368580f0bee3238dcc75f71d5bfba871813a183143520e6f7a2e97b0a0dd54"}, "3": {"node_id": "692db461-266e-44e2-b76b-9c0b1559b214", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "f69b425644d8d1ff0efdb7e851a6ffcd"}, "hash": "ad3cac5c0a6c25def3884c043b392f0c0629fbb7d19867c80deaeb22f630439e"}}, "hash": "6bb8a9e5490668be2ce91224294160cb1193ba5c62e443cc96393768e4805ffa", "text": "The CPU count can be autodetected from the\nKubernetes resource limits specified in the group spec\u2019s pod\n`template`.However, it is sometimes useful to override this autodetected\nvalue.For example, setting `num-cpus:\"0\"` for the Ray head pod will prevent Ray\nworkloads with non-zero CPU requirements from being scheduled on the head.Note that the values of all Ray start parameters, including `num-cpus`,\nmust be supplied as **strings**.### num-gpus\nThis field specifies the number of GPUs available to the Ray container.In future KubeRay versions, the number of GPUs will be auto-detected from Ray container resource limits.Note that the values of all Ray start parameters, including `num-gpus`,\nmust be supplied as **strings**.### memory\nThe memory available to the Ray is detected automatically from the Kubernetes resource\nlimits.If you wish, you may override this autodetected value by setting the desired memory value,\nin bytes, under `rayStartParams.memory`.Note that the values of all Ray start parameters, including `memory`,\nmust be supplied as **strings**.### resources\nThis field can be used to specify custom resource capacities for the Ray pod.These resource capacities will be advertised to the Ray scheduler and Ray autoscaler.For example, the following annotation will mark a Ray pod as having 1 unit of `Custom1` capacity\nand 5 units of `Custom2` capacity.```yaml\nrayStartParams:\n    resources: '\"{\\\"Custom1\\\": 1, \\\"Custom2\\\": 5}\"'\n```\nYou can then annotate tasks and actors with annotations like `@ray.remote(resources={\"Custom2\": 1})`.The Ray scheduler and autoscaler will take appropriate action to schedule such tasks.Note the format used to express the resources string.In particular, note\nthat the backslashes are present as actual characters in the string.If you are specifying a `RayCluster` programmatically, you may have to\n[escape the backslashes](https://github.com/ray-project/ray/blob/cd9cabcadf1607bcda1512d647d382728055e688/python/ray/tests/kuberay/test_autoscaling_e2e.py#L92) to make sure they are processed as part of the string.The field `rayStartParams.resources` should only be used for custom resources.The keys\n`CPU`, `GPU`, and `memory` are forbidden.If you need to specify overrides for those resource\nfields, use the Ray start parameters `num-cpus`, `num-gpus`, or `memory`.(kuberay-networking)=\n## Services and Networking\n### The Ray head service.The KubeRay operator automatically configures a Kubernetes Service exposing the default ports\nfor several services of the Ray head pod, including\n- Ray Client (default port 10001)\n- Ray Dashboard (default port 8265)\n- Ray GCS server (default port 6379)\n- Ray Serve (default port 8000)\n- Ray Prometheus metrics (default port 8080)\n\nThe name of the configured Kubernetes Service is the name, `metadata.name`, of the RayCluster\nfollowed by the suffix <nobr>`head-svc`</nobr>.For the example CR given on this page, the name of\nthe head service will be\n<nobr>`raycluster-example-head-svc`</nobr>.Kubernetes networking (`kube-dns`) then allows us to address\nthe Ray head's services using the name <nobr>`raycluster-example-head-svc`</nobr>.For example, the Ray Client server can be accessed from a pod\nin the same Kubernetes namespace using\n```python\nray.init(\"ray://raycluster-example-head-svc:10001\")\n```\nThe Ray Client server can be accessed from a pod in another namespace using\n```python\nray.init(\"ray://raycluster-example-head-svc.default.svc.cluster.local:10001\")\n```\n(This assumes the Ray cluster was deployed into the default Kubernetes namespace.If the Ray cluster is deployed in a non-default namespace, use that namespace in\nplace of `default`.)### ServiceType, Ingresses\nRay Client and other services can be exposed outside the Kubernetes cluster\nusing port-forwarding or an ingress.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "692db461-266e-44e2-b76b-9c0b1559b214": {"__data__": {"id_": "692db461-266e-44e2-b76b-9c0b1559b214", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "f69b425644d8d1ff0efdb7e851a6ffcd"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md"}, "hash": "816e4dc50386bca588a6d4011ba36de5e65a17eb5db156c16d38079a34b7c18e"}, "2": {"node_id": "617b32f1-cd1f-4683-a4b7-8d9abb607760", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "93eb44d5d812a96c815520b2a5b66094"}, "hash": "6bb8a9e5490668be2ce91224294160cb1193ba5c62e443cc96393768e4805ffa"}}, "hash": "ad3cac5c0a6c25def3884c043b392f0c0629fbb7d19867c80deaeb22f630439e", "text": "The simplest way to access the Ray head's services is to use port-forwarding.Other means of exposing the head's services outside the cluster may require using\na service of type LoadBalancer or NodePort.Set `headGroupSpec.serviceType`\nto the appropriate type for your application.You may wish to set up an ingress to expose the Ray head's services outside the cluster.See the [KubeRay documentation][IngressDoc] for details.### Specifying non-default ports.If you wish to override the ports exposed by the Ray head service, you may do so by specifying\nthe Ray head container's `ports` list, under `headGroupSpec`.Here is an example of a list of non-default ports for the Ray head service.```yaml\nports:\n- containerPort: 6380\n  name: gcs\n- containerPort: 8266\n  name: dashboard\n- containerPort: 10002\n  name: client\n```\nIf the head container's `ports` list is specified, the Ray head service will expose precisely\nthe ports in the list.In the above example, the head service will expose just three ports;\nin particular there will be no port exposed for Ray Serve.For the Ray head to actually use the non-default ports specified in the ports list,\nyou must also specify the relevant `rayStartParams`.For the above example,\n```yaml\nrayStartParams:\n  port: \"6380\"\n  dashboard-port: \"8266\"\n  ray-client-server-port: \"10002\"\n  ...\n```\n(kuberay-config-miscellaneous)=\n## Pod and container lifecyle: preStopHook\n\nIt is recommended for every Ray container's configuration\nto include the following blocking block:\n```yaml\nlifecycle:\n  preStop:\n    exec:\n      command: [\"/bin/sh\",\"-c\",\"ray stop\"]\n```\nTo ensure graceful termination, `ray stop` is executed prior to the Ray pod's termination.[IngressDoc]: https://ray-project.github.io/kuberay/guidance/ingress/", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64144582-8a17-423f-b2d9-aa6320573414": {"__data__": {"id_": "64144582-8a17-423f-b2d9-aa6320573414", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "09b68e65a812106f53b5b019b658516c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4267bc028bc3831c121c91893412728f50930e6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md"}, "hash": "fda25715cc5a505bdb375bc79845504ab5f36569275fb2a2c98e4e813aff1350"}, "3": {"node_id": "c4bfc911-a085-4f49-b354-fdc0a7752175", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "b276cdec3914828aa352f2cf93db9e00"}, "hash": "8ef266aa6667fad83d2f0bfbf50e57f310f6f91809a3c033804ec79a3ec8130b"}}, "hash": "0817f9b8e87be4a1bbf7ca3b3e11e1c02fbab62379e7f1f88d2b4ccab216a872", "text": "(kuberay-autoscaling)=\n\n# KubeRay Autoscaling\n\nThis guide explains how to configure the Ray autoscaler on Kubernetes.The Ray autoscaler is a Ray cluster process that automatically scales a cluster up and down based on resource demand.The autoscaler does this by adjusting the number of nodes (Ray pods) in the cluster based on the resources required by tasks, actors or placement groups.Note that the autoscaler only considers logical resource requests for scaling (i.e., those specified in ``@ray.remote`` and displayed in `ray status`), not physical machine utilization.If a user tries to launch an actor, task, or placement group but there are insufficient resources, the request will be queued.The autoscaler adds nodes to satisfy resource demands in this queue.The autoscaler also removes nodes after they become idle for some time.A node is considered idle if it has no active tasks, actors, or objects.<!-- TODO(ekl): probably should change the default kuberay examples to not use autoscaling -->\n```{admonition} When to use Autoscaling?Autoscaling can reduce workload costs, but adds node launch overheads and can be tricky to configure.We recommend starting with non-autoscaling clusters if you're new to Ray.```\n\n## Overview\nThe following diagram illustrates the integration of the Ray Autoscaler\nwith the KubeRay operator.```{eval-rst}\n.. image:: ../images/AutoscalerOperator.svg\n    :align: center\n..\n    Find the source document here (https://docs.google.com/drawings/d/1LdOg9JQuN5AOII-vDpSaFBsTeg0JGWcsbyNNLP1yovg/edit)\n```\n\nWorker pod upscaling occurs through the following sequence of events:\n1.The user submits a Ray workload.2.Workload resource requirements are aggregated by the Ray head container\n   and communicated to the Ray autoscaler sidecar.3.The autoscaler determines that a Ray worker pod must be added to satisfy the workload's resource requirement.4.The autoscaler requests an additional worker pod by incrementing the RayCluster CR's `replicas` field.5.The KubeRay operator creates a Ray worker pod to match the new `replicas` specification.6.The Ray scheduler places the user's workload on the new worker pod.See also the operator architecture diagram in the [KubeRay documentation](https://ray-project.github.io/kuberay/guidance/autoscaler/).## Quickstart\n\nFirst, follow the [KubeRay quickstart guide](kuberay-quickstart) to:\n* Install `kubectl` and `Helm`\n* Prepare a Kubernetes cluster\n* Deploy a KubeRay operator\n\nNext, create an autoscaling RayCluster custom resource.```bash\n# Create an autoscaling RayCluster custom resource.kubectl apply -f https://raw.githubusercontent.com/ray-project/kuberay/release-0.5/ray-operator/config/samples/ray-cluster.autoscaler.yaml\n```\n\nNow, we can run a Ray program on the head pod that uses [``request_resources``](ref-autoscaler-sdk) to scale the cluster to a total of 3 CPUs.The head and worker pods in our [example cluster config](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.autoscaler.yaml) each have a capacity of 1 CPU, and we specified a minimum of 1 worker pod.Thus, the request should trigger upscaling of one additional worker pod.Note that in real-life scenarios, you will want to use larger Ray pods.In fact, it is advantageous to size each Ray pod to take up an entire Kubernetes node.See the [configuration guide](kuberay-config) for more details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4bfc911-a085-4f49-b354-fdc0a7752175": {"__data__": {"id_": "c4bfc911-a085-4f49-b354-fdc0a7752175", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "b276cdec3914828aa352f2cf93db9e00"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4267bc028bc3831c121c91893412728f50930e6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md"}, "hash": "fda25715cc5a505bdb375bc79845504ab5f36569275fb2a2c98e4e813aff1350"}, "2": {"node_id": "64144582-8a17-423f-b2d9-aa6320573414", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "09b68e65a812106f53b5b019b658516c"}, "hash": "0817f9b8e87be4a1bbf7ca3b3e11e1c02fbab62379e7f1f88d2b4ccab216a872"}, "3": {"node_id": "c29014f0-35e5-4fb8-911b-e29f1a8eac8b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "ab6e360750235b480cab9954d44fa6af"}, "hash": "d9f2e3c2c0dd19633e31636a4e6473d9f79f64469b809c2e1f9a8341b011b436"}}, "hash": "8ef266aa6667fad83d2f0bfbf50e57f310f6f91809a3c033804ec79a3ec8130b", "text": "To run the Ray program, we will first get the name of the Ray head pod:\n\n```bash\n# Trigger Ray Pod upscaling\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec $HEAD_POD -it -c ray-head -- python -c \"import ray; ray.init(); ray.autoscaler.sdk.request_resources(num_cpus=4)\"\n```\n\nThe last command should trigger the upscaling of the Ray pod by requesting 4 CPUs.Initially, there is one head Pod and one worker Pod, each with 1 CPU.Next, the autoscaler will create two new worker Pods to utilize the remaining 2 CPUs.```bash\nkubectl get pod --selector=ray.io/cluster=raycluster-autoscaler\n# NAME                                             READY   STATUS    RESTARTS   AGE\n# raycluster-autoscaler-head-pnb77                 2/2     Running   0          XXs\n# raycluster-autoscaler-worker-small-group-ng2p5   1/1     Running   0          XXs\n# raycluster-autoscaler-worker-small-group-s7pv6   1/1     Running   0          XXs\n# raycluster-autoscaler-worker-small-group-tb6d9   1/1     Running   0          XXs\n```\n\nTo get a summary of your cluster's status, run `ray status` on your cluster's Ray head node.```bash\nkubectl exec $HEAD_POD -it -c ray-head -- ray status\n# ======== Autoscaler status: 2023-04-07 xxxxxxxxxx ========\n# ....\n```\n\nAlternatively, to examine the full autoscaling logs, fetch the stdout of the Ray head pod's autoscaler sidecar:\n```bash\n# This command gets the last 20 lines of autoscaler logs.kubectl logs $HEAD_POD -c autoscaler | tail -n 20\n# ======== Autoscaler status: 2023-04-07 xxxxxxxxxx ========\n# ...\n```\n\n(kuberay-autoscaling-config)=\n## KubeRay Config Parameters\n\nThere are two steps to enabling Ray autoscaling in the KubeRay `RayCluster` custom resource (CR) config:\n\n1.Set `enableInTreeAutoscaling:true`.The KubeRay operator will then automatically configure an autoscaling sidecar container\nfor the Ray head pod.The autoscaler container collects resource metrics from the Ray cluster\nand automatically adjusts the `replicas` field of each `workerGroupSpec` as needed to fulfill\nthe requirements of your Ray application.2.Set the fields `minReplicas` and `maxReplicas` to constrain the number of `replicas` of an autoscaling\n`workerGroup`.When deploying an autoscaling cluster, one typically sets `replicas` and `minReplicas`\nto the same value.The Ray autoscaler will then take over and modify the `replicas` field as pods are added to or removed from the cluster.For an example, check out the [config file](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.autoscaler.yaml) that we used in the above quickstart guide.### Upscaling and downscaling speed\n\nIf needed, you can also control the rate at which nodes should be added to or removed from the cluster.For applications with many short-lived tasks, you may wish to adjust the upscaling and downscaling speed to be more conservative.Use the `RayCluster` CR's `autoscalerOptions` field to do so.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c29014f0-35e5-4fb8-911b-e29f1a8eac8b": {"__data__": {"id_": "c29014f0-35e5-4fb8-911b-e29f1a8eac8b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "ab6e360750235b480cab9954d44fa6af"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4267bc028bc3831c121c91893412728f50930e6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md"}, "hash": "fda25715cc5a505bdb375bc79845504ab5f36569275fb2a2c98e4e813aff1350"}, "2": {"node_id": "c4bfc911-a085-4f49-b354-fdc0a7752175", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "b276cdec3914828aa352f2cf93db9e00"}, "hash": "8ef266aa6667fad83d2f0bfbf50e57f310f6f91809a3c033804ec79a3ec8130b"}, "3": {"node_id": "d6f02e6e-040d-463a-bbfb-dd4aed855935", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "708f928a2693172243cfc4cde39fca1a"}, "hash": "f369055b10c9b0ed616de46827dcfea1d286cb3570b997d1a5b3945969da17f7"}}, "hash": "d9f2e3c2c0dd19633e31636a4e6473d9f79f64469b809c2e1f9a8341b011b436", "text": "The `autoscalerOptions` field\ncarries the following subfields:\n\n**`upscalingMode`**: This controls the rate of Ray pod upscaling.The valid values are:\n\n  - `Conservative`: Upscaling is rate-limited; the number of pending worker pods is at most the number\n      of worker pods connected to the Ray cluster.- `Default`: Upscaling is not rate-limited.- `Aggressive`: An alias for Default; upscaling is not rate-limited.**`idleTimeoutSeconds`** (default 60s): This is the number of seconds to wait before scaling down an idle worker pod.Worker nodes are considered idle when they hold no active tasks, actors, or referenced objects (either in-memory or spilled to disk).### Configuring the autoscaler sidecar container\n\nThe `autoscalerOptions` field also provides options for configuring the autoscaler container.Usually, it is not necessary to specify these options.**`resources`**: The `resources` subfield of `autoscalerOptions` sets optional resource overrides\nfor the autoscaler sidecar container.These overrides\nshould be specified in the standard [container resource\nspec format](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#resources).The default values are indicated below:\n```\nresources:\n  limits:\n    cpu: \"500m\"\n    memory: \"512Mi\"\n  requests:\n    cpu: \"500m\"\n    memory: \"512Mi\"\n```\n\nThe following `autoscalerOptions` suboptions are also available for testing and development of the autoscaler itself.**`image`**: This field overrides the autoscaler container image.If your `RayCluster`'s `spec.RayVersion` is at least `2.0.0`, the autoscaler will default to using\n**the same image** as the Ray container.(Ray autoscaler code is bundled with the rest of Ray.)For older Ray versions, the autoscaler will default to the image `rayproject/ray:2.0.0`.**`imagePullPolicy`**: This field overrides the autoscaler container's\nimage pull policy.The default is `IfNotPresent`.**`env`** and **`envFrom`**: These fields specify autoscaler container\nenvironment variables.These fields should be formatted following the\n[Kubernetes API](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#environment-variables)\nfor container environment variables.## Understanding the Ray Autoscaler in the Context of Kubernetes\nWe describe the relationship between the Ray autoscaler and other autoscalers in the Kubernetes\necosystem.### Ray Autoscaler vs. Horizontal Pod Autoscaler\nThe Ray autoscaler adjusts the number of Ray nodes in a Ray cluster.On Kubernetes, each Ray node is run as a Kubernetes pod.Thus in the context of Kubernetes,\nthe Ray autoscaler scales Ray **pod quantities**.In this sense, the Ray autoscaler\nplays a role similar to that of the Kubernetes\n[Horizontal Pod Autoscaler](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/) (HPA).However, the following features distinguish the Ray Autoscaler from the HPA.#### Load metrics are based on application semantics\nThe Horizontal Pod Autoscaler determines scale based on physical usage metrics like CPU\nand memory.By contrast, the Ray autoscaler uses the logical resources expressed in\ntask and actor annotations.For instance, if each Ray container spec in your RayCluster CR indicates\na limit of 10 CPUs, and you submit twenty tasks annotated with `@ray.remote(num_cpus=5)`,\n10 Ray pods will be created to satisfy the 100-CPU resource demand.In this respect, the Ray autoscaler is similar to the\n[Kubernetes Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler),\nwhich makes scaling decisions based on the logical resources expressed in container\nresource requests.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d6f02e6e-040d-463a-bbfb-dd4aed855935": {"__data__": {"id_": "d6f02e6e-040d-463a-bbfb-dd4aed855935", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "708f928a2693172243cfc4cde39fca1a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a4267bc028bc3831c121c91893412728f50930e6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md"}, "hash": "fda25715cc5a505bdb375bc79845504ab5f36569275fb2a2c98e4e813aff1350"}, "2": {"node_id": "c29014f0-35e5-4fb8-911b-e29f1a8eac8b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "ab6e360750235b480cab9954d44fa6af"}, "hash": "d9f2e3c2c0dd19633e31636a4e6473d9f79f64469b809c2e1f9a8341b011b436"}}, "hash": "f369055b10c9b0ed616de46827dcfea1d286cb3570b997d1a5b3945969da17f7", "text": "#### Fine-grained control of scale-down\nTo accommodate the statefulness of Ray applications, the Ray autoscaler has more\nfine-grained control over scale-down than the Horizontal Pod Autoscaler.In addition to\ndetermining desired scale, the Ray Autoscaler is able to select precisely which pods\nto scale down.The KubeRay operator then deletes that pod.By contrast, the Horizontal Pod Autoscaler can only decrease a replica count, without much\ncontrol over which pods are deleted.For a Ray application, downscaling a random\npod could be dangerous.#### Architecture: One Ray Autoscaler per Ray Cluster.Horizontal Pod Autoscaling is centrally controlled by a manager in the Kubernetes control plane;\nthe manager controls the scale of many Kubernetes objects.By contrast, each Ray cluster is managed by its own Ray autoscaler process,\nrunning as a sidecar container in the Ray head pod.This design choice is motivated\nby the following considerations:\n- **Scalability.** Autoscaling each Ray cluster requires processing a significant volume of resource\n  data from that Ray cluster.- **Simplified versioning and compatibility.** The autoscaler and Ray are both developed\n  as part of the Ray repository.The interface between the autoscaler and the Ray core is complex.To support multiple Ray clusters running at different Ray versions, it is thus best to match\n  Ray and Autoscaler code versions.Running one autoscaler per Ray cluster and matching the code versions\n  ensures compatibility.(kuberay-autoscaler-with-ray-autoscaler)=\n### Ray Autoscaler with Kubernetes Cluster Autoscaler\nThe Ray Autoscaler and the\n[Kubernetes Cluster Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)\ncomplement each other.After the Ray autoscaler decides to create a Ray pod, the Kubernetes Cluster Autoscaler\ncan provision a Kubernetes node so that the pod can be placed.Similarly, after the Ray autoscaler decides to delete an idle pod, the Kubernetes\nCluster Autoscaler can clean up the idle Kubernetes node that remains.It is recommended to configure your RayCluster so that only one Ray pod fits per Kubernetes node.If you follow this pattern, Ray Autoscaler pod scaling events will correspond roughly one-to-one with cluster autoscaler\nnode scaling events.(We say \"roughly\" because it is possible for a Ray pod be deleted and replaced\nwith a new Ray pod before the underlying Kubernetes node is scaled down.)\n\n\n### Vertical Pod Autoscaler\nThere is no relationship between the Ray Autoscaler and the Kubernetes\n[Vertical Pod Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler) (VPA),\nwhich is meant to size individual pods to the appropriate size based on current and past usage.\nIf you find that the load on your individual Ray pods is too high, there are a number\nof manual techniques to decrease the load\nOne method is to schedule fewer tasks/actors per node by increasing the resource\nrequirements specified in the `ray.remote` annotation.\nFor example, changing `@ray.remote(num_cpus=2)` to `@ray.remote(num_cpus=4)`\nwill halve the quantity of that task or actor that can fit in a given Ray pod.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "591a2049-1865-43c7-bb12-ccb90d45c8c2": {"__data__": {"id_": "591a2049-1865-43c7-bb12-ccb90d45c8c2", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/experimental.md", "file_name": "experimental.md", "text_hash": "5f595cf4cb6a5402ca34fb022169a6a9"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "263ee108412167e79e111b51a63451c798fdb732", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/experimental.md", "file_name": "experimental.md"}, "hash": "f9086069ba81cc4ace14dcf1c8ef5245bae70b6c95d40378cd477b783723ac5f"}}, "hash": "997b8dd0dd7b4f4fbd7953fdeeb897814adaefc90126990fe68db2c571f931f2", "text": "(kuberay-experimental)=\n\n# Experimental Features\n\nWe provide an overview of new and experimental features available\nfor deployments of Ray on Kubernetes.\n\n## GCS Fault Tolerance\n\nIn addition to the application-level fault-tolerance provided by the RayService controller,\nRay now supports infrastructure-level fault tolerance for the Ray head pod.\n\nYou can set up an external Redis instance as a data store for the Ray head. If the Ray head crashes,\na new head will be created without restarting the Ray cluster.\nThe Ray head's GCS will recover its state from the external Redis instance.\n\nSee the {ref}`Ray Serve documentation <serve-e2e-ft-guide-gcs>` for more information and\nthe [KubeRay docs on GCS Fault Tolerance][KubeFT] for a detailed guide.\n\n[KubeServe]: https://ray-project.github.io/kuberay/guidance/rayservice/\n[KubeFT]: https://ray-project.github.io/kuberay/guidance/gcs-ft/\n[KubeJob]: https://ray-project.github.io/kuberay/guidance/rayjob/", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8058a0e1-2d1c-4567-9785-98bd1f1d1a19": {"__data__": {"id_": "8058a0e1-2d1c-4567-9785-98bd1f1d1a19", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md", "text_hash": "a0928a34fc4afac9d52e9583ff329eeb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f9ad0b8129b0df68f9d674ea508af7f576f0d2c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md"}, "hash": "66b190983627d1333ae862c77e4b506317ffa225bff9706ee3e8cc6c4d93c99e"}, "3": {"node_id": "34ce08a4-fe86-4552-b947-923d00090a59", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md", "text_hash": "0aedd41111a04bc4f26d595e13329986"}, "hash": "03cc36360206da8da1ac5eaf253cebdb3565214ca98f04f2259f25d9509e9a81"}}, "hash": "5b3441189a852a95d95820ddeb86493426285122778c2358e5e20f25fd9f424a", "text": "(kuberay-gke-gpu-cluster-setup)=\n\n# Start Google Cloud GKE Cluster with GPUs for KubeRay\n\n## Step 1: Create a Kubernetes cluster on GKE\n\nRun this command and all following commands on your local machine or on the [Google Cloud Shell](https://cloud.google.com/shell).If running from your local machine, you will need to install the [Google Cloud SDK](https://cloud.google.com/sdk/docs/install).The following command creates a Kubernetes cluster named `kuberay-gpu-cluster` with 1 CPU node in the `us-west1-b` zone.In this example, we use the `e2-standard-4` machine type, which has 4 vCPUs and 16 GB RAM.```sh\ngcloud container clusters create kuberay-gpu-cluster \\\n    --num-nodes=1 --min-nodes 0 --max-nodes 1 --enable-autoscaling \\\n    --zone=us-west1-b --machine-type e2-standard-4\n```\n\n> Note: You can also create a cluster from the [Google Cloud Console](https://console.cloud.google.com/kubernetes/list).## Step 2: Create a GPU node pool\n\nRun the following command to create a GPU node pool for Ray GPU workers.(You can also create it from the Google Cloud Console; see the [GKE documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/node-taints#create_a_node_pool_with_node_taints) for more details.)```sh\ngcloud container node-pools create gpu-node-pool \\\n  --accelerator type=nvidia-l4-vws,count=1 \\\n  --zone us-west1-b \\\n  --cluster kuberay-gpu-cluster \\\n  --num-nodes 1 \\\n  --min-nodes 0 \\\n  --max-nodes 1 \\\n  --enable-autoscaling \\\n  --machine-type g2-standard-4 \\\n  --node-taints=ray.io/node-type=worker:NoSchedule \n```\n\nThe `--accelerator` flag specifies the type and number of GPUs for each node in the node pool.In this example, we use the [NVIDIA L4](https://cloud.google.com/compute/docs/gpus#l4-gpus) GPU.The machine type `g2-standard-4` has 1 GPU, 24 GB GPU Memory, 4 vCPUs and 16 GB RAM.The taint `ray.io/node-type=worker:NoSchedule` prevents CPU-only Pods such as the Kuberay operator, Ray head, and CoreDNS Pods from being scheduled on this GPU node pool.This is because GPUs are expensive, so we want to use this node pool for Ray GPU workers only.Concretely, any Pod that does not have the following toleration will not be scheduled on this GPU node pool:\n\n```yaml\ntolerations:\n- key: ray.io/node-type\n  operator: Equal\n  value: worker\n  effect: NoSchedule\n```\n\nFor more on taints and tolerations, see the [Kubernetes documentation](https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/).## Step 3: Configure `kubectl` to connect to the cluster\n\nRun the following command to download Google Cloud credentials and configure the Kubernetes CLI to use them.```sh\ngcloud container clusters get-credentials kuberay-gpu-cluster --zone us-west1-b\n```\n\nFor more details, see the [GKE documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl).## Step 4: Install NVIDIA GPU device drivers\n\nThis step is required for GPU support on GKE.See the [GKE documentation](https://cloud.google.com/kubernetes-engine/docs/how-to/gpus#installing_drivers) for more details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "34ce08a4-fe86-4552-b947-923d00090a59": {"__data__": {"id_": "34ce08a4-fe86-4552-b947-923d00090a59", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md", "text_hash": "0aedd41111a04bc4f26d595e13329986"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2f9ad0b8129b0df68f9d674ea508af7f576f0d2c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md"}, "hash": "66b190983627d1333ae862c77e4b506317ffa225bff9706ee3e8cc6c4d93c99e"}, "2": {"node_id": "8058a0e1-2d1c-4567-9785-98bd1f1d1a19", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md", "text_hash": "a0928a34fc4afac9d52e9583ff329eeb"}, "hash": "5b3441189a852a95d95820ddeb86493426285122778c2358e5e20f25fd9f424a"}}, "hash": "03cc36360206da8da1ac5eaf253cebdb3565214ca98f04f2259f25d9509e9a81", "text": "```sh\n# Install NVIDIA GPU device driver\nkubectl apply -f https://raw.githubusercontent.com/GoogleCloudPlatform/container-engine-accelerators/master/nvidia-driver-installer/cos/daemonset-preloaded-latest.yaml\n\n# Verify that your nodes have allocatable GPUs \nkubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.nvidia\\.com/gpu\"\n\n# Example output:\n# NAME                                          GPU\n# gke-kuberay-gpu-cluster-gpu-node-pool-xxxxx   1\n# gke-kuberay-gpu-cluster-default-pool-xxxxx    <none>\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ec291bef-99f3-4381-87da-40610cff7594": {"__data__": {"id_": "ec291bef-99f3-4381-87da-40610cff7594", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "b68cb5906ed9298f6486be94c4b43b79"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4db513a141cd34e9111906a454769b31275a7c1e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst"}, "hash": "2b1f5d450159d7e83ba4bbe8f99966f4a1b47656981e4488c7538866053868e7"}, "3": {"node_id": "cffab070-aa42-46ad-bc53-c2c5e9d6c38b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "b13d144b3dfa96061f7ce5e42abc7bcb"}, "hash": "b5576ded469cc49cf8454f9acb43a5f0728da07b7e6cbec6a6cadd638dab4f9c"}}, "hash": "43426a882e5da14d90b783350c6f15e5ae469cf3c6921867f8eda7dfe39d4925", "text": ".. _kuberay-gpu:\n\nUsing GPUs\n==========\nThis document provides tips on GPU usage with Ray on Kubernetes.\n\nTo use GPUs on Kubernetes, you will need to configure both your Kubernetes setup and add additional values to your Ray cluster configuration.\n\nTo learn about GPU usage on different clouds, see instructions for `GKE`_, for `EKS`_, and for `AKS`_.\n\nML training with GPUs on Kubernetes\n___________________________________________\nSee :ref:`GPU training example <kuberay-gpu-training-example>` for a complete example of training a PyTorch model on a GPU with Ray on Kubernetes.\n\nDependencies for GPU-based machine learning\n___________________________________________\nThe `Ray Docker Hub <https://hub.docker.com/r/rayproject/>`_ hosts CUDA-based container images packaged\nwith Ray and certain machine learning libraries.\nFor example, the image ``rayproject/ray-ml:2.0.0-gpu`` is ideal for running GPU-based ML workloads with Ray 2.0.0.\nThe Ray ML images are packaged with dependencies (such as TensorFlow and PyTorch) needed to use the Ray Libraries covered in these docs.\nTo add custom dependencies, we recommend one, or both, of the following methods:\n\n* Building a docker image using one of the official :ref:`Ray docker images <docker-images>` as base.\n* Using :ref:`Ray Runtime environments <runtime-environments>`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "cffab070-aa42-46ad-bc53-c2c5e9d6c38b": {"__data__": {"id_": "cffab070-aa42-46ad-bc53-c2c5e9d6c38b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "b13d144b3dfa96061f7ce5e42abc7bcb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4db513a141cd34e9111906a454769b31275a7c1e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst"}, "hash": "2b1f5d450159d7e83ba4bbe8f99966f4a1b47656981e4488c7538866053868e7"}, "2": {"node_id": "ec291bef-99f3-4381-87da-40610cff7594", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "b68cb5906ed9298f6486be94c4b43b79"}, "hash": "43426a882e5da14d90b783350c6f15e5ae469cf3c6921867f8eda7dfe39d4925"}, "3": {"node_id": "4a2a1933-008d-40e1-a278-96001d5fb45d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "a2c09c6954df42d889e5ceb855ed1413"}, "hash": "46a6bdabc6d7e03f3740a93419fb95aef1bb0c9400adaa77c47b8e62012fbacd"}}, "hash": "b5576ded469cc49cf8454f9acb43a5f0728da07b7e6cbec6a6cadd638dab4f9c", "text": "Configuring Ray pods for GPU usage\n__________________________________\n\nUsing Nvidia GPUs requires specifying `nvidia.com/gpu` resource `limits` in the container fields of your `RayCluster`'s\n`headGroupSpec` and/or `workerGroupSpecs`.\n(Kubernetes `automatically sets <https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/#using-device-plugins>`_\nthe GPU request equal to the limit. However, you might want to specify requests for purposes of documentation.)\n\nHere is a config snippet for a RayCluster workerGroup of up\nto 5 GPU workers.\n\n.. code-block:: yaml\n\n   groupName: gpu-group\n   replicas: 0\n   minReplicas: 0\n   maxReplicas: 5\n   ...\n   template:\n       spec:\n        ...\n        containers:\n         - name: ray-node\n           image: rayproject/ray-ml:2.0.0-gpu\n           ...\n           resources:\n            nvidia.com/gpu: 1 # Optional, included just for documentation.\n            cpu: 3\n            memory: 50Gi\n           limits:\n            nvidia.com/gpu: 1 # Required to use GPU.\n            cpu: 3\n            memory: 50Gi\n            ...\n\nEach of the Ray pods in the group can be scheduled on an AWS `p2.xlarge` instance (1 GPU, 4vCPU, 61Gi RAM).\n\n.. tip::\n\n    GPU instances are expensive -- consider setting up autoscaling for your GPU Ray workers,\n    as demonstrated with the `minReplicas:0` and `maxReplicas:5` settings above.\n    To enable autoscaling, remember also to set `enableInTreeAutoscaling:True` in your RayCluster's `spec`\n    Finally, make sure your group or pool of GPU Kubernetes nodes are configured to autoscale.\n    Refer to your :ref:`cloud provider's documentation <kuberay-k8s-setup>` for details on autoscaling node pools.\n\nGPUs and Ray\n____________\n\nThis section discuss GPU usage for Ray applications running on Kubernetes.\nFor general guidance on GPU usage with Ray, see also :ref:`gpu-support`.\n\nThe KubeRay operator advertises container GPU resource limits to\nthe Ray scheduler and the Ray autoscaler. In particular, the Ray container's\n`ray start` entrypoint will be automatically configured with the appropriate `--num-gpus` option.\n\nGPU workload scheduling\n~~~~~~~~~~~~~~~~~~~~~~~\nAfter a Ray pod with access to GPU is deployed, it will\nbe able to execute tasks and actors annotated with gpu requests.\nFor example, the decorator `@ray.remote(num_gpus=1)` annotates a task or actor\nrequiring 1 GPU.\n\n\nGPU autoscaling\n~~~~~~~~~~~~~~~\nThe Ray autoscaler is aware of each Ray worker group's GPU capacity.Say we have a RayCluster configured as in the config snippet above:\n\n- There is a worker group of Ray pods with 1 unit of GPU capacity each.- The Ray cluster does not currently have any workers from that group.- `maxReplicas` for the group is at least 2.Then the following Ray program will trigger upscaling of 2 GPU workers... code-block:: python\n\n    import ray\n\n    ray.init()\n\n    @ray.remote(num_gpus=1)\n    class GPUActor:\n        def say_hello(self):\n            print(\"I live in a pod with GPU access.\")# Request actor placement.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4a2a1933-008d-40e1-a278-96001d5fb45d": {"__data__": {"id_": "4a2a1933-008d-40e1-a278-96001d5fb45d", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "a2c09c6954df42d889e5ceb855ed1413"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4db513a141cd34e9111906a454769b31275a7c1e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst"}, "hash": "2b1f5d450159d7e83ba4bbe8f99966f4a1b47656981e4488c7538866053868e7"}, "2": {"node_id": "cffab070-aa42-46ad-bc53-c2c5e9d6c38b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "b13d144b3dfa96061f7ce5e42abc7bcb"}, "hash": "b5576ded469cc49cf8454f9acb43a5f0728da07b7e6cbec6a6cadd638dab4f9c"}, "3": {"node_id": "1db9ccfe-6d92-4837-9da6-bfbe84914a7a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "c79e23fc61dc062ee77b9706ddc45bbf"}, "hash": "52f338db19b29c8ca73610cd76420f1884cd48bdffe61b275e59ce37fa2cc69f"}}, "hash": "46a6bdabc6d7e03f3740a93419fb95aef1bb0c9400adaa77c47b8e62012fbacd", "text": "gpu_actors = [GPUActor.remote() for _ in range(2)]\n    # The following command will block until two Ray pods with GPU access are scaled\n    # up and the actors are placed.ray.get([actor.say_hello.remote() for actor in gpu_actors])\n\nAfter the program exits, the actors will be garbage collected.The GPU worker pods will be scaled down after the idle timeout (60 seconds by default).If the GPU worker pods were running on an autoscaling pool of Kubernetes nodes, the Kubernetes\nnodes will be scaled down as well.Requesting GPUs\n~~~~~~~~~~~~~~~\nYou can also make a :ref:`direct request to the autoscaler <ref-autoscaler-sdk-request-resources>` to scale up GPU resources... code-block:: python\n\n    import ray\n\n    ray.init()\n    ray.autoscaler.sdk.request_resources(bundles=[{\"GPU\": 1}] * 2)\n\nAfter the nodes are scaled up, they will persist until the request is explicitly overridden.The following program will remove the resource request... code-block:: python\n\n    import ray\n\n    ray.init()\n    ray.autoscaler.sdk.request_resources(bundles=[])\n\nThe GPU workers can then scale down... _kuberay-gpu-override:\n\nOverriding Ray GPU capacity (advanced)\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nFor specialized use-cases, it is possible to override the Ray pod GPU capacities advertised to Ray.To do so, set a value for the `num-gpus` key of the head or worker group's `rayStartParams`.For example,\n\n.. code-block:: yaml\n\n    rayStartParams:\n        # Note that all rayStartParam values must be supplied as strings.num-gpus: \"2\"\n\nThe Ray scheduler and autoscaler will then account 2 units of GPU capacity for each\nRay pod in the group, even if the container limits do not indicate the presence of GPU.GPU pod scheduling (advanced)\n_____________________________\n\nGPU taints and tolerations\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n.. note::\n\n  Managed Kubernetes services typically take care of GPU-related taints and tolerations\n  for you.If you are using a managed Kubernetes service, you might not need to worry\n  about this section.The `Nvidia gpu plugin`_ for Kubernetes applies `taints`_ to GPU nodes; these taints prevent non-GPU pods from being scheduled on GPU nodes.Managed Kubernetes services like GKE, EKS, and AKS automatically apply matching `tolerations`_\nto pods requesting GPU resources.Tolerations are applied by means of Kubernetes's `ExtendedResourceToleration`_ `admission controller`_.If this admission controller is not enabled for your Kubernetes cluster, you may need to manually add a GPU toleration to each of your GPU pod configurations.For example,\n\n.. code-block:: yaml\n\n  apiVersion: v1\n  kind: Pod\n  metadata:\n   generateName: example-cluster-ray-worker\n   spec:\n   ...\n   tolerations:\n   - effect: NoSchedule\n     key: nvidia.com/gpu\n     operator: Exists\n   ...\n   containers:\n   - name: ray-node\n     image: rayproject/ray:nightly-gpu\n     ...\n\nNode selectors and node labels\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nTo ensure Ray pods are bound to Kubernetes nodes satisfying specific\nconditions (such as the presence of GPU hardware), you may wish to use\nthe `nodeSelector` field of your `workerGroup`'s pod template `spec`.See the `Kubernetes docs`_ for more about Pod-to-Node assignment.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1db9ccfe-6d92-4837-9da6-bfbe84914a7a": {"__data__": {"id_": "1db9ccfe-6d92-4837-9da6-bfbe84914a7a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "c79e23fc61dc062ee77b9706ddc45bbf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "4db513a141cd34e9111906a454769b31275a7c1e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst"}, "hash": "2b1f5d450159d7e83ba4bbe8f99966f4a1b47656981e4488c7538866053868e7"}, "2": {"node_id": "4a2a1933-008d-40e1-a278-96001d5fb45d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "a2c09c6954df42d889e5ceb855ed1413"}, "hash": "46a6bdabc6d7e03f3740a93419fb95aef1bb0c9400adaa77c47b8e62012fbacd"}}, "hash": "52f338db19b29c8ca73610cd76420f1884cd48bdffe61b275e59ce37fa2cc69f", "text": "Further reference and discussion\n--------------------------------\nRead about Kubernetes device plugins `here <https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/>`__,\nabout Kubernetes GPU plugins `here <https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus>`__,\nand about Nvidia's GPU plugin for Kubernetes `here <https://github.com/NVIDIA/k8s-device-plugin>`__.\n\n.. _`GKE`: https://cloud.google.com/kubernetes-engine/docs/how-to/gpus\n.. _`EKS`: https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html\n.. _`AKS`: https://docs.microsoft.com/en-us/azure/aks/gpu-cluster\n\n.. _`tolerations`: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n.. _`taints`: https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/\n.. _`Nvidia gpu plugin`: https://github.com/NVIDIA/k8s-device-plugin\n.. _`admission controller`: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\n.. _`ExtendedResourceToleration`: https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#extendedresourcetoleration\n.. _`Kubernetes docs`: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/\n.. _`bug`: https://github.com/ray-project/kuberay/pull/497/", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "07898156-be8e-40c3-8969-1382387f0ceb": {"__data__": {"id_": "07898156-be8e-40c3-8969-1382387f0ceb", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/k8s-cluster-setup.md", "file_name": "k8s-cluster-setup.md", "text_hash": "694143df57f72cd60b1211cc1df35490"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "caa42e633b4f103a254c3905e06d00c50dfbacc8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/k8s-cluster-setup.md", "file_name": "k8s-cluster-setup.md"}, "hash": "d05b352935fef73415dd5348d220be3205a5338cae4e99db924909045245eeba"}}, "hash": "2f2666204860f4516d71f67365512c400e1793651bb62c3408d01c07ab47aa42", "text": "(kuberay-k8s-setup)=\n\n# Managed Kubernetes services\n\nThe KubeRay operator and Ray can run on any cloud or on-prem Kubernetes cluster.\nThe simplest way to provision a remote Kubernetes cluster is to use a cloud-based managed service.\nWe collect a few helpful links for users who are getting started with a managed Kubernetes service.\n\n(gke-setup)=\n# Setting up a GKE cluster (Google Cloud)\n\n- {ref}`kuberay-gke-gpu-cluster-setup`\n\n(eks-setup)=\n# Setting up an EKS cluster (AWS)\n\n- {ref}`kuberay-eks-gpu-cluster-setup`\n\n(aks-setup)=\n# Setting up an AKS (Microsoft Azure)\nYou can find the landing page for AKS [here](https://azure.microsoft.com/en-us/services/kubernetes-service/).\nIf you have an account set up, you can immediately start experimenting with Kubernetes clusters in the provider's console.\nAlternatively, check out the [documentation](https://docs.microsoft.com/en-us/azure/aks/) and\n[quickstart guides](https://docs.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-portal?tabs=azure-cli). To successfully deploy Ray on Kubernetes,\nyou will need to configure pools of Kubernetes nodes;\nfind guidance [here](https://docs.microsoft.com/en-us/azure/aks/use-multiple-node-pools).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f50630dc-0b4d-4243-8093-e8683ada589f": {"__data__": {"id_": "f50630dc-0b4d-4243-8093-e8683ada589f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "fec3cbc7e1ca800dccc7678ce9fe992b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3b95688928bf73014d93e625eeccb28f2317c17", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md"}, "hash": "aca28fa5ba2da9b9010785a588fb3a9097aa735e4bf4f92523f14168019edd09"}, "3": {"node_id": "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "358dcfbdd4dfed387a9a1694ea2296b3"}, "hash": "5c3c549588cf71cf04721bef02d5b8167c492513d54408c9f53569f4ff7f628d"}}, "hash": "760debccfd5eac2f42b886655f80130d69244c4ea1ae721eb41ad8d8b46f4eb2", "text": "(kuberay-logging)=\n\n# Log Persistence\n\nLogs (both system and application logs) are useful for troubleshooting Ray applications and Clusters. For example, you may want to access system logs if a node terminates unexpectedly.\n\nSimilar to Kubernetes, Ray does not provide a native storage solution for log data. Users need to manage the lifecycle of the logs by themselves. This page provides instructions on how to collect logs from Ray Clusters that are running on Kubernetes.\n\n:::{tip}\nSkip to {ref}`the deployment instructions <kuberay-logging-tldr>`\nfor a sample configuration showing how to extract logs from a Ray pod.\n:::\n\n## Ray log directory\nBy default, Ray writes logs to files in the directory `/tmp/ray/session_*/logs` on each Ray pod's file system, including application and system logs. Learn more about the {ref}`log directory and log files <logging-directory>` and the {ref}`log rotation configuration <log-rotation>` before you start to collect the logs.\n\n## Log processing tools\nThere are a number of open source log processing tools available within the Kubernetes ecosystem. This page shows how to extract Ray logs using [Fluent Bit][FluentBit].\nOther popular tools include [Vector][Vector], [Fluentd][Fluentd], [Filebeat][Filebeat], and [Promtail][Promtail].\n\n## Log collection strategies\nCollect logs written to a pod's filesystem using one of two logging strategies:\n**sidecar containers** or **daemonsets**. Read more about these logging\npatterns in the [Kubernetes documentation][KubDoc].\n\n### Sidecar containers\nWe provide an {ref}`example <kuberay-fluentbit>` of the sidecar strategy in this guide.\nYou can process logs by configuring a log-processing sidecar\nfor each Ray pod. Ray containers should be configured to share the `/tmp/ray`\ndirectory with the logging sidecar via a volume mount.\n\nYou can configure the sidecar to do either of the following:\n* Stream Ray logs to the sidecar's stdout.\n* Export logs to an external service.\n\n### Daemonset\nAlternatively, it is possible to collect logs at the Kubernetes node level.\nTo do this, one deploys a log-processing daemonset onto the Kubernetes cluster's\nnodes. With this strategy, it is key to mount\nthe Ray container's `/tmp/ray` directory to the relevant `hostPath`.\n\n(kuberay-fluentbit)=\n## Setting up logging sidecars with Fluent Bit\nIn this section, we give an example of how to set up log-emitting\n[Fluent Bit][FluentBit] sidecars for Ray pods.\n\nSee the full config for a single-pod RayCluster with a logging sidecar [here][ConfigLink].\nWe now discuss this configuration and show how to deploy it.\n\n### Configuring log processing\nThe first step is to create a ConfigMap with configuration\nfor Fluent Bit.\n\nHere is a minimal ConfigMap which tells a Fluent Bit sidecar to\n* Tail Ray logs.\n* Output the logs to the container's stdout.\n```{literalinclude} ../configs/ray-cluster.log.yaml\n:language: yaml\n:start-after: Fluent Bit ConfigMap\n:end-before: ---\n```\nA few notes on the above config:\n- In addition to streaming logs to stdout, you can use an [OUTPUT] clause to export logs to any\n  [storage backend][FluentBitStorage] supported by Fluent Bit.\n- The `Path_Key true` line above ensures that file names are included in the log records\n  emitted by Fluent Bit.\n- The `Refresh_Interval 5` line asks Fluent Bit to refresh the list of files\n  in the log directory once per 5 seconds, rather than the default 60.\n  The reason is that the directory `/tmp/ray/session_latest/logs/` does not exist\n  initially (Ray must create it first). Setting the `Refresh_Interval` low allows us to see logs\n  in the Fluent Bit container's stdout sooner.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e": {"__data__": {"id_": "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "358dcfbdd4dfed387a9a1694ea2296b3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3b95688928bf73014d93e625eeccb28f2317c17", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md"}, "hash": "aca28fa5ba2da9b9010785a588fb3a9097aa735e4bf4f92523f14168019edd09"}, "2": {"node_id": "f50630dc-0b4d-4243-8093-e8683ada589f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "fec3cbc7e1ca800dccc7678ce9fe992b"}, "hash": "760debccfd5eac2f42b886655f80130d69244c4ea1ae721eb41ad8d8b46f4eb2"}, "3": {"node_id": "dc12baf2-8408-461a-84bd-50f978883e89", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "0e38fd8c4a69e77e5ee4c0a56856c855"}, "hash": "dd6850145c0d1644795071565e5a33df582789e1af59d4794ade4da564c0e336"}}, "hash": "5c3c549588cf71cf04721bef02d5b8167c492513d54408c9f53569f4ff7f628d", "text": "### Adding logging sidecars to RayCluster Custom Resource (CR)\n\n#### Adding log and config volumes\nFor each pod template in our RayCluster CR, we\nneed to add two volumes: One volume for Ray's logs\nand another volume to store Fluent Bit configuration from the ConfigMap\napplied above.\n```{literalinclude} ../configs/ray-cluster.log.yaml\n:language: yaml\n:start-after: Log and config volumes\n```\n\n#### Mounting the Ray log directory\nAdd the following volume mount to the Ray container's configuration.\n```{literalinclude} ../configs/ray-cluster.log.yaml\n:language: yaml\n:start-after: Share logs with Fluent Bit\n:end-before: Fluent Bit sidecar\n```\n\n#### Adding the Fluent Bit sidecar\nFinally, add the Fluent Bit sidecar container to each Ray pod config\nin your RayCluster CR.\n```{literalinclude} ../configs/ray-cluster.log.yaml\n:language: yaml\n:start-after: Fluent Bit sidecar\n:end-before: Log and config volumes\n```\nMounting the `ray-logs` volume gives the sidecar container access to Ray's logs.\nThe <nobr>`fluentbit-config`</nobr> volume gives the sidecar access to logging configuration.\n\n#### Putting everything together\nPutting all of the above elements together, we have the following yaml configuration\nfor a single-pod RayCluster will a log-processing sidecar.\n```{literalinclude} ../configs/ray-cluster.log.yaml\n:language: yaml\n```\n\n(kuberay-logging-tldr)=\n### Deploying a RayCluster with logging sidecar\n\n\nTo deploy the configuration described above, deploy the KubeRay Operator if you haven't yet:\nRefer to the {ref}`Getting Started guide <kuberay-operator-deploy>`\nfor instructions on this step.\n\nNow, run the following commands to deploy the Fluent Bit ConfigMap and a single-pod RayCluster with\na Fluent Bit sidecar.\n```shell\nkubectl apply -f https://raw.githubusercontent.com/ray-project/ray/releases/2.4.0/doc/source/cluster/kubernetes/configs/ray-cluster.log.yaml\n```\n\nDetermine the Ray pod's name with\n```shell\nkubectl get pod | grep raycluster-complete-logs\n```\n\nExamine the FluentBit sidecar's STDOUT to see logs for Ray's component processes.\n```shell\n# Substitute the name of your Ray pod.\nkubectl logs raycluster-complete-logs-head-xxxxx -c fluentbit\n```\n\n[Vector]: https://vector.dev/\n[FluentBit]: https://docs.fluentbit.io/manual\n[FluentBitStorage]: https://docs.fluentbit.io/manual\n[Filebeat]: https://www.elastic.co/guide/en/beats/filebeat/7.17/index.html\n[Fluentd]: https://docs.fluentd.org/\n[Promtail]: https://grafana.com/docs/loki/latest/clients/promtail/\n[KubDoc]: https://kubernetes.io/docs/concepts/cluster-administration/logging/\n[ConfigLink]: https://raw.githubusercontent.com/ray-project/ray/releases/2.4.0/doc/source/cluster/kubernetes/configs/ray-cluster.log.yaml", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dc12baf2-8408-461a-84bd-50f978883e89": {"__data__": {"id_": "dc12baf2-8408-461a-84bd-50f978883e89", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "0e38fd8c4a69e77e5ee4c0a56856c855"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a3b95688928bf73014d93e625eeccb28f2317c17", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md"}, "hash": "aca28fa5ba2da9b9010785a588fb3a9097aa735e4bf4f92523f14168019edd09"}, "2": {"node_id": "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "358dcfbdd4dfed387a9a1694ea2296b3"}, "hash": "5c3c549588cf71cf04721bef02d5b8167c492513d54408c9f53569f4ff7f628d"}}, "hash": "dd6850145c0d1644795071565e5a33df582789e1af59d4794ade4da564c0e336", "text": "(redirect-to-stderr)=\n## Redirecting Ray logs to stderr\n\nBy default, Ray writes logs to files under the ``/tmp/ray/session_*/logs`` directory. If you prefer to redirect logs to stderr of the host pods instead, set the environment variable ``RAY_LOG_TO_STDERR=1`` on all Ray nodes. This practice is not recommended but may be useful if your log processing tool only captures log records written to stderr.\n\n```{admonition} Alert\n:class: caution\nThere are known issues with this feature. For example, it may break features like {ref}`Worker log redirection to Driver <log-redirection-to-driver>`. If those features are wanted, use the {ref}`Fluent Bit solution <kuberay-fluentbit>` above.\n\nFor Clusters on VMs, do not redirect logs to stderr. Follow {ref}`this guide <vm-logging>` to persist logs.\n```\n\nRedirecting logging to stderr also prepends a ``({component})`` prefix, for example ``(raylet)``, to each log record messages.\n\n```bash\n[2022-01-24 19:42:02,978 I 1829336 1829336] (gcs_server) grpc_server.cc:103: GcsServer server started, listening on port 50009.\n[2022-01-24 19:42:06,696 I 1829415 1829415] (raylet) grpc_server.cc:103: ObjectManager server started, listening on port 40545.\n2022-01-24 19:42:05,087 INFO (dashboard) dashboard.py:95 -- Setup static dir for dashboard: /mnt/data/workspace/ray/python/ray/dashboard/client/build\n2022-01-24 19:42:07,500 INFO (dashboard_agent) agent.py:105 -- Dashboard agent grpc address: 0.0.0.0:49228\n```\n\nThese prefixes allow you to filter the stderr stream of logs down to the component of interest. Note that multi-line log records do **not** have this component marker at the beginning of each line.\n\nFollow the steps below to set the environment variable ``RAY_LOG_TO_STDERR=1`` on all Ray nodes\n\n  ::::{tab-set}\n\n  :::{tab-item} Single-node local cluster\n  **Start the cluster explicitly with CLI** <br/>\n  ```bash\n  env RAY_LOG_TO_STDERR=1 ray start\n  ```\n\n  **Start the cluster implicitly with `ray.init`** <br/>\n  ```python\n  os.environ[\"RAY_LOG_TO_STDERR\"] = \"1\"\n  ray.init()\n  ```\n  :::\n\n  :::{tab-item} KubeRay\n  Set `RAY_LOG_TO_STDERR` to `1` in `spec.headGroupSpec.template.spec.containers.env` and `spec.workerGroupSpec.template.spec.containers.env`. Check out this [example YAML file](https://gist.github.com/scottsun94/da4afda045d6e1cc32f9ccd6c33281c2)\n\n  :::\n\n\n  ::::", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "59d0f25c-204d-45b8-b813-554978fd7092": {"__data__": {"id_": "59d0f25c-204d-45b8-b813-554978fd7092", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "6627a011a260a3796f38f5ab0fcddcd4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ee0de0c75182ef1a301db805eb4d3d281a174db", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md"}, "hash": "6e4cfd50f63c5dd502ffc73c941a6fb9904309cdb8c6b199e3fd875fb6a19848"}, "3": {"node_id": "e5998a6f-2648-4186-bf8a-511347e50d51", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "1b4a30fa2e3e36ad90edb3364b069e2d"}, "hash": "bd2280eb815e5025610f5b999fa52fd2097a7a9b1283cf52bffe5fffae429c60"}}, "hash": "fc7638f19d1bb18b1171fb461a41fd89f2208770f7bd6986b7aa9245427f43b3", "text": "(kuberay-pod-command)=\n\n# Specify container commands for Ray head/worker Pods\nYou can execute commands on the head/worker pods at two timings:\n\n* (1) **Before `ray start`**: As an example, you can set up some environment variables that will be used by `ray start`.* (2) **After `ray start` (RayCluster is ready)**: As an example, you can launch a Ray serve deployment when the RayCluster is ready.## Current KubeRay operator behavior for container commands\n* The current behavior for container commands is not finalized, and **may be updated in the future**.* See [code](https://github.com/ray-project/kuberay/blob/47148921c7d14813aea26a7974abda7cf22bbc52/ray-operator/controllers/ray/common/pod.go#L301-L326) for more details.## Timing 1: Before `ray start`\nCurrently, for timing (1), we can set the container's `Command` and `Args` in RayCluster specification to reach the goal.```yaml\n# https://github.com/ray-project/kuberay/ray-operator/config/samples/ray-cluster.head-command.yaml\n    rayStartParams:\n        ...\n    #pod template\n    template:\n      spec:\n        containers:\n        - name: ray-head\n          image: rayproject/ray:2.5.0\n          resources:\n            ...\n          ports:\n            ...\n          # `command` and `args` will become a part of `spec.containers.0.args` in the head Pod.command: [\"echo 123\"]\n          args: [\"456\"]\n```\n\n* Ray head Pod\n    * `spec.containers.0.command` is hardcoded with `[\"/bin/bash\", \"-lc\", \"--\"]`.* `spec.containers.0.args` contains two parts:\n        * (Part 1) **user-specified command**: A string concatenates `headGroupSpec.template.spec.containers.0.command` from RayCluster and `headGroupSpec.template.spec.containers.0.args` from RayCluster together.* (Part 2) **ray start command**: The command is created based on `rayStartParams` specified in RayCluster.The command will look like `ulimit -n 65536; ray start ...`.* To summarize, `spec.containers.0.args` will be `$(user-specified command) && $(ray start command)`.* Example\n    ```sh\n    # Prerequisite: There is a KubeRay operator in the Kubernetes cluster.# Download `ray-cluster.head-command.yaml`\n    curl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.head-command.yaml\n\n    # Create a RayCluster\n    kubectl apply -f ray-cluster.head-command.yaml\n\n    # Check ${RAYCLUSTER_HEAD_POD}\n    kubectl get pod -l ray.io/node-type=head\n\n    # Check `spec.containers.0.command` and `spec.containers.0.args`.kubectl describe pod ${RAYCLUSTER_HEAD_POD}\n\n    # Command:\n    #   /bin/bash\n    #   -lc\n    #   --\n    # Args:\n    #    echo 123  456  && ulimit -n 65536; ray start --head  --dashboard-host=0.0.0.0  --num-cpus=1  --block  --metrics-export-port=8080  --memory=2147483648\n    ```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e5998a6f-2648-4186-bf8a-511347e50d51": {"__data__": {"id_": "e5998a6f-2648-4186-bf8a-511347e50d51", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "1b4a30fa2e3e36ad90edb3364b069e2d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ee0de0c75182ef1a301db805eb4d3d281a174db", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md"}, "hash": "6e4cfd50f63c5dd502ffc73c941a6fb9904309cdb8c6b199e3fd875fb6a19848"}, "2": {"node_id": "59d0f25c-204d-45b8-b813-554978fd7092", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "6627a011a260a3796f38f5ab0fcddcd4"}, "hash": "fc7638f19d1bb18b1171fb461a41fd89f2208770f7bd6986b7aa9245427f43b3"}, "3": {"node_id": "050bd1d8-66f8-4a05-8b48-0ff0e26b74b0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "181bd5528e01f6b972ac079acc29e10d"}, "hash": "1e369cf4fba0387facd07471984835b226eb7c4acaba77009afa699babfbfd58"}}, "hash": "bd2280eb815e5025610f5b999fa52fd2097a7a9b1283cf52bffe5fffae429c60", "text": "## Timing 2: After `ray start` (RayCluster is ready)\nWe have two solutions to execute commands after the RayCluster is ready.The main difference between these two solutions is users can check the logs via `kubectl logs` with Solution 1.### Solution 1: Container command (Recommended)\nAs we mentioned in the section \"Timing 1: Before `ray start`\", user-specified command will be executed before the `ray start` command.Hence, we can execute the `ray_cluster_resources.sh` in background by updating `headGroupSpec.template.spec.containers.0.command` in `ray-cluster.head-command.yaml`.```yaml\n# https://github.com/ray-project/kuberay/ray-operator/config/samples/ray-cluster.head-command.yaml\n# Parentheses for the command is required.command: [\"(/home/ray/samples/ray_cluster_resources.sh&)\"]\n\n# ray_cluster_resources.sh\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: ray-example\ndata:\n  ray_cluster_resources.sh: |\n    #!/bin/bash\n\n    # wait for ray cluster to finish initialization\n    while true; do\n        ray health-check 2>/dev/null\n        if [ \"$?\"= \"0\" ]; then\n            break\n        else\n            echo \"INFO: waiting for ray head to start\"\n            sleep 1\n        fi\n    done\n\n    # Print the resources in the ray cluster after the cluster is ready.python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n\n    echo \"INFO: Print Ray cluster resources\"\n```\n\n* Example\n    ```sh\n    # (1) Update `command` to [\"(/home/ray/samples/ray_cluster_resources.sh&)\"]\n    # (2) Comment out `postStart` and `args`.kubectl apply -f ray-cluster.head-command.yaml\n\n    # Check ${RAYCLUSTER_HEAD_POD}\n    kubectl get pod -l ray.io/node-type=head\n\n    # Check the logs\n    kubectl logs ${RAYCLUSTER_HEAD_POD}\n\n    # INFO: waiting for ray head to start\n    # .# .=> Cluster initialization\n    # .# 2023-02-16 18:44:43,724 INFO worker.py:1231 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS\n    # 2023-02-16 18:44:43,724 INFO worker.py:1352 -- Connecting to existing Ray cluster at address: 10.244.0.26:6379...\n    # 2023-02-16 18:44:43,735 INFO worker.py:1535 -- Connected to Ray cluster.View the dashboard at http://10.244.0.26:8265\n    # {'object_store_memory': 539679129.0, 'node:10.244.0.26': 1.0, 'CPU': 1.0, 'memory': 2147483648.0}\n    # INFO: Print Ray cluster resources\n    ```\n\n### Solution 2: postStart hook\n```yaml\n# https://github.com/ray-project/kuberay/ray-operator/config/samples/ray-cluster.head-command.yaml\nlifecycle:\n  postStart:\n    exec:\n      command: [\"/bin/sh\",\"-c\",\"/home/ray/samples/ray_cluster_resources.sh\"]\n```\n\n* We execute the script `ray_cluster_resources.sh` via the postStart hook.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "050bd1d8-66f8-4a05-8b48-0ff0e26b74b0": {"__data__": {"id_": "050bd1d8-66f8-4a05-8b48-0ff0e26b74b0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "181bd5528e01f6b972ac079acc29e10d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7ee0de0c75182ef1a301db805eb4d3d281a174db", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md"}, "hash": "6e4cfd50f63c5dd502ffc73c941a6fb9904309cdb8c6b199e3fd875fb6a19848"}, "2": {"node_id": "e5998a6f-2648-4186-bf8a-511347e50d51", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "1b4a30fa2e3e36ad90edb3364b069e2d"}, "hash": "bd2280eb815e5025610f5b999fa52fd2097a7a9b1283cf52bffe5fffae429c60"}}, "hash": "1e369cf4fba0387facd07471984835b226eb7c4acaba77009afa699babfbfd58", "text": "Based on [this document](https://kubernetes.io/docs/concepts/containers/container-lifecycle-hooks/#container-hooks), there is no guarantee that the hook will execute before the container ENTRYPOINT.Hence, we need to wait for RayCluster to finish initialization in `ray_cluster_resources.sh`.* Example\n    ```sh\n    kubectl apply -f ray-cluster.head-command.yaml\n\n    # Check ${RAYCLUSTER_HEAD_POD}\n    kubectl get pod -l ray.io/node-type=head\n\n    # Forward the port of Dashboard\n    kubectl port-forward --address 0.0.0.0 ${RAYCLUSTER_HEAD_POD} 8265:8265\n\n    # Open the browser and check the Dashboard (${YOUR_IP}:8265/#/job).# You shold see a SUCCEEDED job with the following Entrypoint:\n    #\n    # `python -c \"import ray; ray.init(); print(ray.cluster_resources())\"`\n    ```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "09901628-a61d-4054-aaa7-040ec89f096d": {"__data__": {"id_": "09901628-a61d-4054-aaa7-040ec89f096d", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md", "text_hash": "7e6facc87956f8a3c7f73c88158de97e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "795bde4c244d3a4acb0819ac10094d6da841753b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md"}, "hash": "047ae41a616a14d398416e39634dc5e857952ca1786f408717a6dd038f8ed28f"}, "3": {"node_id": "c6b92eec-8d55-4f48-9022-192ffa2ef6e9", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md", "text_hash": "aa70f35879761a8eeac77faf4be04010"}, "hash": "ab3b8ece2532d4060b1c86cede1d025168962f743f49c271a43296dede0d1578"}}, "hash": "b03d725b92c795488b68c68f72b5c842974f0623382c95332140218626a26cbd", "text": "(kuberay-pod-security)=\n\n# Pod Security\n\nKubernetes defines three different Pod Security Standards, including `privileged`, `baseline`, and `restricted`, to broadly\ncover the security spectrum.The `privileged` standard allows users to do known privilege escalations, and thus it is not \nsafe enough for security-critical applications.This document describes how to configure RayCluster YAML file to apply `restricted` Pod security standard.The following \nreferences can help you understand this document better:\n\n* [Kubernetes - Pod Security Standards](https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted)\n* [Kubernetes - Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/)\n* [Kubernetes - Auditing](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/)\n* [Kind - Auditing](https://kind.sigs.k8s.io/docs/user/auditing/)\n\n# Preparation\n\nPlease clone the [KubeRay repository](https://github.com/ray-project/kuberay) and checkout the `master` branch.This tutorial requires several files in the repository.# Step 1: Create a Kind cluster\n\n```bash\n# Path: kuberay/\nkind create cluster --config ray-operator/config/security/kind-config.yaml --image=kindest/node:v1.24.0\n```\n\nThe `kind-config.yaml` enables audit logging with the audit policy defined in `audit-policy.yaml`.The `audit-policy.yaml`\ndefines an auditing policy to listen to the Pod events in the namespace `pod-security`.With this policy, we can check\nwhether our Pods violate the policies in `restricted` standard or not.The feature [Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/) is firstly \nintroduced in Kubernetes v1.22 (alpha) and becomes stable in Kubernetes v1.25.In addition, KubeRay currently supports \nKubernetes from v1.19 to v1.24.(At the time of writing, we have not tested KubeRay with Kubernetes v1.25).Hence, I use **Kubernetes v1.24** in this step.# Step 2: Check the audit logs\n\n```bash\ndocker exec kind-control-plane cat /var/log/kubernetes/kube-apiserver-audit.log\n```\nThe log should be empty because the namespace `pod-security` does not exist.# Step 3: Create the `pod-security` namespace\n\n```bash\nkubectl create ns pod-security\nkubectl label --overwrite ns pod-security \\\n  pod-security.kubernetes.io/warn=restricted \\\n  pod-security.kubernetes.io/warn-version=latest \\\n  pod-security.kubernetes.io/audit=restricted \\\n  pod-security.kubernetes.io/audit-version=latest \\\n  pod-security.kubernetes.io/enforce=restricted \\\n  pod-security.kubernetes.io/enforce-version=latest\n```\n\nWith the `pod-security.kubernetes.io` labels, the built-in Kubernetes Pod security admission controller will apply the \n`restricted` Pod security standard to all Pods in the namespace `pod-security`.The label\n`pod-security.kubernetes.io/enforce=restricted` means that the Pod will be rejected if it violate the policies defined in \n`restricted` security standard.See [Pod Security Admission](https://kubernetes.io/docs/concepts/security/pod-security-admission/) for more details about the labels.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c6b92eec-8d55-4f48-9022-192ffa2ef6e9": {"__data__": {"id_": "c6b92eec-8d55-4f48-9022-192ffa2ef6e9", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md", "text_hash": "aa70f35879761a8eeac77faf4be04010"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "795bde4c244d3a4acb0819ac10094d6da841753b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md"}, "hash": "047ae41a616a14d398416e39634dc5e857952ca1786f408717a6dd038f8ed28f"}, "2": {"node_id": "09901628-a61d-4054-aaa7-040ec89f096d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md", "text_hash": "7e6facc87956f8a3c7f73c88158de97e"}, "hash": "b03d725b92c795488b68c68f72b5c842974f0623382c95332140218626a26cbd"}}, "hash": "ab3b8ece2532d4060b1c86cede1d025168962f743f49c271a43296dede0d1578", "text": "# Step 4: Install the KubeRay operator\n\n```bash\n# Update the field securityContext in helm-chart/kuberay-operator/values.yaml\nsecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop: [\"ALL\"]\n  runAsNonRoot: true\n  seccompProfile:\n    type: RuntimeDefault\n\n# Path: kuberay/helm-chart/kuberay-operator\nhelm install -n pod-security kuberay-operator .```\n\n# Step 5: Create a RayCluster (Choose either Step 5.1 or Step 5.2)\n\n* If you choose Step 5.1, no Pod will be created in the namespace `pod-security`.* If you choose Step 5.2, Pods can be created successfully.## Step 5.1: Create a RayCluster without proper `securityContext` configurations\n\n```bash\n# Path: kuberay/ray-operator/config/samples\nkubectl apply -n pod-security -f ray-cluster.complete.yaml\n\n# Wait 20 seconds and check audit logs for the error messages.docker exec kind-control-plane cat /var/log/kubernetes/kube-apiserver-audit.log\n\n# Example error messagess\n# \"pods \\\"raycluster-complete-head-fkbf5\\\" is forbidden: violates PodSecurity \\\"restricted:latest\\\": allowPrivilegeEscalation != false (container \\\"ray-head\\\" must set securityContext.allowPrivilegeEscalation=false) ...\n\nkubectl get pod -n pod-security\n# NAME                               READY   STATUS    RESTARTS   AGE\n# kuberay-operator-8b6d55dbb-t8msf   1/1     Running   0          62s\n\n# Clean up the RayCluster\nkubectl delete rayclusters.ray.io -n pod-security raycluster-complete\n# raycluster.ray.io \"raycluster-complete\" deleted\n```\n\nNo Pod will be created in the namespace `pod-security`, and check audit logs for error messages.## Step 5.2: Create a RayCluster with proper `securityContext` configurations\n\n```bash\n# Path: kuberay/ray-operator/config/security\nkubectl apply -n pod-security -f ray-cluster.pod-security.yaml\n\n# Wait for the RayCluster convergence and check audit logs for the messages.docker exec kind-control-plane cat /var/log/kubernetes/kube-apiserver-audit.log\n\n# Forward the dashboard port\nkubectl port-forward --address 0.0.0.0 svc/raycluster-pod-security-head-svc -n pod-security 8265:8265\n\n# Log in to the head Pod\nkubectl exec -it -n pod-security ${YOUR_HEAD_POD} -- bash\n\n# (Head Pod) Run a sample job in the Pod\npython3 samples/xgboost_example.py\n\n# Check the job status in the dashboard on your browser.# http://127.0.0.1:8265/#/job => The job status should be \"SUCCEEDED\".# (Head Pod) Make sure Python dependencies can be installed under `restricted` security standard \npip3 install jsonpatch\necho $?# Check the exit code of `pip3 install jsonpatch`.It should be 0.# Clean up the RayCluster\nkubectl delete -n pod-security -f ray-cluster.pod-security.yaml\n# raycluster.ray.io \"raycluster-pod-security\" deleted\n# configmap \"xgboost-example\" deleted\n```\n\nOne head Pod and one worker Pod will be created as specified in `ray-cluster.pod-security.yaml`.First, we log in to the head Pod, run a XGBoost example script, and check the job\nstatus in the dashboard.Next, we use `pip` to install a Python dependency (i.e.`jsonpatch`), and the exit code of the `pip` command should be 0.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0ce4a921-4009-457c-b8d3-d9e7edfcad99": {"__data__": {"id_": "0ce4a921-4009-457c-b8d3-d9e7edfcad99", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "df1f6557e5d959e1db9aee3b97462cdf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md"}, "hash": "029dd7924aedd8d285eeb42568437cb12eb8c5cef3766a313888f3b5d2051cc2"}, "3": {"node_id": "7008f0bf-8120-4b70-a6ee-a1d92bc941d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "2d632e9a7b30f6540460c405abfd1b88"}, "hash": "99aa779468ba9c7a43258af898b294492120dd21c7d162fde1f8b6d163cbc008"}}, "hash": "34490a414761f4f1b9934667d2ed9552a94a2a367f254c3614ce21678155008a", "text": "(kuberay-dev-serve)=\n\n# Developing Ray Serve Python scripts on a RayCluster\n\nIn this tutorial, you will learn how to effectively debug your Ray Serve scripts against a RayCluster, enabling enhanced observability and faster iteration speed compared to developing the script directly with a RayService.\nMany RayService issues are related to the Ray Serve Python scripts, so it is important to ensure the correctness of the scripts before deploying them to a RayService.\nThis tutorial will show you how to develop a Ray Serve Python script for a MobileNet image classifier on a RayCluster.\nYou can deploy and serve the classifier on your local Kind cluster without requiring a GPU.\nPlease refer to [ray-service.mobilenet.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-service.mobilenet.yaml) and [mobilenet-rayservice.md](https://github.com/ray-project/kuberay/blob/master/docs/guidance/mobilenet-rayservice.md) for more details.\n\n\n# Step 1: Install a KubeRay cluster\n\nFollow [this document](kuberay-operator-deploy) to install the latest stable KubeRay operator via Helm repository.# Step 2: Create a RayCluster CR\n\n```sh\nhelm install raycluster kuberay/ray-cluster --version 0.6.0\n```\n\n# Step 3: Log in to the head Pod\n\n```sh\nexport HEAD_POD=$(kubectl get pods --selector=ray.io/node-type=head -o custom-columns=POD:metadata.name --no-headers)\nkubectl exec -it $HEAD_POD -- bash\n```\n\n# Step 4: Prepare your Ray Serve Python scripts and run the Ray Serve application\n\n```sh\n# Execute the following command in the head Pod\ngit clone https://github.com/ray-project/serve_config_examples.git\ncd serve_config_examples\n\n# Try to launch the Ray Serve application\nserve run mobilenet.mobilenet:app\n# [Error message]\n#     from tensorflow.keras.preprocessing import image\n# ModuleNotFoundError: No module named 'tensorflow'\n```\n\n* `serve run mobilenet.mobilenet:app`: The first `mobilenet` is the name of the directory in the `serve_config_examples/`,\nthe second `mobilenet` is the name of the Python file in the directory `mobilenet/`, and `app` is the name of the variable representing Ray Serve application within the Python file.See the section \"import_path\" in [rayservice-troubleshooting.md](kuberay-raysvc-troubleshoot) for more details.# Step 5: Change the Ray image from `rayproject/ray:${RAY_VERSION}` to `rayproject/ray-ml:${RAY_VERSION}`\n\n```sh\n# Uninstall RayCluster\nhelm uninstall raycluster\n\n# Install the RayCluster CR with the Ray image `rayproject/ray-ml:${RAY_VERSION}`\nhelm install raycluster kuberay/ray-cluster --version 0.6.0 --set image.repository=rayproject/ray-ml\n```\n\nThe error message in Step 4 indicates that the Ray image `rayproject/ray:${RAY_VERSION}` does not have the TensorFlow package.Due to the significant size of TensorFlow, we have opted to use an image with TensorFlow as the base instead of installing it within {ref}`Runtime Environments <runtime-environments>`.In this Step, we will change the Ray image from `rayproject/ray:${RAY_VERSION}` to `rayproject/ray-ml:${RAY_VERSION}`.# Step 6: Repeat Step 3 and Step 4\n\n```sh\n# Repeat Step 3 and Step 4 to log in to the new head Pod and run the Ray Serve application.# You should successfully launch the Ray Serve application this time.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7008f0bf-8120-4b70-a6ee-a1d92bc941d0": {"__data__": {"id_": "7008f0bf-8120-4b70-a6ee-a1d92bc941d0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "2d632e9a7b30f6540460c405abfd1b88"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md"}, "hash": "029dd7924aedd8d285eeb42568437cb12eb8c5cef3766a313888f3b5d2051cc2"}, "2": {"node_id": "0ce4a921-4009-457c-b8d3-d9e7edfcad99", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "df1f6557e5d959e1db9aee3b97462cdf"}, "hash": "34490a414761f4f1b9934667d2ed9552a94a2a367f254c3614ce21678155008a"}, "3": {"node_id": "c39e4c52-7f1e-43be-9b06-dd89269367e4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "294d9e6505547ed7a2fbfa68279650b5"}, "hash": "04d3c190fc2a2dc61234d21751df612cf9daee97bc874e3ae796359caf555d1f"}}, "hash": "99aa779468ba9c7a43258af898b294492120dd21c7d162fde1f8b6d163cbc008", "text": "serve run mobilenet.mobilenet:app\n\n# [Example output]\n# (ServeReplica:default_ImageClassifier pid=139, ip=10.244.0.8) Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n#     8192/14536120 [..............................] - ETA: 0s)\n#  4202496/14536120 [=======>......................] - ETA: 0s)\n# 12902400/14536120 [=========================>....] - ETA: 0s)\n# 14536120/14536120 [==============================] - 0s 0us/step\n# 2023-07-17 14:04:43,737 SUCC scripts.py:424 -- Deployed Serve app successfully.```\n\n# Step 7: Submit a request to the Ray Serve application\n\n```sh\n# (On your local machine) Forward the serve port of the head Pod\nkubectl port-forward --address 0.0.0.0 $HEAD_POD 8000\n\n# Clone the repository on your local machine\ngit clone https://github.com/ray-project/serve_config_examples.git\ncd serve_config_examples/mobilenet\n\n# Prepare a sample image file.`stable_diffusion_example.png` is a cat image generated by the Stable Diffusion model.curl -O https://raw.githubusercontent.com/ray-project/kuberay/master/docs/images/stable_diffusion_example.png\n\n# Update `image_path` in `mobilenet_req.py` to the path of `stable_diffusion_example.png`\n# Send a request to the Ray Serve application.python3 mobilenet_req.py\n\n# [Error message]\n# Unexpected error, traceback: ray::ServeReplica:default_ImageClassifier.handle_request() (pid=139, ip=10.244.0.8)\n#   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/_private/utils.py\", line 254, in wrap_to_ray_error\n#     raise exception\n#   File \"/home/ray/anaconda3/lib/python3.7/site-packages/ray/serve/_private/replica.py\", line 550, in invoke_single\n#     result = await method_to_call(*args, **kwargs)\n#   File \"./mobilenet/mobilenet.py\", line 24, in __call__\n#   File \"/home/ray/anaconda3/lib/python3.7/site-packages/starlette/requests.py\", line 256, in _get_form\n#     ), \"The `python-multipart` library must be installed to use form parsing.\"# AssertionError: The `python-multipart` library must be installed to use form parsing..\n```\n\n`python-multipart` is required for the request parsing function `starlette.requests.form()`, so the error message is reported when we send a request to the Ray Serve application.# Step 8: Restart the Ray Serve application with runtime environment.```sh\n# In the head Pod, stop the Ray Serve application\nserve shutdown\n\n# Check the Ray Serve application status\nserve status\n# [Example output]\n# There are no applications running on this cluster.# Launch the Ray Serve application with runtime environment.serve run mobilenet.mobilenet:app --runtime-env-json='{\"pip\": [\"python-multipart==0.0.6\"]}'\n\n# (On your local machine) Submit a request to the Ray Serve application again, and you should get the correct prediction.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c39e4c52-7f1e-43be-9b06-dd89269367e4": {"__data__": {"id_": "c39e4c52-7f1e-43be-9b06-dd89269367e4", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "294d9e6505547ed7a2fbfa68279650b5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md"}, "hash": "029dd7924aedd8d285eeb42568437cb12eb8c5cef3766a313888f3b5d2051cc2"}, "2": {"node_id": "7008f0bf-8120-4b70-a6ee-a1d92bc941d0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "2d632e9a7b30f6540460c405abfd1b88"}, "hash": "99aa779468ba9c7a43258af898b294492120dd21c7d162fde1f8b6d163cbc008"}}, "hash": "04d3c190fc2a2dc61234d21751df612cf9daee97bc874e3ae796359caf555d1f", "text": "python3 mobilenet_req.py\n# [Example output]\n# {\"prediction\": [\"n02123159\", \"tiger_cat\", 0.2994779646396637]}\n```\n\n# Step 9: Create a RayService YAML file\n\nIn the previous steps, we found that the Ray Serve application can be successfully launched using the Ray image `rayproject/ray-ml:${RAY_VERSION}` and the {ref}`runtime environments <runtime-environments>` `python-multipart==0.0.6`.Therefore, we can create a RayService YAML file with the same Ray image and runtime environment.For more details, please refer to [ray-service.mobilenet.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-service.mobilenet.yaml) and [mobilenet-rayservice.md](kuberay-mobilenet-rayservice-example).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "89e34c1b-3fe6-49c5-b2f4-3242c12a02cb": {"__data__": {"id_": "89e34c1b-3fe6-49c5-b2f4-3242c12a02cb", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "ffaf23a1efb2c689be942d7c9a8802eb"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md"}, "hash": "633e34c38092d2772503d7348574ee34f16ee20a9eb5729fdce33e322004f4e8"}, "3": {"node_id": "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "37c264f1656bfb2d4257270c33bded97"}, "hash": "5d29e018188cc15aff65b368ac828fd66c3b7d6117bf41a641dc148d736a9d4a"}}, "hash": "c38027b4c2e23a6e914d05908dd2f7846edb391bd7035f7e4e8f5237753d4fda", "text": "(deploy-a-static-ray-cluster-without-kuberay)=\n\n# (Advanced) Deploying a static Ray cluster without KubeRay\n\nThis deployment method for Ray no longer requires the use of CustomResourceDefinitions (CRDs).In contrast, the CRDs is a prerequisite to use KubeRay.One of its key components, the KubeRay operator,\nmanages the Ray cluster resources by watching for Kubernetes events (create/delete/update).Although the KubeRay operator can function within a single namespace, the use of CRDs has a cluster-wide scope.If the necessary Kubernetes admin permissions are not available for deploying KubeRay, this doc introduces a way to deploy a static Ray cluster to Kubernetes without using KubeRay.However, it should be noted that this deployment method lacks the built-in\nautoscaling feature that KubeRay provides.## Preparation\n\n### Install the latest Ray release\n\nThis step is necessary for interacting with remote clusters using {ref}`Ray Job Submission <jobs-overview>`.```\n!pip install -U \"ray[default]\"\n```\n\nSee {ref}`installation` for more details.### Install kubectl\n\nTo interact with Kubernetes, we will use kubectl.Installation instructions can be found in the [Kubernetes documentation](https://kubernetes.io/docs/tasks/tools/#kubectl).### Access a Kubernetes cluster\n\nWe will need access to a Kubernetes cluster.There are two options:\n\n1.Configure access to a remote Kubernetes cluster\n**OR**\n\n2.Run the examples locally by [installing kind](https://kind.sigs.k8s.io/docs/user/quick-start/#installation).Start your [kind](https://kind.sigs.k8s.io/) cluster by running the following command:\n\n```\n!kind create cluster\n```\n\nTo execute the example in this guide, ensure that your Kubernetes cluster (or local Kind cluster) can handle additional resource requests of 3 CPU and 3Gi memory.Also, ensure that both your Kubernetes cluster and Kubectl are at least version 1.19.### Deploying a Redis for fault tolerance\n\nNote that [the Kubernetes deployment config file](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml) has a section to deploy a Redis to Kubernetes so that the Ray head can write through the GCS metadata.If a Redis has already been deployed on Kubernetes, this section can be omitted.## Deploying a static Ray cluster\n\nIn this section, we will deploy a static Ray cluster into the `default` namespace without using KubeRay.To use another\nnamespace, specify the namespace in your kubectl commands:\n\n`kubectl -n <your-namespace> ...`\n\n```\n# Deploy a sample Ray Cluster from the Ray repo:\n\n!kubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\n\n# Note that the Ray cluster has fault tolerance enabled by default using the external Redis.# Please set the Redis IP address in the config.# The password is currently set as '' for the external Redis.# Please download the config file and substitute the real password for the empty string if the external Redis has a password.```\n\nOnce the Ray cluster has been deployed, you can view the pods for the head node and worker nodes by running\n\n```\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f": {"__data__": {"id_": "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "37c264f1656bfb2d4257270c33bded97"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md"}, "hash": "633e34c38092d2772503d7348574ee34f16ee20a9eb5729fdce33e322004f4e8"}, "2": {"node_id": "89e34c1b-3fe6-49c5-b2f4-3242c12a02cb", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "ffaf23a1efb2c689be942d7c9a8802eb"}, "hash": "c38027b4c2e23a6e914d05908dd2f7846edb391bd7035f7e4e8f5237753d4fda"}, "3": {"node_id": "db5e5308-fb39-4336-ae2e-0d5416dce7b2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "b547a4ee662a245f83e87436912407f3"}, "hash": "6c4c39163670dbf9c5720bf850523edd45984ff24c1b11175c07d72766b846e2"}}, "hash": "5d29e018188cc15aff65b368ac828fd66c3b7d6117bf41a641dc148d736a9d4a", "text": "kubectl get pods\n\n# NAME                                             READY   STATUS    RESTARTS   AGE\n# deployment-ray-head-xxxxx                        1/1     Running   0          XXs\n# deployment-ray-worker-xxxxx                      1/1     Running   0          XXs\n# deployment-ray-worker-xxxxx                      1/1     Running   0          XXs\n```\n\nWait for the pods to reach the `Running` state.This may take a few minutes -- most of this time is spent downloading the Ray images.In a separate shell, you may wish to observe the pods' status in real-time with the following command:\n\n```\n# If you're on MacOS, first `brew install watch`.# Run in a separate shell:\n\n!watch -n 1 kubectl get pod\n```\n\nIf your pods are stuck in the `Pending` state, you can check for errors via `kubectl describe pod deployment-ray-head-xxxx-xxxxx`\nand ensure that your Docker resource limits are set high enough.Note that in production scenarios, you will want to use larger Ray pods.In fact, it is advantageous to size each Ray pod to take up an entire Kubernetes node.See the [configuration guide](kuberay-config) for more details.## Deploying a network policy for the static Ray cluster\n\nIf your Kubernetes has a default deny network policy for pods, you need to manually create a network policy to allow bidirectional\ncommunication among the head and worker nodes in the Ray cluster as mentioned in [the ports configurations doc](https://docs.ray.io/en/latest/ray-core/configure.html#ports-configurations).```\n# Create a sample network policy for the static Ray cluster from the Ray repo:\n!kubectl apply -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml\n```\n\nOnce the network policy has been deployed, you can view the network policy for the static Ray cluster by running\n\n```\n!kubectl get networkpolicies\n\n# NAME                               POD-SELECTOR                           AGE\n# ray-head-egress                    app=ray-cluster-head                   XXs\n# ray-head-ingress                   app=ray-cluster-head                   XXs\n# ray-worker-egress                  app=ray-cluster-worker                 XXs\n# ray-worker-ingress                 app=ray-cluster-worker                 XXs\n```\n\n### External Redis Integration for fault tolerance\n\nRay by default uses an internal key-value store, called the Global Control Store (GCS).The GCS runs on the head node and stores cluster\nmetadata.One drawback of this approach is that the head node loses the metadata if it crashes.Ray can also write this metadata to an external Redis for reliability and high availability.With this setup, the static Ray cluster can recover from head node crashes and tolerate GCS failures without losing connections to worker nodes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db5e5308-fb39-4336-ae2e-0d5416dce7b2": {"__data__": {"id_": "db5e5308-fb39-4336-ae2e-0d5416dce7b2", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "b547a4ee662a245f83e87436912407f3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md"}, "hash": "633e34c38092d2772503d7348574ee34f16ee20a9eb5729fdce33e322004f4e8"}, "2": {"node_id": "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "37c264f1656bfb2d4257270c33bded97"}, "hash": "5d29e018188cc15aff65b368ac828fd66c3b7d6117bf41a641dc148d736a9d4a"}, "3": {"node_id": "20deaa3c-55ad-4769-8840-1fb167e4d9ee", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "d857b8769ad678bfa993295fd74eacfa"}, "hash": "ecc5b6fab63cdf9acd0c712c144d9f6d9a2709ba11dffcc1a14a4bbcfd3ab35b"}}, "hash": "6c4c39163670dbf9c5720bf850523edd45984ff24c1b11175c07d72766b846e2", "text": "To use this feature, we need to pass in the `RAY_REDIS_ADDRESS` env var and `--redis-password` in the Ray head node section of [the Kubernetes deployment config file](https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml).## Running Applications on the static Ray Cluster\n\nIn this section, we will interact with the static Ray cluster that just got deployed.### Accessing the cluster with kubectl exec\n\nSame as the Ray cluster that deployed using KubeRay, we can exec directly into the head pod and run a Ray program.Firstly, run the command below to get the head pod:\n\n```\n!kubectl get pods --selector=app=ray-cluster-head\n\n# NAME                                             READY   STATUS    RESTARTS   AGE\n# deployment-ray-head-xxxxx                        1/1     Running   0          XXs\n```\n\nWe can now execute a Ray program on the previously identified head pod.The following command connects to the Ray Cluster and then terminates the Ray program.```\n# Substitute your output from the last cell in place of \"deployment-ray-head-xxxxx\"\n\n!kubectl exec deployment-ray-head-xxxxx -it -c ray-head -- python -c \"import ray; ray.init('auto')\"\n# 2022-08-10 11:23:17,093 INFO worker.py:1312 -- Connecting to existing Ray cluster at address: <IP address>:6380...\n# 2022-08-10 11:23:17,097 INFO worker.py:1490 -- Connected to Ray cluster.View the dashboard at ...\n```\n\nAlthough the above cell can be useful for occasional execution on the Ray Cluster, the recommended approach for running an application on a Ray Cluster is to use [Ray Jobs](jobs-quickstart).### Ray Job submission\n\nTo set up your Ray Cluster for Ray Jobs submission, it is necessary to ensure that the Ray Jobs port is accessible to the client.Ray receives job requests through the Dashboard server on the head node.First, we need to identify the Ray head node.The static Ray cluster configuration file sets up a\n[Kubernetes service](https://kubernetes.io/docs/concepts/services-networking/service/) that targets the Ray head pod.This service lets us interact with Ray clusters without directly executing commands in the Ray container.To identify the Ray head service for our example cluster, run:\n\n```\n!kubectl get service service-ray-cluster\n\n# NAME                             TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)                            AGE\n# service-ray-cluster              ClusterIP   10.92.118.20   <none>        6380/TCP,8265/TCP,10001/TCP...     XXs\n```\n\nNow that we have the name of the service, we can use port-forwarding to access the Ray Dashboard port (8265 by default).```\n# Execute this in a separate shell.# Substitute the service name in place of service-ray-cluster\n\n!kubectl port-forward service/service-ray-cluster 8265:8265\n```\n\nNow that we have access to the Dashboard port, we can submit jobs to the Ray Cluster for execution:\n\n```\n!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "20deaa3c-55ad-4769-8840-1fb167e4d9ee": {"__data__": {"id_": "20deaa3c-55ad-4769-8840-1fb167e4d9ee", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "d857b8769ad678bfa993295fd74eacfa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md"}, "hash": "633e34c38092d2772503d7348574ee34f16ee20a9eb5729fdce33e322004f4e8"}, "2": {"node_id": "db5e5308-fb39-4336-ae2e-0d5416dce7b2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "b547a4ee662a245f83e87436912407f3"}, "hash": "6c4c39163670dbf9c5720bf850523edd45984ff24c1b11175c07d72766b846e2"}}, "hash": "ecc5b6fab63cdf9acd0c712c144d9f6d9a2709ba11dffcc1a14a4bbcfd3ab35b", "text": "ray job submit --address http://localhost:8265 -- python -c \"import ray; ray.init(); print(ray.cluster_resources())\"\n```\n\n## Cleanup\n\n### Deleting a Ray Cluster\n\nDelete the static Ray cluster service and deployments\n\n```\n!kubectl delete -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster.with-fault-tolerance.yaml\n```\n\nDelete the static Ray cluster network policy\n\n```\n!kubectl delete -f https://raw.githubusercontent.com/ray-project/ray/master/doc/source/cluster/kubernetes/configs/static-ray-cluster-networkpolicy.yaml\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "48cc0889-d983-45e6-83aa-7fa9e56d497f": {"__data__": {"id_": "48cc0889-d983-45e6-83aa-7fa9e56d497f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "e9cf83d9e7ea1377f9ab6016848a33e3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "375bc84261ca3cac1e7707fce62d9e734af45c56", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md"}, "hash": "67459108f1a392df935f2a09bb1059e3ffe43cd5f9119a059eae118f381462ca"}, "3": {"node_id": "630dd8e4-1238-4b2d-be4e-a7b5924608e4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "c33c054d037901e917c3c76c16c6d106"}, "hash": "7b1a75e63b563a7ca12d0bd28a80514dca7e46186a5bab502aec48415f4c3a3a"}}, "hash": "228f694f04294aaacfb6d6ff9bc1f58594b483fbb242ff1afef6c96e846cd8be", "text": "(kuberay-tls)=\n\n# TLS Authentication\n\nRay can be configured to use TLS on its gRPC channels.This means that\nconnecting to the Ray head will require an appropriate\nset of credentials and also that data exchanged between various processes\n(client, head, workers) will be encrypted ([Ray's document](https://docs.ray.io/en/latest/ray-core/configure.html?highlight=tls#tls-authentication)).This document provides detailed instructions for generating a public-private\nkey pair and CA certificate for configuring KubeRay.> Warning: Enabling TLS will cause a performance hit due to the extra\noverhead of mutual authentication and encryption.Testing has shown that\nthis overhead is large for small workloads and becomes relatively smaller\nfor large workloads.The exact overhead will depend on the nature of your\nworkload.# Prerequisites\n\nTo fully understand this document, it's highly recommended that you have a\nsolid understanding of the following concepts:\n\n* private/public key\n* CA (certificate authority)\n* CSR (certificate signing request)\n* self-signed certificate\n\nThis [YouTube video](https://youtu.be/T4Df5_cojAs) is a good start.# TL;DR\n\n> Please note that this document is designed to support KubeRay version 0.5.0 or later.If you are using an older version of KubeRay, some of the instructions or configurations may not apply or may require additional modifications.> Warning: Please note that the `ray-cluster.tls.yaml` file is intended for demo purposes only.It is crucial that you **do not** store\nyour CA private key in a Kubernetes Secret in your production environment.```sh\n# Install v0.6.0 KubeRay operator\n# `ray-cluster.tls.yaml` will cover from Step 1 to Step 3\n\n# Download `ray-cluster.tls.yaml`\ncurl -LO https://raw.githubusercontent.com/ray-project/kuberay/master/ray-operator/config/samples/ray-cluster.tls.yaml\n\n# Create a RayCluster\nkubectl apply -f ray-cluster.tls.yaml\n\n# Jump to Step 4 \"Verify TLS authentication\" to verify the connection.```\n\n`ray-cluster.tls.yaml` will create:\n\n* A Kubernetes Secret containing the CA's private key (`ca.key`) and self-signed certificate (`ca.crt`) (**Step 1**)\n* A Kubernetes ConfigMap containing the scripts `gencert_head.sh` and `gencert_worker.sh`, which allow Ray Pods to generate private keys\n(`tls.key`) and self-signed certificates (`tls.crt`) (**Step 2**)\n* A RayCluster with proper TLS environment variables configurations (**Step 3**)\n\nThe certificate (`tls.crt`) for a Ray Pod is encrypted using the CA's private key (`ca.key`).Additionally, all Ray Pods have the CA's public key included in `ca.crt`, which allows them to decrypt certificates from other Ray Pods.# Step 1: Generate a private key and self-signed certificate for CA\n\nIn this document, a self-signed certificate is used, but users also have the\noption to choose a publicly trusted certificate authority (CA) for their TLS\nauthentication.```sh\n# Step 1-1: Generate a self-signed certificate and a new private key file for CA.openssl req -x509 \\\n            -sha256 -days 3650 \\\n            -nodes \\\n            -newkey rsa:2048 \\\n            -subj \"/CN=*.kuberay.com/C=US/L=San Francisco\" \\\n            -keyout ca.key -out ca.crt\n\n# Step 1-2: Check the CA's public key from the self-signed certificate.openssl x509 -in ca.crt -noout -text\n\n# Step 1-3\n# Method 1: Use `cat $FILENAME | base64` to encode `ca.key` and `ca.crt`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "630dd8e4-1238-4b2d-be4e-a7b5924608e4": {"__data__": {"id_": "630dd8e4-1238-4b2d-be4e-a7b5924608e4", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "c33c054d037901e917c3c76c16c6d106"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "375bc84261ca3cac1e7707fce62d9e734af45c56", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md"}, "hash": "67459108f1a392df935f2a09bb1059e3ffe43cd5f9119a059eae118f381462ca"}, "2": {"node_id": "48cc0889-d983-45e6-83aa-7fa9e56d497f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "e9cf83d9e7ea1377f9ab6016848a33e3"}, "hash": "228f694f04294aaacfb6d6ff9bc1f58594b483fbb242ff1afef6c96e846cd8be"}, "3": {"node_id": "002471ba-af14-4042-afdc-d4e701d4fa58", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "962169d6450b5178585befb1c2728b84"}, "hash": "0336a6cb1cda85ce3cfea8e9aa8a72a007c3fa9601e59a1b0116e63fe5b67713"}}, "hash": "7b1a75e63b563a7ca12d0bd28a80514dca7e46186a5bab502aec48415f4c3a3a", "text": "#           Then, paste the encoding strings to the Kubernetes Secret in `ray-cluster.tls.yaml`.# Method 2: Use kubectl to encode the certifcate as Kubernetes Secret automatically.#           (Note: You should comment out the Kubernetes Secret in `ray-cluster.tls.yaml`.)kubectl create secret generic ca-tls --from-file=ca.key --from-file=ca.crt\n```\n\n* `ca.key`: CA's private key\n* `ca.crt`: CA's self-signed certificate\n\nThis step is optional because the `ca.key` and `ca.crt` files have\nalready been included in the Kubernetes Secret specified in [ray-cluster.tls.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.tls.yaml).# Step 2: Create separate private key and self-signed certificate for Ray Pods\n\nIn [ray-cluster.tls.yaml](https://github.com/ray-project/kuberay/blob/master/ray-operator/config/samples/ray-cluster.tls.yaml), each Ray\nPod (both head and workers) generates its own private key file (`tls.key`) and self-signed\ncertificate file (`tls.crt`) in its init container.We generate separate files for each Pod\nbecause worker Pods do not have deterministic DNS names, and we cannot use the same\ncertificate across different Pods.In the YAML file, you'll find a ConfigMap named `tls` that contains two shell scripts:\n`gencert_head.sh` and `gencert_worker.sh`.These scripts are used to generate the private key\nand self-signed certificate files (`tls.key` and `tls.crt`) for the Ray head and worker Pods.An alternative approach for users is to prebake the shell scripts directly into the docker image that's utilized\nby the init containers, rather than relying on a ConfigMap.Please find below a brief explanation of what happens in each of these scripts:\n1.A 2048-bit RSA private key is generated and saved as `/etc/ray/tls/tls.key`.2.A Certificate Signing Request (CSR) is generated using the private key file (`tls.key`)\nand the `csr.conf` configuration file.3.A self-signed certificate (`tls.crt`) is generated using the private key of the\nCertificate Authority (`ca.key`) and the previously generated CSR.The only difference between `gencert_head.sh` and `gencert_worker.sh` is the `[ alt_names ]`\nsection in `csr.conf` and `cert.conf`.The worker Pods use the fully qualified domain name\n(FQDN) of the head Kubernetes Service to establish a connection with the head Pod.Therefore, the `[alt_names]` section for the head Pod needs to include the FQDN of the head\nKubernetes Service.By the way, the head Pod uses `$POD_IP` to communicate with worker Pods.```sh\n# gencert_head.sh\n[alt_names]\nDNS.1 = localhost\nDNS.2 = $FQ_RAY_IP\nIP.1 = 127.0.0.1\nIP.2 = $POD_IP\n\n# gencert_worker.sh\n[alt_names]\nDNS.1 = localhost\nIP.1 = 127.0.0.1\nIP.2 = $POD_IP\n```\n\nIn [Kubernetes networking model](https://github.com/kubernetes/design-proposals-archive/blob/main/network/networking.md#pod-to-pod), the IP that a Pod sees itself as is the same IP that others see it as.That's why Ray Pods can self-register for the certificates.# Step 3: Configure environment variables for Ray TLS authentication\n\nTo enable TLS authentication in your Ray cluster, set the following environment variables:\n\n- `RAY_USE_TLS`: Either 1 or 0 to use/not-use TLS.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "002471ba-af14-4042-afdc-d4e701d4fa58": {"__data__": {"id_": "002471ba-af14-4042-afdc-d4e701d4fa58", "embedding": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "962169d6450b5178585befb1c2728b84"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "375bc84261ca3cac1e7707fce62d9e734af45c56", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md"}, "hash": "67459108f1a392df935f2a09bb1059e3ffe43cd5f9119a059eae118f381462ca"}, "2": {"node_id": "630dd8e4-1238-4b2d-be4e-a7b5924608e4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "c33c054d037901e917c3c76c16c6d106"}, "hash": "7b1a75e63b563a7ca12d0bd28a80514dca7e46186a5bab502aec48415f4c3a3a"}}, "hash": "0336a6cb1cda85ce3cfea8e9aa8a72a007c3fa9601e59a1b0116e63fe5b67713", "text": "If this is set to 1 then all of the environment variables below must be set.Default: 0.- `RAY_TLS_SERVER_CERT`: Location of a certificate file which is presented to other endpoints so as to achieve mutual authentication (i.e.`tls.crt`).- `RAY_TLS_SERVER_KEY`: Location of a private key file which is the cryptographic means to prove to other endpoints that you are the authorized user of a given certificate (i.e.`tls.key`).- `RAY_TLS_CA_CERT`: Location of a CA certificate file which allows TLS to decide whether an endpoint\u2019s certificate has been signed by the correct authority (i.e.`ca.crt`).For more information on how to configure Ray with TLS authentication, please refer to [Ray's document](https://docs.ray.io/en/latest/ray-core/configure.html#tls-authentication).# Step 4: Verify TLS authentication\n\n```sh\n# Log in to the worker Pod\nkubectl exec -it ${WORKER_POD} -- bash\n\n# Since the head Pod has the certificate of $FQ_RAY_IP, the connection to the worker Pods\n# will be established successfully, and the exit code of the ray health-check command\n# should be 0.\nray health-check --address $FQ_RAY_IP:6379\necho $?# 0\n\n# Since the head Pod has the certificate of $RAY_IP, the connection will fail and an error\n# message similar to the following will be displayed: \"Peer name raycluster-tls-head-svc is\n# not in peer certificate\".ray health-check --address $RAY_IP:6379\n\n# If you add `DNS.3 = $RAY_IP` to the [alt_names] section in `gencert_head.sh`,\n# the head Pod will generate the certificate of $RAY_IP.#\n# For KubeRay versions prior to 0.5.0, this step is necessary because Ray workers in earlier\n# versions use $RAY_IP to connect with Ray head.```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4f1762bb-9149-4166-ae4e-7ba4073ba3a5": {"__data__": {"id_": "4f1762bb-9149-4166-ae4e-7ba4073ba3a5", "embedding": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "2e5af2d63c01dc0a888b7af1610196e6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md"}, "hash": "6e7562b34f212b6db8a75d2b79e2628348b269ee13992c6a41db4507f951018b"}, "3": {"node_id": "528bcaac-3569-45ba-a00f-7444e32e335e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "0d12cff1eb0dbf75d139f7ee3f515c19"}, "hash": "14fa95832a03b0839ce7f42ca3ead543e17c4188357105cca262127ca55d6509"}}, "hash": "374f78da863be8e907cd085b5a0a4f37aa0de4ee6537118247902bd7f11187ef", "text": "(collect-metrics)=\n# Collecting and monitoring metrics\nMetrics are useful for monitoring and troubleshooting Ray applications and Clusters. For example, you may want to access a node's metrics if it terminates unexpectedly.\n\nRay records and emits time-series metrics using the [Prometheus format](https://prometheus.io/docs/instrumenting/exposition_formats/). Ray does not provide a native storage solution for metrics. Users need to manage the lifecycle of the metrics by themselves. This page provides instructions on how to collect and monitor metrics from Ray Clusters.\n\n\n## System and application metrics\nRay exports metrics if you use `ray[default]`, `ray[air]`, or {ref}`other installation commands <installation>` that include Dashboard component. Dashboard agent process is responsible for aggregating and reporting metrics to the endpoints for Prometheus to scrape.\n\n**System metrics**: Ray exports a number of system metrics. View {ref}`system metrics <system-metrics>` for more details about the emitted metrics.\n\n**Application metrics**: Application-specific metrics are useful for monitoring your application states. View {ref}`adding application metrics <application-level-metrics>` for how to record metrics.\n\n(prometheus-setup)=\n## Setting up your Prometheus server\nUse Prometheus to scrape metrics from Ray Clusters. Ray doesn't start Prometheus servers for users. Users need to decide where to host and configure it to scrape the metrics from Clusters.\n\n```{admonition} Tip\n:class: tip\nThe instructions below describe one way of setting up Prometheus on your local machine. View [Prometheus documentation](https://prometheus.io/docs/introduction/overview/) for the best strategy to set up your Prometheus server.\n\nFor KubeRay users, follow [these instructions](kuberay-prometheus-grafana) to set up Prometheus.\n```\n\nFirst, [download Prometheus](https://prometheus.io/download/). Make sure to download the correct binary for your operating system. (For example, Darwin for macOS X.)\n\nThen, unzip the archive into a local directory using the following command:\n\n```bash\ntar xvfz prometheus-*.tar.gz\ncd prometheus-*\n```\n\nRay provides a Prometheus config that works out of the box. After running Ray, you can find the config at `/tmp/ray/session_latest/metrics/prometheus/prometheus.yml`.\n\n```yaml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nscrape_configs:\n# Scrape from each Ray node as defined in the service_discovery.json provided by Ray.\n- job_name: 'ray'\n  file_sd_configs:\n  - files:\n    - '/tmp/ray/prom_metrics_service_discovery.json'\n```\n\nNext, start Prometheus:\n\n```shell\n./prometheus --config.file=/tmp/ray/session_latest/metrics/prometheus/prometheus.yml\n```\n```{admonition} Note\n:class: note\nIf you are using macOS, you may receive an error at this point about trying to launch an application where the developer has not been verified. See the \"Troubleshooting\" guide below to fix the issue.\n```\n\nNow, you can access Ray metrics from the default Prometheus URL, `http://localhost:9090`.\n\n\n### Troubleshooting\n#### Using Ray configurations in Prometheus with Homebrew on macOS X\nHomebrew installs Prometheus as a service that is automatically launched for you.\nTo configure these services, you cannot simply pass in the config files as command line arguments.\n\nInstead, change the --config-file line in `/usr/local/etc/prometheus.args` to read `--config.file /tmp/ray/session_latest/metrics/prometheus/prometheus.yml`.\n\nYou can then start or restart the services with `brew services start prometheus`.\n\n\n#### macOS does not trust the developer to install Prometheus\nYou may receive the following error:\n\n![trust error](https://raw.githubusercontent.com/ray-project/Images/master/docs/troubleshooting/prometheus-trusted-developer.png)\n\nWhen downloading binaries from the internet, macOS requires that the binary be signed by a trusted developer ID.Many developers are not on macOS's trusted list.Users can manually override this requirement.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "528bcaac-3569-45ba-a00f-7444e32e335e": {"__data__": {"id_": "528bcaac-3569-45ba-a00f-7444e32e335e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "0d12cff1eb0dbf75d139f7ee3f515c19"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md"}, "hash": "6e7562b34f212b6db8a75d2b79e2628348b269ee13992c6a41db4507f951018b"}, "2": {"node_id": "4f1762bb-9149-4166-ae4e-7ba4073ba3a5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "2e5af2d63c01dc0a888b7af1610196e6"}, "hash": "374f78da863be8e907cd085b5a0a4f37aa0de4ee6537118247902bd7f11187ef"}, "3": {"node_id": "483f3f3d-30ab-4acb-a327-1fc4d919fa0c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "04b6062d9b7956aa3efd09b5192a4ed7"}, "hash": "8d3a21bbf44c9125f51dfb1b543742163e05f6b23ce7ee961cc8f6d87474b04c"}}, "hash": "14fa95832a03b0839ce7f42ca3ead543e17c4188357105cca262127ca55d6509", "text": "See [these instructions](https://support.apple.com/guide/mac-help/open-a-mac-app-from-an-unidentified-developer-mh40616/mac) for how to override the restriction and install or run the application.#### Loading Ray Prometheus configurations with Docker Compose\nIn the Ray container, the symbolic link \"/tmp/ray/session_latest/metrics\" points to the latest active Ray session.However, Docker does not support the mounting of symbolic links on shared volumes and you may fail to load the Prometheus configuration files.To fix this issue, employ an automated shell script for seamlessly transferring the Prometheus configurations from the Ray container to a shared volume.To ensure a proper setup, mount the shared volume on the respective path for the container, which contains the recommended configurations to initiate the Prometheus servers.(scrape-metrics)=\n## Scraping metrics\nRay runs a metrics agent per node to export system and application metrics.Each metrics agent collects metrics from the local\nnode and exposes them in a Prometheus format.You can then scrape each endpoint to access the metrics.To scrape the endpoints, we need to ensure service discovery, which allows Prometheus to find the metrics agents' endpoints on each node.### Auto-discovering metrics endpoints\n\nYou can allow Prometheus to dynamically find the endpoints to scrape by using Prometheus' [file based service discovery](https://prometheus.io/docs/guides/file-sd/#installing-configuring-and-running-prometheus).Use auto-discovery to export Prometheus metrics when using the Ray {ref}`cluster launcher <vm-cluster-quick-start>`, as node IP addresses can often change as the cluster scales up and down.Ray auto-generates a Prometheus [service discovery file](https://prometheus.io/docs/guides/file-sd/#installing-configuring-and-running-prometheus) on the head node to facilitate metrics agents' service discovery.This function allows you to scrape all metrics in the cluster without knowing their IPs.The following information guides you on the setup.The service discovery file is generated on the {ref}`head node <cluster-head-node>`.On this node, look for ``/tmp/ray/prom_metrics_service_discovery.json`` (or the eqiuvalent file if using a custom Ray ``temp_dir``).Ray periodically updates this file with the addresses of all metrics agents in the cluster.Ray automatically produces a Prometheus config, which scrapes the file for service discovery found at `/tmp/ray/session_latest/metrics/prometheus/prometheus.yml`.You can choose to use this config or modify your own config to enable this behavior.See the details of the config below.Find the full documentation [here](https://prometheus.io/docs/prometheus/latest/configuration/configuration/).With this config, Prometheus automatically updates the addresses that it scrapes based on the contents of Ray's service discovery file.```yaml\n# Prometheus config file\n\n# my global config\nglobal:\n  scrape_interval:     2s\n  evaluation_interval: 2s\n\n# Scrape from Ray.scrape_configs:\n- job_name: 'ray'\n  file_sd_configs:\n  - files:\n    - '/tmp/ray/prom_metrics_service_discovery.json'\n```\n\n### Manually discovering metrics endpoints\n\nIf you know the IP addresses of the nodes in your Ray Cluster, you can configure Prometheus to read metrics from a static list of endpoints.Set a fixed port that Ray should use to export metrics.If you're using the VM Cluster Launcher, pass ``--metrics-export-port=<port>`` to ``ray start``.If you're using KubeRay, specify ``rayStartParams.metrics-export-port`` in the RayCluster configuration file.You must specify the port on all nodes in the cluster.If you do not know the IP addresses of the nodes in your Ray Cluster, you can also programmatically discover the endpoints by reading the Ray Cluster information.The following example uses a Python script and the {py:obj}`ray.nodes` API to find the metrics agents' URLs, by combining the ``NodeManagerAddress`` with the ``MetricsExportPort``.```python\n# On a cluster node:\nimport ray\nray.init()\nfrom pprint import pprint\npprint(ray.nodes())\n\n\"\"\"\nPass the <NodeManagerAddress>:<MetricsExportPort> from each of these entries\nto Prometheus.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "483f3f3d-30ab-4acb-a327-1fc4d919fa0c": {"__data__": {"id_": "483f3f3d-30ab-4acb-a327-1fc4d919fa0c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "04b6062d9b7956aa3efd09b5192a4ed7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md"}, "hash": "6e7562b34f212b6db8a75d2b79e2628348b269ee13992c6a41db4507f951018b"}, "2": {"node_id": "528bcaac-3569-45ba-a00f-7444e32e335e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "0d12cff1eb0dbf75d139f7ee3f515c19"}, "hash": "14fa95832a03b0839ce7f42ca3ead543e17c4188357105cca262127ca55d6509"}, "3": {"node_id": "db4fe360-861c-4758-bdbf-606a7bb18307", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "abedfaaf19077d82c4e39abe5852c75e"}, "hash": "0cb7af9127fe8817725712f8ae34a1fd3a31ef83511a1037447d518750106b2d"}}, "hash": "8d3a21bbf44c9125f51dfb1b543742163e05f6b23ce7ee961cc8f6d87474b04c", "text": "[{'Alive': True,\n  'MetricsExportPort': 8080,\n  'NodeID': '2f480984702a22556b90566bdac818a4a771e69a',\n  'NodeManagerAddress': '192.168.1.82',\n  'NodeManagerHostname': 'host2.attlocal.net',\n  'NodeManagerPort': 61760,\n  'ObjectManagerPort': 61454,\n  'ObjectStoreSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/plasma_store',\n  'RayletSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/raylet',\n  'Resources': {'CPU': 1.0,\n                'memory': 123.0,\n                'node:192.168.1.82': 1.0,\n                'object_store_memory': 2.0},\n  'alive': True},\n{'Alive': True,\n  'MetricsExportPort': 8080,\n  'NodeID': 'ce6f30a7e2ef58c8a6893b3df171bcd464b33c77',\n  'NodeManagerAddress': '192.168.1.82',\n  'NodeManagerHostname': 'host1.attlocal.net',\n  'NodeManagerPort': 62052,\n  'ObjectManagerPort': 61468,\n  'ObjectStoreSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/plasma_store.1',\n  'RayletSocketName': '/tmp/ray/session_2020-08-04_18-18-16_481195_34255/sockets/raylet.1',\n  'Resources': {'CPU': 1.0,\n                'memory': 134.0,\n                'node:192.168.1.82': 1.0,\n                'object_store_memory': 2.0},\n  'alive': True}]\n\"\"\"\n```\n\n## Processing and exporting metrics\nIf you need to process and export metrics into other storage or management systems, check out open source metric processing tools like [Vector][Vector].[Vector]: https://vector.dev/", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "db4fe360-861c-4758-bdbf-606a7bb18307": {"__data__": {"id_": "db4fe360-861c-4758-bdbf-606a7bb18307", "embedding": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "abedfaaf19077d82c4e39abe5852c75e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md"}, "hash": "6e7562b34f212b6db8a75d2b79e2628348b269ee13992c6a41db4507f951018b"}, "2": {"node_id": "483f3f3d-30ab-4acb-a327-1fc4d919fa0c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "04b6062d9b7956aa3efd09b5192a4ed7"}, "hash": "8d3a21bbf44c9125f51dfb1b543742163e05f6b23ce7ee961cc8f6d87474b04c"}, "3": {"node_id": "73536aa5-a2db-4434-bf87-c393d1649dd5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "99348d904eac72f7fd0a5d23b5142acc"}, "hash": "d937ad4c6b0d28d56a44a96606f2a95b7a097df3ea8224d03ddc3442e495a005"}}, "hash": "0cb7af9127fe8817725712f8ae34a1fd3a31ef83511a1037447d518750106b2d", "text": "## Monitoring metrics\nTo visualize and monitor collected metrics, there are 3 common paths:\n\n1. **Simplist**: Use Grafana with Ray-provided configurations, which include default Grafana dashboards showing some of the most valuable metrics for debugging Ray applications.\n2. **Recommended**: Use Ray Dashboard which embeds Grafana visualizations and look at metrics together with logs, Job info and so on in a single pane of glass.\n3. **Manual**: Set up Grafana or other tools like CloudWatch, Cloud Monitoring, and Datadog from scratch.\n\nHere are some instructions for each of the paths:\n\n(grafana)=\n### Simplist: Setting up Grafana with Ray-provided configurations\nGrafana is a tool that supports advanced visualizations of Prometheus metrics and allows you to create custom dashboards with your favorite metrics. \n\n::::{tab-set}\n\n:::{tab-item} Creating a new Grafana server\n\n```{admonition} Note\n:class: note\nThe instructions below describe one way of starting a Grafana server on a macOS machine. Refer to the [Grafana documentation](https://grafana.com/docs/grafana/latest/setup-grafana/start-restart-grafana/#start-the-grafana-server) for how to start Grafana servers in different systems. \n\nFor KubeRay users, follow [these instructions](kuberay-prometheus-grafana) to set up Grafana.\n```\n\nFirst, [download Grafana](https://grafana.com/grafana/download). Follow the instructions on the download page to download the right binary for your operating system.\n\nGo to the location of the binary and run Grafana using the built-in configuration found in the `/tmp/ray/session_latest/metrics/grafana` folder.\n\n```shell\n./bin/grafana-server --config /tmp/ray/session_latest/metrics/grafana/grafana.ini web\n```\n\nAccess Grafana using the default grafana URL, `http://localhost:3000`.\nSee the default dashboard by going to dashboards -> manage -> Ray -> Default Dashboard. The same {ref}`metric graphs <system-metrics>` are accessible in {ref}`Ray Dashboard <observability-getting-started>` after you integrate Grafana with Ray Dashboard.\n\n```{admonition} Note\n:class: note\nIf this is your first time using Grafana, login with the username: `admin` and password `admin`.\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "73536aa5-a2db-4434-bf87-c393d1649dd5": {"__data__": {"id_": "73536aa5-a2db-4434-bf87-c393d1649dd5", "embedding": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "99348d904eac72f7fd0a5d23b5142acc"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md"}, "hash": "6e7562b34f212b6db8a75d2b79e2628348b269ee13992c6a41db4507f951018b"}, "2": {"node_id": "db4fe360-861c-4758-bdbf-606a7bb18307", "node_type": null, "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "abedfaaf19077d82c4e39abe5852c75e"}, "hash": "0cb7af9127fe8817725712f8ae34a1fd3a31ef83511a1037447d518750106b2d"}}, "hash": "d937ad4c6b0d28d56a44a96606f2a95b7a097df3ea8224d03ddc3442e495a005", "text": "![grafana login](images/graphs.png)\n\n**Troubleshooting**\n***Using Ray configurations in Grafana with Homebrew on macOS X***\n\nHomebrew installs Grafana as a service that is automatically launched for you.\nTherefore, to configure these services, you cannot simply pass in the config files as command line arguments.\n\nInstead, update the `/usr/local/etc/grafana/grafana.ini` file so that it matches the contents of `/tmp/ray/session_latest/metrics/grafana/grafana.ini`.\n\nYou can then start or restart the services with `brew services start grafana` and `brew services start prometheus`.\n\n***Loading Ray Grafana configurations with Docker Compose***\nIn the Ray container, the symbolic link \"/tmp/ray/session_latest/metrics\" points to the latest active Ray session. However, Docker does not support the mounting of symbolic links on shared volumes and you may fail to load the Grafana configuration files and default dashboards.\n\nTo fix this issue, employ an automated shell script for seamlessly transferring the necessary Grafana configurations and dashboards from the Ray container to a shared volume. To ensure a proper setup, mount the shared volume on the respective path for the container, which contains the recommended configurations and default dashboards to initiate Grafana servers.\n\n:::\n\n:::{tab-item} Using an existing Grafana server\n\nAfter your Grafana server is running, find the Ray-provided default Grafana dashboard JSON at `/tmp/ray/session_latest/metrics/grafana/dashboards/default_grafana_dashboard.json`. [Import this dashboard](https://grafana.com/docs/grafana/latest/dashboards/manage-dashboards/#import-a-dashboard) to your Grafana.\n\nIf Grafana reports that the datasource is not found, [add a datasource variable](https://grafana.com/docs/grafana/latest/dashboards/variables/add-template-variables/?pg=graf&plcmt=data-sources-prometheus-btn-1#add-a-data-source-variable). The datasource's name must be the same as value in the `RAY_PROMETHEUS_NAME` environment. By default, `RAY_PROMETHEUS_NAME` equals `Prometheus`.\n:::\n\n::::\n\n\n\n### Recommended: Use Ray Dashboard with embedded Grafana visualizations\n1. Follow the instructions above to set up Grafana with Ray-provided visualizations\n2. View {ref}`configuring and managing Ray Dashboard <embed-grafana-in-dashboard>` for how to embed Grafana visualizations into Dashboard\n3. View {ref}`Dashboard's metrics view<dash-metrics-view>` for how to inspect the metrics in Ray Dashboard.\n\n\n### Manual: Set up Grafana, or other tools like CloudWatch, Cloud Monitoring and Datadog from scratch\nRefer to the documentation of these tools for how to query and visualize the metrics.\n\n```{admonition} Tip\n:class: tip\nIf you need to write Prometheus queries manually, check out the Prometheus queries in Ray-provided Grafana dashboard JSON at `/tmp/ray/session_latest/metrics/grafana/dashboards/default_grafana_dashboard.json` for inspiration.\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c98c13a9-c3e9-4f80-8f56-03e077d74e7a": {"__data__": {"id_": "c98c13a9-c3e9-4f80-8f56-03e077d74e7a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/package-overview.rst", "file_name": "package-overview.rst", "text_hash": "8f9ee70000f14e90057d04c52452e66f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "1aef16167d64461a82d43b35a79ed3cbd34015b1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/package-overview.rst", "file_name": "package-overview.rst"}, "hash": "a6c16fedf977228c36e83886c01edf508c8f97131891f04ba9e7b2531cae8073"}}, "hash": "d3459fba057b43daf26b6dcf8b0ae26b130b724f64386f76132973480eed5add", "text": ".. _cluster-api-ref:\n\nRay Cluster Management API\n==========================\n\nThis section contains a reference for the cluster management API. If there is anything missing, please open an issue\non `Github`_.\n\n.. _`GitHub`: https://github.com/ray-project/ray/issues\n\n.. toctree::\n    :maxdepth: 2\n\n    cli.rst\n    running-applications/job-submission/jobs-package-ref.rst\n    running-applications/job-submission/cli.rst\n    running-applications/autoscaling/reference.rst", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c03654ba-8d53-42e2-b774-41685dc114bd": {"__data__": {"id_": "c03654ba-8d53-42e2-b774-41685dc114bd", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/autoscaling/reference.rst", "file_name": "reference.rst", "text_hash": "bc67ce5ff163b6d403b27e3fe1be755f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c0f44c732a1139656d5f79045c246bffbdbd1b99", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/autoscaling/reference.rst", "file_name": "reference.rst"}, "hash": "541d299b5445a1b050de5cd91fcec5222a01c96353e46772e96b28a0ee11f460"}}, "hash": "07225fb58a320b1717dd0221de63d8fe0e1857f3f7541f7dc522c701c1f27881", "text": ".. _ref-autoscaler-sdk:\n\nProgrammatic Cluster Scaling\n============================\n\n.. _ref-autoscaler-sdk-request-resources:\n\nray.autoscaler.sdk.request_resources\n------------------------------------\n\nWithin a Ray program, you can command the autoscaler to scale the cluster up to a desired size with ``request_resources()`` call. The cluster will immediately attempt to scale to accommodate the requested resources, bypassing normal upscaling speed constraints.\n\n.. autofunction:: ray.autoscaler.sdk.request_resources\n    :noindex:", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b94d5f22-bb97-4703-8f46-55eb7d7ae23c": {"__data__": {"id_": "b94d5f22-bb97-4703-8f46-55eb7d7ae23c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/index.md", "file_name": "index.md", "text_hash": "1e0a17ebfac43e718c634e3358bb810b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "0c49800e6448d4a3988965eb78514279036abe09", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/index.md", "file_name": "index.md"}, "hash": "6ee5a407dbfd86499b7be6496b8c2d9e9133ab4d75af155c2ccbd9e24615f4f6"}}, "hash": "d524b30b8ef4bd81d906f5b66862d154e1b4d7b5de8c46c0a69737fbfb9c883f", "text": "# Application guide\n\nThis section introduces the main differences in running a Ray application on your laptop vs on a Ray Cluster.\nTo get started, check out the [job submissions](jobs-quickstart) page.\n\n```{toctree}\n:maxdepth: '2'\n\njob-submission/index\nautoscaling/reference\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "56743901-aaf1-48f2-963c-ae5be28418e3": {"__data__": {"id_": "56743901-aaf1-48f2-963c-ae5be28418e3", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/cli.rst", "file_name": "cli.rst", "text_hash": "925aeb1fb40a73201435f779211df398"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "c8b3cca8117b420d18b7e9f2151c77227cd7d0c0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/cli.rst", "file_name": "cli.rst"}, "hash": "fec951bb5a44f95efcbb08bd8c2bc988ddb77c8f980e55c1ea8f3c029b9a5fae"}}, "hash": "029395a45fd156cf36c96328ed2b875384338fe7a1eba8bcac17e7041084db19", "text": ".. _ray-job-submission-cli-ref:\n\nRay Jobs CLI API Reference\n==========================\n\nThis section contains commands for :ref:`Ray Job Submission <jobs-quickstart>`.    \n\n.. _ray-job-submit-doc:\n\n.. click:: ray.dashboard.modules.job.cli:submit\n   :prog: ray job submit\n\n.. warning::\n\n    When using the CLI, do not wrap the entrypoint command in quotes.  For example, use \n    ``ray job submit --working_dir=\".\" -- python script.py`` instead of ``ray job submit --working_dir=\".\" -- \"python script.py\"``.\n    Otherwise you may encounter the error ``/bin/sh: 1: python script.py: not found``.\n\n.. warning::\n\n   You must provide the entrypoint command, ``python script.py``, last (after the ``--``), and any other arguments to `ray job submit` (e.g., ``--working_dir=\".\"``) must be provided before the  two hyphens (``--``).\n   For example, use ``ray job submit --working_dir=\".\" -- python script.py`` instead of ``ray job submit -- python script.py --working_dir=\".\"``.\n   This syntax supports the use of ``--`` to separate arguments to `ray job submit` from arguments to the entrypoint command.\n\n.. _ray-job-status-doc:\n\n.. click:: ray.dashboard.modules.job.cli:status\n   :prog: ray job status\n   :show-nested:\n\n.. _ray-job-stop-doc:\n\n.. click:: ray.dashboard.modules.job.cli:stop\n   :prog: ray job stop\n   :show-nested:\n\n.. _ray-job-logs-doc:\n\n.. click:: ray.dashboard.modules.job.cli:logs\n   :prog: ray job logs\n   :show-nested:\n\n.. _ray-job-list-doc:\n\n.. click:: ray.dashboard.modules.job.cli:list\n   :prog: ray job list\n   :show-nested:", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d1fb481-dd12-40fe-840c-8639e9dd6e0b": {"__data__": {"id_": "0d1fb481-dd12-40fe-840c-8639e9dd6e0b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/index.md", "file_name": "index.md", "text_hash": "37be36869f4b158017f70c683fe368b2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "bf3902efa57370e1725f0eecf7e0d43daf52904a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/index.md", "file_name": "index.md"}, "hash": "c03c50ce32cecda81a0118e7b1fd7017bdaa8cbc2fdc8dce6e003f4c49c257d5"}}, "hash": "b04a53f2379564f83074f586aeab0b90c29b2c904f0234d420eca605c1a5d361", "text": "(jobs-overview)=\n\n# Ray Jobs Overview\n\nOnce you have deployed a Ray cluster (on [VMs](vm-cluster-quick-start) or [Kubernetes](kuberay-quickstart)), you are ready to run a Ray application!\n![A diagram that shows three ways of running a job on a Ray cluster.](../../images/ray-job-diagram.svg \"Three ways of running a job on a Ray cluster.\")\n\n## Ray Jobs API\n\nThe recommended way to run a job on a Ray cluster is to use the *Ray Jobs API*, which consists of a CLI tool, Python SDK, and a REST API.\n\nThe Ray Jobs API allows you to submit locally developed applications to a remote Ray Cluster for execution.\nIt simplifies the experience of packaging, deploying, and managing a Ray application.\n\nA submission to the Ray Jobs API consists of:\n\n1. An entrypoint command, like `python my_script.py`, and\n2. A [runtime environment](runtime-environments), which specifies the application's file and package dependencies.\n\nA job can be submitted by a remote client that lives outside of the Ray Cluster.\nWe will show this workflow in the following user guides.\n\nAfter a job is submitted, it runs once to completion or failure, regardless of the original submitter's connectivity.\nRetries or different runs with different parameters should be handled by the submitter.\nJobs are bound to the lifetime of a Ray cluster, so if the cluster goes down, all running jobs on that cluster will be terminated.\n\nTo get started with the Ray Jobs API, check out the [quickstart](jobs-quickstart) guide, which walks you through the CLI tools for submitting and interacting with a Ray Job.\nThis is suitable for any client that can communicate over HTTP to the Ray Cluster.\nIf needed, the Ray Jobs API also provides APIs for [programmatic job submission](ray-job-sdk) and [job submission using REST](ray-job-rest-api).\n\n## Running Jobs Interactively\n\nIf you would like to run an application *interactively* and see the output in real time (for example, during development or debugging), you can:\n\n- (Recommended) Run your script directly on a cluster node (e.g. after SSHing into the node using [`ray attach`](ray-attach-doc)), or\n- (For Experts only) Use [Ray Client](ray-client-ref) to run a script from your local machine while maintaining a connection to the cluster.\n\nNote that jobs started in these ways are not managed by the Ray Jobs API, so the Ray Jobs API will not be able to see them or interact with them (with the exception of `ray job list` and `JobSubmissionClient.list_jobs()`).\n\n## Contents\n\n```{toctree}\n:maxdepth: '1'\n\nquickstart\nsdk\njobs-package-ref\ncli\nrest\nray-client\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3d4a7350-3bc8-45bf-aa7c-d6823fa711cb": {"__data__": {"id_": "3d4a7350-3bc8-45bf-aa7c-d6823fa711cb", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/jobs-package-ref.rst", "file_name": "jobs-package-ref.rst", "text_hash": "af7df164f14251fad2cc7d489562d3b5"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ed2d9709ef8c706dbbc05c18a339eb5e4a10c5d8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/jobs-package-ref.rst", "file_name": "jobs-package-ref.rst"}, "hash": "bfd96d1f1997d98a440a7031069813f78a78681d7fa965175c1f4467893f0c78"}}, "hash": "29cd6567a02d745168c453f772defda052d436a9ea6556c38d2963717f1c9c24", "text": ".. _ray-job-submission-sdk-ref:\n\nPython SDK API Reference\n========================\n\n.. currentmodule:: ray.job_submission\n\nFor an overview with examples see :ref:`Ray Jobs <jobs-overview>`.\n\nFor the CLI reference see :ref:`Ray Job Submission CLI Reference <ray-job-submission-cli-ref>`.\n\n.. _job-submission-client-ref:\n\nJobSubmissionClient\n-------------------\n\n.. autosummary::\n   :toctree: doc/\n\n   JobSubmissionClient\n\n.. autosummary::\n   :toctree: doc/\n\n   JobSubmissionClient.submit_job\n   JobSubmissionClient.stop_job\n   JobSubmissionClient.get_job_status\n   JobSubmissionClient.get_job_info\n   JobSubmissionClient.list_jobs\n   JobSubmissionClient.get_job_logs\n   JobSubmissionClient.tail_job_logs\n\n.. _job-status-ref:\n\nJobStatus\n---------\n\n.. autosummary::\n   :toctree: doc/\n\n   JobStatus\n\n.. _job-info-ref:\n\nJobInfo\n-------\n\n.. autosummary::\n   :toctree: doc/\n\n   JobInfo\n\n.. _job-details-ref:\n\nJobDetails\n----------\n\n.. autosummary::\n   :toctree: doc/\n\n   JobDetails\n\n.. _job-type-ref:\n\nJobType\n-------\n\n.. autosummary::\n   :toctree: doc/\n\n   JobType\n\n.. _driver-info-ref:\n\nDriverInfo\n----------\n\n.. autosummary::\n   :toctree: doc/\n\n   DriverInfo", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1be93d3d-31ce-4142-9e97-9da986722059": {"__data__": {"id_": "1be93d3d-31ce-4142-9e97-9da986722059", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "ba5eedc99d6d48f026fdfcfe6b225e99"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst"}, "hash": "f80e5acc7d16aaf5fc40ec0542cc6578b9f1b04dc2964a62a966a24a416bdf4b"}, "3": {"node_id": "776a58af-6583-4fd7-b8ba-7473f2dcd914", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "6e83cf39bed3a2cf89fa27ab7e922ac8"}, "hash": "32e538ddcdc74364a5dd89af1582ee3ade2d199a17e6e8e57eb490fa7cf957ff"}}, "hash": "466262dfe595d9fc9db6ef43b138799d97f09f46b18470136d191b950bbcf139", "text": ".. _jobs-quickstart:\n\n=================================\nQuickstart Using the Ray Jobs CLI\n=================================\n\n\nIn this guide, we will walk through the Ray Jobs CLI commands available for submitting and interacting with a Ray Job.\n\nTo use the Jobs API programmatically via a Python SDK instead of a CLI, see :ref:`ray-job-sdk`.\n\nSetup\n-----\n\nRay Jobs is available in versions 1.9+ and requires a full installation of Ray. You can do this by running:\n\n.. code-block:: shell\n\n    pip install \"ray[default]\"\n\nSee the :ref:`installation guide <installation>` for more details on installing Ray.\n\nTo submit a job, we also need to be able to send HTTP requests to a Ray Cluster.\nFor convenience, this guide will assume that you are using a local Ray Cluster, which we can start by running:\n\n.. code-block:: shell\n\n    ray start --head\n    # ...\n    # 2022-08-10 09:54:57,664   INFO services.py:1476 -- View the Ray dashboard at http://127.0.0.1:8265\n    # ...\n\nThis will create a Ray head node on our local machine that we can use for development purposes.\nNote the Ray Dashboard URL that is printed when starting or connecting to a Ray Cluster; we will use this URL later to submit a job.\nFor more details on production deployment scenarios, check out the guides for deploying Ray on :ref:`VMs <vm-cluster-quick-start>` and :ref:`Kubernetes <kuberay-quickstart>`.\n\n\nSubmitting a job\n----------------\n\nLet's start with a sample script that can be run locally. The following script uses Ray APIs to submit a task and print its return value:\n\n.. code-block:: python\n\n    # script.py\n    import ray\n\n    @ray.remote\n    def hello_world():\n        return \"hello world\"\n\n    # Automatically connect to the running Ray cluster.\n    ray.init()\n    print(ray.get(hello_world.remote()))\n\nCreate an empty working directory with the above Python script inside a file named ``script.py``. \n\n.. code-block:: bash\n\n  | your_working_directory\n  | \u251c\u2500\u2500 script.py\n\nNext, we will find the HTTP address of the Ray Cluster to which we can submit a job request.\nJobs are submitted to the same address used by the **Ray Dashboard**.\nBy default, this uses port 8265.\n\nIf you are using a local Ray Cluster (``ray start --head``), you can connect directly at ``http://127.0.0.1:8265``.\nIf you are using a Ray Cluster started on VMs or Kubernetes, you can follow the instructions there for setting up network access from a client. See :ref:`Using a Remote Cluster <jobs-remote-cluster>` for tips.\n\n\nTo tell the Ray Jobs CLI how to find your Ray Cluster, we will pass the Ray Dashboard address.This can be done by setting the ``RAY_ADDRESS`` environment variable:\n\n.. code-block:: bash\n\n    $ export RAY_ADDRESS=\"http://127.0.0.1:8265\"\n\nAlternatively, you can also pass the ``--address=http://127.0.0.1:8265`` flag explicitly to each Ray Jobs CLI command, or prepend each command with ``RAY_ADDRESS=http://127.0.0.1:8265``.Additionally, if you wish to pass headers per HTTP request to the Cluster, then utilize the `RAY_JOB_HEADERS` environment variable.` Must be in JSON form... code-block:: bash\n    \n    $ export RAY_JOB_HEADERS='{\"KEY\": \"VALUE\"}'\n\nTo submit the job, we use ``ray job submit``.Make sure to specify the path to the working directory in the ``--working-dir`` argument.(For local clusters this is not strictly necessary, but for remote clusters this is required in order to upload the working directory to the cluster.)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "776a58af-6583-4fd7-b8ba-7473f2dcd914": {"__data__": {"id_": "776a58af-6583-4fd7-b8ba-7473f2dcd914", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "6e83cf39bed3a2cf89fa27ab7e922ac8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst"}, "hash": "f80e5acc7d16aaf5fc40ec0542cc6578b9f1b04dc2964a62a966a24a416bdf4b"}, "2": {"node_id": "1be93d3d-31ce-4142-9e97-9da986722059", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "ba5eedc99d6d48f026fdfcfe6b225e99"}, "hash": "466262dfe595d9fc9db6ef43b138799d97f09f46b18470136d191b950bbcf139"}, "3": {"node_id": "92f1bffb-3509-40b9-9700-36a9b5d55f0b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "88f3619d2ba2fd311d970073daa61036"}, "hash": "5863901e5e97d965e7dc727dbb977882afeddc68327f67c5ebcd4a6c9cac280a"}}, "hash": "32e538ddcdc74364a5dd89af1582ee3ade2d199a17e6e8e57eb490fa7cf957ff", "text": ".. code-block:: bash\n\n    $ ray job submit --working-dir your_working_directory -- python script.py \n\n    # Job submission server address: http://127.0.0.1:8265\n\n    # -------------------------------------------------------\n    # Job 'raysubmit_inB2ViQuE29aZRJ5' submitted successfully\n    # -------------------------------------------------------\n\n    # Next steps\n    #   Query the logs of the job:\n    #     ray job logs raysubmit_inB2ViQuE29aZRJ5\n    #   Query the status of the job:\n    #     ray job status raysubmit_inB2ViQuE29aZRJ5\n    #   Request the job to be stopped:\n    #     ray job stop raysubmit_inB2ViQuE29aZRJ5\n\n    # Tailing logs until the job exits (disable with --no-wait):\n    # hello world\n\n    # ------------------------------------------\n    # Job 'raysubmit_inB2ViQuE29aZRJ5' succeeded\n    # ------------------------------------------\n\nThis command will run the script on the Ray Cluster and wait until the job has finished.Note that it also streams the stdout of the job back to the client (``hello world`` in this case).Ray will also make the contents of the directory passed as `--working-dir` available to the Ray job by downloading the directory to all nodes in your cluster... note::\n\n    The double dash (`--`) separates the arguments for the entrypoint command (e.g.`python script.py --arg1=val1`) from the arguments to `ray job submit`.Interacting with Long-running Jobs\n----------------------------------\n\nFor long-running applications, it is not desirable to require the client to wait for the job to finish.To do this, we can pass the ``--no-wait`` flag to ``ray job submit`` and use the other CLI commands to check on the job's status.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "92f1bffb-3509-40b9-9700-36a9b5d55f0b": {"__data__": {"id_": "92f1bffb-3509-40b9-9700-36a9b5d55f0b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "88f3619d2ba2fd311d970073daa61036"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst"}, "hash": "f80e5acc7d16aaf5fc40ec0542cc6578b9f1b04dc2964a62a966a24a416bdf4b"}, "2": {"node_id": "776a58af-6583-4fd7-b8ba-7473f2dcd914", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "6e83cf39bed3a2cf89fa27ab7e922ac8"}, "hash": "32e538ddcdc74364a5dd89af1582ee3ade2d199a17e6e8e57eb490fa7cf957ff"}, "3": {"node_id": "96961802-1273-4170-8324-f2a2307c08b2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "e3404d8cbeac86d4a791fab9d821c514"}, "hash": "e9ccec6bddb3116da3677bb17bb2679668c0f9e6ad27c29cabe455a6db9337d5"}}, "hash": "5863901e5e97d965e7dc727dbb977882afeddc68327f67c5ebcd4a6c9cac280a", "text": "Let's try this out with a modified script that submits a task every second in an infinite loop:\n\n.. code-block:: python\n\n    # script.py\n    import ray\n    import time\n\n    @ray.remote\n    def hello_world():\n        return \"hello world\"\n\n    ray.init()\n    while True:\n        print(ray.get(hello_world.remote()))\n        time.sleep(1)\n\nNow let's submit the job:\n\n.. code-block:: shell\n\n\t$ ray job submit --no-wait --working-dir your_working_directory -- python script.py \n\t# Job submission server address: http://127.0.0.1:8265\n\n\t# -------------------------------------------------------\n\t# Job 'raysubmit_tUAuCKubPAEXh6CW' submitted successfully\n\t# -------------------------------------------------------\n\n\t# Next steps\n\t#   Query the logs of the job:\n\t# \tray job logs raysubmit_tUAuCKubPAEXh6CW\n\t#   Query the status of the job:\n\t# \tray job status raysubmit_tUAuCKubPAEXh6CW\n\t#   Request the job to be stopped:\n\t# \tray job stop raysubmit_tUAuCKubPAEXh6CW\n\nWe can later get the stdout using the provided ``ray job logs`` command:\n\n.. code-block:: shell\n\n    $ ray job logs raysubmit_tUAuCKubPAEXh6CW\n    # Job submission server address: http://127.0.0.1:8265\n    # hello world\n    # hello world\n    # hello world\n    # hello world\n    # hello world\n\nAnd the current status of the job using ``ray job status``:\n\n.. code-block:: shell\n\n    $ ray job status raysubmit_tUAuCKubPAEXh6CW\n    # Job submission server address: http://127.0.0.1:8265\n    # Status for job 'raysubmit_tUAuCKubPAEXh6CW': RUNNING\n    # Status message: Job is currently running.Finally, if we want to cancel the job, we can use ``ray job stop``:\n\n.. code-block:: shell\n\n    $ ray job stop raysubmit_tUAuCKubPAEXh6CW\n    # Job submission server address: http://127.0.0.1:8265\n    # Attempting to stop job raysubmit_tUAuCKubPAEXh6CW\n    # Waiting for job 'raysubmit_tUAuCKubPAEXh6CW' to exit (disable with --no-wait):\n    # Job 'raysubmit_tUAuCKubPAEXh6CW' was stopped\n\n    $ ray job status raysubmit_tUAuCKubPAEXh6CW\n    # Job submission server address: http://127.0.0.1:8265\n    # Job 'raysubmit_tUAuCKubPAEXh6CW' was stopped", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96961802-1273-4170-8324-f2a2307c08b2": {"__data__": {"id_": "96961802-1273-4170-8324-f2a2307c08b2", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "e3404d8cbeac86d4a791fab9d821c514"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst"}, "hash": "f80e5acc7d16aaf5fc40ec0542cc6578b9f1b04dc2964a62a966a24a416bdf4b"}, "2": {"node_id": "92f1bffb-3509-40b9-9700-36a9b5d55f0b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "88f3619d2ba2fd311d970073daa61036"}, "hash": "5863901e5e97d965e7dc727dbb977882afeddc68327f67c5ebcd4a6c9cac280a"}, "3": {"node_id": "f463cba3-8ec0-4629-917d-495d2f4192b7", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "5a844be207f01b6fdee7ae20ca734980"}, "hash": "b9ba660fa1f7ca3cfe23758dabcaae2d6e87b1ab84ac7411a3d7f0bd6c158338"}}, "hash": "e9ccec6bddb3116da3677bb17bb2679668c0f9e6ad27c29cabe455a6db9337d5", "text": ".. _jobs-remote-cluster:\n\nUsing a Remote Cluster\n----------------------\n\nThe example above was for a local Ray cluster.  When connecting to a `remote` cluster, you need to be able to access the dashboard port of the cluster over HTTP.\n\nOne way to do this is to port forward ``127.0.0.1:8265`` on your local machine to ``127.0.0.1:8265`` on the head node. If you started your remote cluster with the :ref:`Ray Cluster Launcher <cluster-index>`, then the port forwarding can be set up automatically using the ``ray dashboard`` command (see :ref:`monitor-cluster` for details).\n\nTo use this, run the following command on your local machine, where ``cluster.yaml`` is the configuration file you used to launch your cluster:\n\n.. code-block:: bash\n\n    ray dashboard cluster.yaml\n\nOnce this is running, check that you can view the Ray Dashboard in your local browser at ``http://127.0.0.1:8265``.  \nOnce you have verified this and you have set the environment variable ``RAY_ADDRESS`` to ``\"http://127.0.0.1:8265\"``, you will be able to use the Jobs CLI on your local machine as in the example above to interact with your remote Ray cluster.\n\nUsing the CLI on Kubernetes\n^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nThe instructions above still apply, but you can achieve the dashboard port forwarding using ``kubectl port-forward``:\nhttps://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\n\nAlternatively, you can set up Ingress to the dashboard port of the cluster over HTTP: https://kubernetes.io/docs/concepts/services-networking/ingress/\n\n\nDependency Management\n---------------------\n\nTo run a distributed application, we need to make sure that all workers run in the same environment.This can be challenging if multiple applications in the same Ray Cluster have different and conflicting dependencies.To avoid dependency conflicts, Ray provides a mechanism called :ref:`runtime environments <runtime-environments>`.Runtime environments allow an application to override the default environment on the Ray Cluster and run in an isolated environment, similar to virtual environments in single-node Python.Dependencies can include both files and Python packages.The Ray Jobs API provides an option to specify the runtime environment when submitting a job.On the Ray Cluster, Ray will then install the runtime environment across the workers and ensure that tasks in that job run in the same environment.To see how this works, we'll use a Python script that prints the current version of the ``requests`` module in a Ray task... code-block:: python\n\n    import ray\n    import requests\n\n    @ray.remote\n    def get_requests_version():\n        return requests.__version__\n\n    # Note: No need to specify the runtime_env in ray.init() in the driver script.ray.init()\n    print(\"requests version:\", ray.get(get_requests_version.remote()))\n\nFirst, let's submit this job using the default environment.This is the environment in which the Ray Cluster was started.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f463cba3-8ec0-4629-917d-495d2f4192b7": {"__data__": {"id_": "f463cba3-8ec0-4629-917d-495d2f4192b7", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "5a844be207f01b6fdee7ae20ca734980"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst"}, "hash": "f80e5acc7d16aaf5fc40ec0542cc6578b9f1b04dc2964a62a966a24a416bdf4b"}, "2": {"node_id": "96961802-1273-4170-8324-f2a2307c08b2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "e3404d8cbeac86d4a791fab9d821c514"}, "hash": "e9ccec6bddb3116da3677bb17bb2679668c0f9e6ad27c29cabe455a6db9337d5"}}, "hash": "b9ba660fa1f7ca3cfe23758dabcaae2d6e87b1ab84ac7411a3d7f0bd6c158338", "text": ".. code-block:: bash\n\n    $ ray job submit -- python script.py \n    # Job submission server address: http://127.0.0.1:8265\n    # \n    # -------------------------------------------------------\n    # Job 'raysubmit_seQk3L4nYWcUBwXD' submitted successfully\n    # -------------------------------------------------------\n    # \n    # Next steps\n    #   Query the logs of the job:\n    #     ray job logs raysubmit_seQk3L4nYWcUBwXD\n    #   Query the status of the job:\n    #     ray job status raysubmit_seQk3L4nYWcUBwXD\n    #   Request the job to be stopped:\n    #     ray job stop raysubmit_seQk3L4nYWcUBwXD\n    # \n    # Tailing logs until the job exits (disable with --no-wait):\n    # requests version: 2.28.1\n    # \n    # ------------------------------------------\n    # Job 'raysubmit_seQk3L4nYWcUBwXD' succeeded\n    # ------------------------------------------\n\nNow let's try it with a runtime environment that pins the version of the ``requests`` module:\n\n.. code-block:: bash\n\n    $ ray job submit --runtime-env-json='{\"pip\": [\"requests==2.26.0\"]}' -- python script.py \n    # Job submission server address: http://127.0.0.1:8265\n\n    # -------------------------------------------------------\n    # Job 'raysubmit_vGGV4MiP9rYkYUnb' submitted successfully\n    # -------------------------------------------------------\n\n    # Next steps\n    #   Query the logs of the job:\n    #     ray job logs raysubmit_vGGV4MiP9rYkYUnb\n    #   Query the status of the job:\n    #     ray job status raysubmit_vGGV4MiP9rYkYUnb\n    #   Request the job to be stopped:\n    #     ray job stop raysubmit_vGGV4MiP9rYkYUnb\n\n    # Tailing logs until the job exits (disable with --no-wait):\n    # requests version: 2.26.0\n\n    # ------------------------------------------\n    # Job 'raysubmit_vGGV4MiP9rYkYUnb' succeeded\n    # ------------------------------------------\n\n.. warning::\n\n    When using the Ray Jobs API, the runtime environment should be specified only in the Jobs API (e.g.in `ray job submit --runtime-env=...` or `JobSubmissionClient.submit_job(runtime_env=...)`), not via `ray.init(runtime_env=...)` in the driver script.- The full API reference for the Ray Jobs CLI can be found :ref:`here <ray-job-submission-cli-ref>`.- The full API reference for the Ray Jobs SDK can be found :ref:`here <ray-job-submission-sdk-ref>`.- For more information, check out the guides for :ref:`programmatic job submission <ray-job-sdk>` and :ref:`job submission using REST <ray-job-rest-api>`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ae1a0b32-23f8-40e4-a8a5-8126a3924cae": {"__data__": {"id_": "ae1a0b32-23f8-40e4-a8a5-8126a3924cae", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "e8a3776cfd48b9888da832dad839677b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a153e682f4476fea19423ec384e7d661ed5e5d26", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst"}, "hash": "35f10e2279d5e61f8eee15c6cb3d2088a57759adef5bf943be096fb11028e717"}, "3": {"node_id": "30ee1aa7-9c84-485c-b94c-a79f2660d2ca", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "791cd550c06ae04a45d7df2529674481"}, "hash": "173e59a62f0ecfba1e629a53ec5327e7dc4d71e243f90ea3750db12e951b4758"}}, "hash": "ec81bd2a90a909030596f0275d03792d83b498e8450aba7c042a066008ad8832", "text": ".. _ray-client-ref:\n\nRay Client\n==========\n\n.. warning::\n   Ray Client requires pip package `ray[client]`. If you installed the minimal Ray (e.g. `pip install ray`), please reinstall by executing `pip install ray[client]`.\n\n**What is the Ray Client?**\n\nThe Ray Client is an API that connects a Python script to a **remote** Ray cluster. Effectively, it allows you to leverage a remote Ray cluster just like you would with Ray running on your local machine.\n\nBy changing ``ray.init()`` to ``ray.init(\"ray://<head_node_host>:<port>\")``, you can connect from your laptop (or anywhere) directly to a remote cluster and scale-out your Ray code, while maintaining the ability to develop interactively in a Python shell. **This will only work with Ray 1.5+.**\n\n\n.. code-block:: python\n\n   # You can run this code outside of the Ray cluster!\n   import ray\n\n   # Starting the Ray client. This connects to a remote Ray cluster.\n   ray.init(\"ray://<head_node_host>:10001\")\n\n   # Normal Ray code follows\n   @ray.remote\n   def do_work(x):\n       return x ** x\n\n   do_work.remote(2)\n   #....\n\n\nWhen to use Ray Client\n----------------------\n\n.. note::\n   Ray Client has architectural limitations and may not work as expected when using Ray for ML workloads (like Ray Tune or Ray Train).Use :ref:`Ray Jobs API<jobs-overview>` for interactive development on ML projects.Ray Client can be used when you want to connect an interactive Python shell to a **remote** cluster.* Use ``ray.init(\"ray://<head_node_host>:10001\")`` (Ray Client) if you've set up a remote cluster at ``<head_node_host>`` and you want to do interactive work.This will connect your shell to the cluster.See the section on :ref:`using Ray Client<how-do-you-use-the-ray-client>` for more details on setting up your cluster.* Use ``ray.init()`` (non-client connection, no address specified) if you're developing locally and want to connect to an existing cluster (i.e.``ray start --head`` has already been run), or automatically create a local cluster and attach directly to it.This can also be used for :ref:`Ray Job <jobs-overview>` submission.Ray Client is useful for developing interactively in a local Python shell.However, it requires a stable connection to the remote cluster and will terminate the workload if the connection is lost for :ref:`more than 30 seconds <client-disconnections>`.If you have a long running workload that you want to run on your cluster, we recommend using :ref:`Ray Jobs <jobs-overview>` instead.Client arguments\n----------------\n\nRay Client is used when the address passed into ``ray.init`` is prefixed with ``ray://``.Besides the address, Client mode currently accepts two other arguments:\n\n- ``namespace`` (optional): Sets the namespace for the session.- ``runtime_env`` (optional): Sets the :ref:`runtime environment <runtime-environments>` for the session, allowing you to dynamically specify environment variables, packages, local files, and more... code-block:: python\n\n   # Connects to an existing cluster at 1.2.3.4 listening on port 10001, using\n   # the namespace \"my_namespace\".The Ray workers will run inside a cluster-side\n   # copy of the local directory \"files/my_project\", in a Python environment with\n   # `toolz` and `requests` installed.ray.init(\n       \"ray://1.2.3.4:10001\",\n       namespace=\"my_namespace\",\n       runtime_env={\"working_dir\": \"files/my_project\", \"pip\": [\"toolz\", \"requests\"]},\n   )\n   #....\n\n.. _how-do-you-use-the-ray-client:\n\nHow do you use the Ray Client?------------------------------\n\nStep 1: Set up your Ray cluster\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you have a running Ray cluster (version >= 1.5), Ray Client server is likely already running on port ``10001`` of the head node by default.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "30ee1aa7-9c84-485c-b94c-a79f2660d2ca": {"__data__": {"id_": "30ee1aa7-9c84-485c-b94c-a79f2660d2ca", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "791cd550c06ae04a45d7df2529674481"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a153e682f4476fea19423ec384e7d661ed5e5d26", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst"}, "hash": "35f10e2279d5e61f8eee15c6cb3d2088a57759adef5bf943be096fb11028e717"}, "2": {"node_id": "ae1a0b32-23f8-40e4-a8a5-8126a3924cae", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "e8a3776cfd48b9888da832dad839677b"}, "hash": "ec81bd2a90a909030596f0275d03792d83b498e8450aba7c042a066008ad8832"}, "3": {"node_id": "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "a7880235dacce4e6915ad0a31542e72c"}, "hash": "31116d896ced8871fe3c874e3cf6a08d8ec62eac1580c7bb4ea483fb572246fc"}}, "hash": "173e59a62f0ecfba1e629a53ec5327e7dc4d71e243f90ea3750db12e951b4758", "text": "Otherwise, you'll want to create a Ray cluster.To start a Ray cluster locally, you can run\n\n.. code-block:: bash\n\n   ray start --head\n\nTo start a Ray cluster remotely, you can follow the directions in :ref:`vm-cluster-quick-start`.If necessary, you can modify the Ray Client server port to be other than ``10001``, by specifying ``--ray-client-server-port=...`` to the ``ray start`` :ref:`command <ray-start-doc>`.Step 2: Check ports\n~~~~~~~~~~~~~~~~~~~\n\nEnsure that the Ray Client port on the head node is reachable from your local machine.This means opening that port up by configuring security groups or other access controls (on  `EC2 <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/authorizing-access-to-an-instance.html>`_)\nor proxying from your local machine to the cluster (on `K8s <https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/#forward-a-local-port-to-a-port-on-the-pod>`_)... tab-set::\n\n    .. tab-item:: AWS\n\n        With the Ray cluster launcher, you can configure the security group\n        to allow inbound access by defining :ref:`cluster-configuration-security-group`\n        in your `cluster.yaml`... code-block:: yaml\n\n            # An unique identifier for the head node and workers of this cluster.cluster_name: minimal_security_group\n\n            # Cloud-provider specific configuration.provider:\n                type: aws\n                region: us-west-2\n                security_group:\n                    GroupName: ray_client_security_group\n                    IpPermissions:\n                          - FromPort: 10001\n                            ToPort: 10001\n                            IpProtocol: TCP\n                            IpRanges:\n                                # This will enable inbound access from ALL IPv4 addresses.- CidrIp: 0.0.0.0/0\n\nStep 3: Run Ray code\n~~~~~~~~~~~~~~~~~~~~\n\nNow, connect to the Ray Cluster with the following and then use Ray like you normally would:\n\n..\n.. code-block:: python\n\n   import ray\n\n   # replace with the appropriate host and port\n   ray.init(\"ray://<head_node_host>:10001\")\n\n   # Normal Ray code follows\n   @ray.remote\n   def do_work(x):\n       return x ** x\n\n   do_work.remote(2)\n\n   #....Alternative Approach: SSH Port Forwarding\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAs an alternative to configuring inbound traffic rules, you can also set up\nRay Client via port forwarding.While this approach does require an open SSH\nconnection, it can be useful in a test environment where the\n``head_node_host`` often changes.First, open up an SSH connection with your Ray cluster and forward the\nlistening port (``10001``)... code-block:: bash\n\n  $ ray up cluster.yaml\n  $ ray attach cluster.yaml -p 10001\n\nThen, you can connect to the Ray cluster **from another terminal** using  ``localhost`` as the\n``head_node_host``.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0": {"__data__": {"id_": "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "a7880235dacce4e6915ad0a31542e72c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a153e682f4476fea19423ec384e7d661ed5e5d26", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst"}, "hash": "35f10e2279d5e61f8eee15c6cb3d2088a57759adef5bf943be096fb11028e717"}, "2": {"node_id": "30ee1aa7-9c84-485c-b94c-a79f2660d2ca", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "791cd550c06ae04a45d7df2529674481"}, "hash": "173e59a62f0ecfba1e629a53ec5327e7dc4d71e243f90ea3750db12e951b4758"}, "3": {"node_id": "4c475f09-a5fe-416f-8e3f-480d05890147", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "f500cd885a3ade9df8dd8f6b7e48aad2"}, "hash": "3233e256ed6a3913fd31ee1d8087f9ed188a50c0218fe4301d231bd369e7c770"}}, "hash": "31116d896ced8871fe3c874e3cf6a08d8ec62eac1580c7bb4ea483fb572246fc", "text": ".. code-block:: python\n\n   import ray\n\n   # This will connect to the cluster via the open SSH session.ray.init(\"ray://localhost:10001\")\n\n   # Normal Ray code follows\n   @ray.remote\n   def do_work(x):\n       return x ** x\n\n   do_work.remote(2)\n\n   #....\n\nConnect to multiple Ray clusters (Experimental)\n-----------------------------------------------\n\nRay Client allows connecting to multiple Ray clusters in one Python process.To do this, just pass ``allow_multiple=True`` to ``ray.init``:\n\n.. code-block:: python\n\n    import ray\n    # Create a default client.ray.init(\"ray://<head_node_host_cluster>:10001\")\n\n    # Connect to other clusters.cli1 = ray.init(\"ray://<head_node_host_cluster_1>:10001\", allow_multiple=True)\n    cli2 = ray.init(\"ray://<head_node_host_cluster_2>:10001\", allow_multiple=True)\n\n    # Data is put into the default cluster.obj = ray.put(\"obj\")\n\n    with cli1:\n        obj1 = ray.put(\"obj1\")\n\n    with cli2:\n        obj2 = ray.put(\"obj2\")\n\n    with cli1:\n        assert ray.get(obj1) == \"obj1\"\n        try:\n            ray.get(obj2)  # Cross-cluster ops not allowed.except:\n            print(\"Failed to get object which doesn't belong to this cluster\")\n\n    with cli2:\n        assert ray.get(obj2) == \"obj2\"\n        try:\n            ray.get(obj1)  # Cross-cluster ops not allowed.except:\n            print(\"Failed to get object which doesn't belong to this cluster\")\n    assert \"obj\" == ray.get(obj)\n    cli1.disconnect()\n    cli2.disconnect()\n\n\nWhen using Ray multi-client, there are some different behaviors to pay attention to:\n\n* The client won't be disconnected automatically. Call ``disconnect`` explicitly to close the connection.\n* Object references can only be used by the client from which it was obtained.\n* ``ray.init`` without ``allow_multiple`` will create a default global Ray client.\n\nThings to know\n--------------\n\n.. _client-disconnections:\n\nClient disconnections\n~~~~~~~~~~~~~~~~~~~~~\n\nWhen the client disconnects, any object or actor references held by the server on behalf of the client are dropped, as if directly disconnecting from the cluster.\n\nIf the client disconnects unexpectedly, i.e. due to a network failure, the client will attempt to reconnect to the server for 30 seconds before all of the references are dropped. You can increase this time by setting the environment variable ``RAY_CLIENT_RECONNECT_GRACE_PERIOD=N``, where ``N`` is the number of seconds that the client should spend trying to reconnect before giving up.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4c475f09-a5fe-416f-8e3f-480d05890147": {"__data__": {"id_": "4c475f09-a5fe-416f-8e3f-480d05890147", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "f500cd885a3ade9df8dd8f6b7e48aad2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a153e682f4476fea19423ec384e7d661ed5e5d26", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst"}, "hash": "35f10e2279d5e61f8eee15c6cb3d2088a57759adef5bf943be096fb11028e717"}, "2": {"node_id": "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "a7880235dacce4e6915ad0a31542e72c"}, "hash": "31116d896ced8871fe3c874e3cf6a08d8ec62eac1580c7bb4ea483fb572246fc"}}, "hash": "3233e256ed6a3913fd31ee1d8087f9ed188a50c0218fe4301d231bd369e7c770", "text": "Versioning requirements\n~~~~~~~~~~~~~~~~~~~~~~~\n\nGenerally, the client Ray version must match the server Ray version. An error will be raised if an incompatible version is used.\n\nSimilarly, the minor Python (e.g., 3.6 vs 3.7) must match between the client and server. An error will be raised if this is not the case.\n\nStarting a connection on older Ray versions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you encounter ``socket.gaierror: [Errno -2] Name or service not known`` when using ``ray.init(\"ray://...\")`` then you may be on a version of Ray prior to 1.5 that does not support starting client connections through ``ray.init``.\n\nConnection through the Ingress\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you encounter the following error message when connecting to the ``Ray Cluster`` using an ``Ingress``,  it may be caused by the Ingress's configuration.\n\n..\n.. code-block:: python\n\n   grpc._channel._MultiThreadedRendezvous: <_MultiThreadedRendezvous of RPC that terminated with:\n       status = StatusCode.INVALID_ARGUMENT\n       details = \"\"\n       debug_error_string = \"{\"created\":\"@1628668820.164591000\",\"description\":\"Error received from peer ipv4:10.233.120.107:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"\",\"grpc_status\":3}\"\n   >\n   Got Error from logger channel -- shutting down: <_MultiThreadedRendezvous of RPC that terminated with:\n       status = StatusCode.INVALID_ARGUMENT\n       details = \"\"\n       debug_error_string = \"{\"created\":\"@1628668820.164713000\",\"description\":\"Error received from peer ipv4:10.233.120.107:443\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1062,\"grpc_message\":\"\",\"grpc_status\":3}\"\n   >\n\n\nIf you are using the ``nginx-ingress-controller``, you may be able to resolve the issue by adding the following Ingress configuration.\n\n\n.. code-block:: yaml\n\n   metadata:\n     annotations:\n        nginx.ingress.kubernetes.io/server-snippet: |\n          underscores_in_headers on;\n          ignore_invalid_headers on;\n\nRay client logs\n~~~~~~~~~~~~~~~\n\nRay client logs can be found at ``/tmp/ray/session_latest/logs`` on the head node.\n\nUploads\n~~~~~~~\n\nIf a ``working_dir`` is specified in the runtime env, when running ``ray.init()`` the Ray client will upload the ``working_dir`` on the laptop to ``/tmp/ray/session_latest/runtime_resources/_ray_pkg_<hash of directory contents>``.\n\nRay workers are started in the ``/tmp/ray/session_latest/runtime_resources/_ray_pkg_<hash of directory contents>`` directory on the cluster. This means that relative paths in the remote tasks and actors in the code will work on the laptop and on the cluster without any code changes. For example, if the ``working_dir`` on the laptop contains ``data.txt`` and ``run.py``, inside the remote task definitions in ``run.py`` one can just use the relative path ``\"data.txt\"``. Then ``python run.py`` will work on my laptop, and also on the cluster. As a side note, since relative paths can be used in the code, the absolute path is only useful for debugging purposes.\n\nTroubleshooting\n---------------\n\nError: Attempted to reconnect a session that has already been cleaned up \n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThis error happens when Ray Client reconnects to a head node that does not recognize the client. This can happen if the head node restarts unexpectedly and loses state. On Kubernetes, this can happen if the head pod restarts after being evicted or crashing.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "daa65b7d-3353-47db-a084-680ed2e142e7": {"__data__": {"id_": "daa65b7d-3353-47db-a084-680ed2e142e7", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/rest.rst", "file_name": "rest.rst", "text_hash": "25eda7ce32fb311aa83f235883cd741f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "abc8b5a830cd6e67653f12e8402722f1f74b1090", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/rest.rst", "file_name": "rest.rst"}, "hash": "468bd65934600236a5c8fe5b936124af7ba0da3c4e239bdf054bcb5bc55cb248"}}, "hash": "468bd65934600236a5c8fe5b936124af7ba0da3c4e239bdf054bcb5bc55cb248", "text": ".. _ray-job-rest-api:\n\nRay Jobs REST API\n^^^^^^^^^^^^^^^^^\n\nUnder the hood, both the Python SDK and the CLI make HTTP calls to the job server running on the Ray head node. You can also directly send requests to the corresponding endpoints via HTTP if needed:\n\nContinue on for examples, or jump to the :ref:`OpenAPI specification <ray-job-rest-api-spec>`.\n\n**Submit Job**\n\n.. code-block:: python\n\n    import requests\n    import json\n    import time\n\n    resp = requests.post(\n        \"http://127.0.0.1:8265/api/jobs/\",\n        json={\n            \"entrypoint\": \"echo hello\",\n            \"runtime_env\": {},\n            \"job_id\": None,\n            \"metadata\": {\"job_submission_id\": \"123\"}\n        }\n    )\n    rst = json.loads(resp.text)\n    job_id = rst[\"job_id\"]\n    print(job_id)\n\n**Query and poll for Job status**\n\n.. code-block:: python\n\n    start = time.time()\n    while time.time() - start <= 10:\n        resp = requests.get(\n            f\"http://127.0.0.1:8265/api/jobs/{job_id}\"\n        )\n        rst = json.loads(resp.text)\n        status = rst[\"status\"]\n        print(f\"status: {status}\")\n        if status in {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED}:\n            break\n        time.sleep(1)\n\n**Query for logs**\n\n.. code-block:: python\n\n    resp = requests.get(\n        f\"http://127.0.0.1:8265/api/jobs/{job_id}/logs\"\n    )\n    rst = json.loads(resp.text)\n    logs = rst[\"logs\"]\n    print(logs)\n\n**List all jobs**\n\n.. code-block:: python\n\n    resp = requests.get(\n        \"http://127.0.0.1:8265/api/jobs/\"\n    )\n    print(resp.json())\n    # {\"job_id\": {\"metadata\": ..., \"status\": ..., \"message\": ...}, ...}\n\n.. _ray-job-rest-api-spec:\n\nOpenAPI Documentation (Beta)\n----------------------------\n\nWe provide an OpenAPI specification for the Ray Job API. You can use this to generate client libraries for other languages.\n\nView the `Ray Jobs REST API OpenAPI documentation <api.html>`_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "fafd028b-0881-4c69-8af9-4617374e8dc7": {"__data__": {"id_": "fafd028b-0881-4c69-8af9-4617374e8dc7", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "865eb28fbf5acb2ed02e8c07664e0b37"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst"}, "hash": "5a59558d31b6cf7498b84e1a98c7777bca5df03f4b205200f7f9d53961b03788"}, "3": {"node_id": "b508c27b-7c3c-4e8a-99fe-1e73f2976d33", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "d773f6c6db6b0716f9dfa84341babd5e"}, "hash": "b6c15177fc64d62f6df5e3b41cdef000e0dbc632dd59db1108e85d80a48498c4"}}, "hash": "ff3caf2693b5c1d35fca555c796e540a0d081cfdb4b4f5a5a869711a1adc526d", "text": ".. _ray-job-sdk:\n\nPython SDK Overview\n^^^^^^^^^^^^^^^^^^^\n\nThe Ray Jobs Python SDK is the recommended way to submit jobs programmatically.Jump to the :ref:`API Reference <ray-job-submission-sdk-ref>`, or continue reading for a quick overview.Setup\n-----\n\nRay Jobs is available in versions 1.9+ and requires a full installation of Ray.You can do this by running:\n\n.. code-block:: shell\n\n    pip install \"ray[default]\"\n\nSee the :ref:`installation guide <installation>` for more details on installing Ray.To run a Ray Job, we also need to be able to send HTTP requests to a Ray Cluster.For convenience, this guide will assume that you are using a local Ray Cluster, which we can start by running:\n\n.. code-block:: shell\n\n    ray start --head\n    # ...\n    # 2022-08-10 09:54:57,664   INFO services.py:1476 -- View the Ray dashboard at http://127.0.0.1:8265\n    # ...This will create a Ray head node on our local machine that we can use for development purposes.Note the Ray Dashboard URL that is printed when starting or connecting to a Ray Cluster; we will use this URL later to submit a Ray Job.See :ref:`Using a Remote Cluster <jobs-remote-cluster>` for tips on port-forwarding if using a remote cluster.For more details on production deployment scenarios, check out the guides for deploying Ray on :ref:`VMs <vm-cluster-quick-start>` and :ref:`Kubernetes <kuberay-quickstart>`.Submitting a Ray Job\n--------------------\n\nLet's start with a sample script that can be run locally.The following script uses Ray APIs to submit a task and print its return value:\n\n.. code-block:: python\n\n    # script.py\n    import ray\n\n    @ray.remote\n    def hello_world():\n        return \"hello world\"\n\n    ray.init()\n    print(ray.get(hello_world.remote()))\n\nSDK calls are made via a ``JobSubmissionClient`` object.To initialize the client, provide the Ray cluster head node address and the port used by the Ray Dashboard (``8265`` by default).For this example, we'll use a local Ray cluster, but the same example will work for remote Ray cluster addresses... code-block:: python\n\n    from ray.job_submission import JobSubmissionClient\n\n    # If using a remote cluster, replace 127.0.0.1 with the head node's IP address.client = JobSubmissionClient(\"http://127.0.0.1:8265\")\n    job_id = client.submit_job(\n        # Entrypoint shell command to execute\n        entrypoint=\"python script.py\",\n        # Path to the local directory that contains the script.py file\n        runtime_env={\"working_dir\": \"./\"}\n    )\n    print(job_id)\n\n.. tip::\n\n    By default, the Ray job server will generate a new ``job_id`` and return it, but you can alternatively choose a unique ``job_id`` string first and pass it into :code:`submit_job`.In this case, the Job will be executed with your given id, and will throw an error if the same ``job_id`` is submitted more than once for the same Ray cluster.Because job submission is asynchronous, the above call will return immediately with output like the following:\n\n.. code-block:: bash\n\n    raysubmit_g8tDzJ6GqrCy7pd6\n\nNow we can write a simple polling loop that checks the job status until it reaches a terminal state (namely, ``JobStatus.SUCCEEDED``, ``JobStatus.STOPPED``, or ``JobStatus.FAILED``).We can also get the output of the job by calling ``client.get_job_logs``... code-block:: python\n\n    from ray.job_submission import JobSubmissionClient, JobStatus\n    import time\n\n    # If using a remote cluster, replace 127.0.0.1 with the head node's IP address.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b508c27b-7c3c-4e8a-99fe-1e73f2976d33": {"__data__": {"id_": "b508c27b-7c3c-4e8a-99fe-1e73f2976d33", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "d773f6c6db6b0716f9dfa84341babd5e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst"}, "hash": "5a59558d31b6cf7498b84e1a98c7777bca5df03f4b205200f7f9d53961b03788"}, "2": {"node_id": "fafd028b-0881-4c69-8af9-4617374e8dc7", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "865eb28fbf5acb2ed02e8c07664e0b37"}, "hash": "ff3caf2693b5c1d35fca555c796e540a0d081cfdb4b4f5a5a869711a1adc526d"}, "3": {"node_id": "a0636629-900d-45e2-89ba-4c24c16bbd06", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "8b3f768194774d4f075bcf32b0a851de"}, "hash": "77dea3380f7397c615d9708cb4e45635d4a27ba8d7a31a5512e81b2787deb429"}}, "hash": "b6c15177fc64d62f6df5e3b41cdef000e0dbc632dd59db1108e85d80a48498c4", "text": "client = JobSubmissionClient(\"http://127.0.0.1:8265\")\n    job_id = client.submit_job(\n        # Entrypoint shell command to execute\n        entrypoint=\"python script.py\",\n        # Path to the local directory that contains the script.py file\n        runtime_env={\"working_dir\": \"./\"}\n    )\n    print(job_id)\n\n    def wait_until_status(job_id, status_to_wait_for, timeout_seconds=5):\n        start = time.time()\n        while time.time() - start <= timeout_seconds:\n            status = client.get_job_status(job_id)\n            print(f\"status: {status}\")\n            if status in status_to_wait_for:\n                break\n            time.sleep(1)", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a0636629-900d-45e2-89ba-4c24c16bbd06": {"__data__": {"id_": "a0636629-900d-45e2-89ba-4c24c16bbd06", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "8b3f768194774d4f075bcf32b0a851de"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst"}, "hash": "5a59558d31b6cf7498b84e1a98c7777bca5df03f4b205200f7f9d53961b03788"}, "2": {"node_id": "b508c27b-7c3c-4e8a-99fe-1e73f2976d33", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "d773f6c6db6b0716f9dfa84341babd5e"}, "hash": "b6c15177fc64d62f6df5e3b41cdef000e0dbc632dd59db1108e85d80a48498c4"}, "3": {"node_id": "74d82ea6-3418-43c2-87cf-29e54a21f3bb", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "501a75a5da4d20e2b1a48d89fe58f52a"}, "hash": "f4be7c3e7ee55a2fbcb8f22fbc581416fa1a90e4154fd9a40923d6f4231c2dca"}}, "hash": "77dea3380f7397c615d9708cb4e45635d4a27ba8d7a31a5512e81b2787deb429", "text": "wait_until_status(job_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})\n    logs = client.get_job_logs(job_id)\n    print(logs)\n\nThe output should look something like this:\n\n.. code-block:: bash\n\n    raysubmit_pBwfn5jqRE1E7Wmc\n    status: PENDING\n    status: PENDING\n    status: RUNNING\n    status: RUNNING\n    status: RUNNING\n    2022-08-22 15:05:55,652 INFO worker.py:1203 -- Using address 127.0.0.1:6379 set in the environment variable RAY_ADDRESS\n    2022-08-22 15:05:55,652 INFO worker.py:1312 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379...\n    2022-08-22 15:05:55,660 INFO worker.py:1487 -- Connected to Ray cluster. View the dashboard at http://127.0.0.1:8265.\n    hello world\n\nInteracting with Long-running Jobs\n----------------------------------\n\nIn addition to getting the current status and output of a job, a submitted job can also be stopped by the user before it finishes executing.\n\n.. code-block:: python\n\n    job_id = client.submit_job(\n        # Entrypoint shell command to execute\n        entrypoint=\"python -c 'import time; print(\\\"Sleeping...\\\"); time.sleep(60)'\"\n    )\n    wait_until_status(job_id, {JobStatus.RUNNING})\n    print(f'Stopping job {job_id}')\n    client.stop_job(job_id)\n    wait_until_status(job_id, {JobStatus.SUCCEEDED, JobStatus.STOPPED, JobStatus.FAILED})\n    logs = client.get_job_logs(job_id)\n    print(logs)\n\nThe output should look something like the following:\n\n.. code-block:: bash\n\n    status: PENDING\n    status: PENDING\n    status: RUNNING\n    Stopping job raysubmit_VYCZZ2BQb4tfeCjq\n    status: STOPPED\n    Sleeping...\n\nTo get information about all jobs, call ``client.list_jobs()``.  This returns a ``Dict[str, JobInfo]`` object mapping Job IDs to their information.\n\nJob information (status and associated metadata) is stored on the cluster indefinitely.  \nTo delete this information, you may call ``client.delete_job(job_id)`` for any job that is already in a terminal state.  \nSee the :ref:`SDK API Reference <ray-job-submission-sdk-ref>` for more details.\n\nDependency Management\n---------------------\n\nSimilar to the :ref:`Jobs CLI <jobs-quickstart>`, we can also package our application's dependencies by using a Ray :ref:`runtime environment <runtime-environments>`.\nUsing the Python SDK, the syntax looks something like this:\n\n.. code-block:: python\n\n    job_id = client.submit_job(\n        # Entrypoint shell command to execute\n        entrypoint=\"python script.py\",\n        # Runtime environment for the job, specifying a working directory and pip package\n        runtime_env={\n            \"working_dir\": \"./\",\n            \"pip\": [\"requests==2.26.0\"]\n        }\n    )\n\n.. tip::\n\n    Instead of a local directory (``\"./\"`` in this example), you can also specify remote URIs for your job's working directory, such as S3 buckets or Git repositories. See :ref:`remote-uris` for details.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "74d82ea6-3418-43c2-87cf-29e54a21f3bb": {"__data__": {"id_": "74d82ea6-3418-43c2-87cf-29e54a21f3bb", "embedding": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "501a75a5da4d20e2b1a48d89fe58f52a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst"}, "hash": "5a59558d31b6cf7498b84e1a98c7777bca5df03f4b205200f7f9d53961b03788"}, "2": {"node_id": "a0636629-900d-45e2-89ba-4c24c16bbd06", "node_type": null, "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "8b3f768194774d4f075bcf32b0a851de"}, "hash": "77dea3380f7397c615d9708cb4e45635d4a27ba8d7a31a5512e81b2787deb429"}}, "hash": "f4be7c3e7ee55a2fbcb8f22fbc581416fa1a90e4154fd9a40923d6f4231c2dca", "text": "For full details, see the :ref:`API Reference <ray-job-submission-sdk-ref>`.\n\n\nSpecifying CPU and GPU resources\n--------------------------------\n\nWe recommend doing heavy computation within Ray tasks, actors, or Ray libraries, not directly in the top level of your entrypoint script.\nNo extra configuration is needed to do this.\n\nHowever, if you need to do computation directly in the entrypoint script and would like to reserve CPU and GPU resources for the entrypoint script, you may specify the ``entrypoint_num_cpus``, ``entrypoint_num_gpus`` and ``entrypoint_resources`` arguments to ``submit_job``.  These arguments function\nidentically to the ``num_cpus``, ``num_gpus``, and ``resources`` arguments to ``@ray.remote()`` decorator for tasks and actors as described in :ref:`resource-requirements`.\n\n.. code-block:: python\n\n    job_id = client.submit_job(\n        entrypoint=\"python script.py\",\n        runtime_env={\n            \"working_dir\": \"./\",\n        }\n        # Reserve 1 GPU for the entrypoint script\n        entrypoint_num_gpus=1\n    )\n\nThe same arguments are also available as options ``--entrypoint-num-cpus``, ``--entrypoint-num-gpus``, and ``--entrypoint-resources`` to ``ray job submit`` in the Jobs CLI; see :ref:`Ray Job Submission CLI Reference <ray-job-submission-cli-ref>`.\n\nIf ``num_gpus`` is not specified, GPUs will still be available to the entrypoint script, but Ray will not provide isolation in terms of visible devices. \nTo be precise, the environment variable ``CUDA_VISIBLE_DEVICES`` will not be set in the entrypoint script; it will only be set inside tasks and actors that have `num_gpus` specified in their ``@ray.remote()`` decorator.\n\n.. note::\n\n    Resources specified by ``entrypoint_num_cpus``, ``entrypoint_num_gpus``, and ``entrypoint_resources`` are separate from any resources specified\n    for tasks and actors within the job.  \n    \n    For example, if you specify ``entrypoint_num_gpus=1``, then the entrypoint script will be scheduled on a node with at least 1 GPU,\n    but if your script also contains a Ray task defined with ``@ray.remote(num_gpus=1)``, then the task will be scheduled to use a different GPU (on the same node if the node has at least 2 GPUs, or on a different node otherwise).\n\n.. note::\n    \n    As with the ``num_cpus``, ``num_gpus``, and ``resources`` arguments to ``@ray.remote()`` described in :ref:`resource-requirements`, these arguments only refer to logical resources used for scheduling purposes. The actual CPU and GPU utilization is not controlled or limited by Ray.\n\n\n.. note::\n\n    By default, 0 CPUs and 0 GPUs are reserved for the entrypoint script.\n\n\nClient Configuration\n--------------------------------\n\nAdditional client connection options, such as custom HTTP headers and cookies, can be passed to the ``JobSubmissionClient`` class. \nA full list of options can be found in the :ref:`API Reference <ray-job-submission-sdk-ref>`.\n\nTLS Verification\n~~~~~~~~~~~~~~~~~\nBy default, any HTTPS client connections will be verified using system certificates found by the underlying ``requests`` and ``aiohttp`` libraries.\nThe ``verify`` parameter can be set to override this behavior. For example:\n\n.. code-block:: python\n\n    client = JobSubmissionClient(\"https://<job-server-url>\", verify=\"/path/to/cert.pem\")\n\nwill use the certificate found at ``/path/to/cert.pem`` to verify the job server's certificate.\nCertificate verification can be disabled by setting the ``verify`` parameter to ``False``.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "394969df-f50a-49be-a41b-99e28a98665c": {"__data__": {"id_": "394969df-f50a-49be-a41b-99e28a98665c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst", "text_hash": "ff123026a778b7a032d6b85256fa2ad1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a191a1b577180b83bb6ec9a4792268fdcff13885", "node_type": null, "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst"}, "hash": "fcd2814655ed2c459feba758cdf47acde3fce66e5082430f22271d00bca61b97"}, "3": {"node_id": "3b6e4178-818d-4c23-abc7-ef68c066b191", "node_type": null, "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst", "text_hash": "c8dba49ce67a9091bff274fe38650eef"}, "hash": "4fe72503f63fbf2dbc7b9297e40ad3586560b16b4ab1f686705f455f188bd024"}}, "hash": "c7cc6a28ea16b9835e5adb5a0f66646a382f98a3d57507007d995a6ac2a66786", "text": ".. _ref-usage-stats:\n\nUsage Stats Collection\n======================\n\nStarting in Ray 1.13, Ray collects usage stats data by default (guarded by an opt-out prompt).\nThis data will be used by the open-source Ray engineering team to better understand how to improve our libraries and core APIs, and how to prioritize bug fixes and enhancements.\n\nHere are the guiding principles of our collection policy:\n\n- **No surprises** \u2014 you will be notified before we begin collecting data. You will be notified of any changes to the data being collected or how it is used.\n- **Easy opt-out:** You will be able to easily opt-out of data collection\n- **Transparency** \u2014 you will be able to review all data that is sent to us\n- **Control** \u2014 you will have control over your data, and we will honor requests to delete your data.\n- We will **not** collect any personally identifiable data or proprietary code/data\n- We will **not** sell data or buy data about you.\n\nYou will always be able to :ref:`disable the usage stats collection <usage-disable>`.\n\nFor more context, please refer to this `RFC <https://github.com/ray-project/ray/issues/20857>`_.\n\nWhat data is collected?\n-----------------------\n\nWe collect non-sensitive data that helps us understand how Ray is used (e.g., which Ray libraries are used).\n**Personally identifiable data will never be collected.** Please check the UsageStatsToReport class to see the data we collect.\n\n.. _usage-disable:\n\nHow to disable it\n-----------------\nThere are multiple ways to disable usage stats collection before starting a cluster:\n\n#. Add ``--disable-usage-stats`` option to the command that starts the Ray cluster (e.g., ``ray start --head --disable-usage-stats`` :ref:`command <ray-start-doc>`).\n\n#. Run :ref:`ray disable-usage-stats <ray-disable-usage-stats-doc>` to disable collection for all future clusters. This won't affect currently running clusters. Under the hood, this command writes ``{\"usage_stats\": true}`` to the global config file ``~/.ray/config.json``.\n\n#. Set the environment variable ``RAY_USAGE_STATS_ENABLED`` to 0 (e.g., ``RAY_USAGE_STATS_ENABLED=0 ray start --head`` :ref:`command <ray-start-doc>`).\n\n#. If you're using `KubeRay <https://github.com/ray-project/kuberay/>`_, you can add ``disable-usage-stats: 'true'`` to ``.spec.[headGroupSpec|workerGroupSpecs].rayStartParams.``.\n\nCurrently there is no way to enable or disable collection for a running cluster; you have to stop and restart the cluster.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3b6e4178-818d-4c23-abc7-ef68c066b191": {"__data__": {"id_": "3b6e4178-818d-4c23-abc7-ef68c066b191", "embedding": null, "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst", "text_hash": "c8dba49ce67a9091bff274fe38650eef"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "a191a1b577180b83bb6ec9a4792268fdcff13885", "node_type": null, "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst"}, "hash": "fcd2814655ed2c459feba758cdf47acde3fce66e5082430f22271d00bca61b97"}, "2": {"node_id": "394969df-f50a-49be-a41b-99e28a98665c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst", "text_hash": "ff123026a778b7a032d6b85256fa2ad1"}, "hash": "c7cc6a28ea16b9835e5adb5a0f66646a382f98a3d57507007d995a6ac2a66786"}}, "hash": "4fe72503f63fbf2dbc7b9297e40ad3586560b16b4ab1f686705f455f188bd024", "text": "How does it work?\n-----------------\n\nWhen a Ray cluster is started via :ref:`ray start --head <ray-start-doc>`, :ref:`ray up <ray-up-doc>`, :ref:`ray submit --start <ray-submit-doc>` or :ref:`ray exec --start <ray-exec-doc>`,\nRay will decide whether usage stats collection should be enabled or not by considering the following factors in order:\n\n#. It checks whether the environment variable ``RAY_USAGE_STATS_ENABLED`` is set: 1 means enabled and 0 means disabled.\n\n#. If the environment variable is not set, it reads the value of key ``usage_stats`` in the global config file ``~/.ray/config.json``: true means enabled and false means disabled.\n\n#. If neither is set and the console is interactive, then the user will be prompted to enable or disable the collection. If the console is non-interactive, usage stats collection will be enabled by default. The decision will be saved to ``~/.ray/config.json``, so the prompt is only shown once.\n\nNote: usage stats collection is not enabled when using local dev clusters started via ``ray.init()`` unless it's a nightly wheel. This means that Ray will never collect data from third-party library users not using Ray directly.\n\nIf usage stats collection is enabled, a background process on the head node will collect the usage stats\nand report to ``https://usage-stats.ray.io/`` every hour. The reported usage stats will also be saved to\n``/tmp/ray/session_xxx/usage_stats.json`` on the head node for inspection. You can check the existence of this file to see if collection is enabled.\n\nUsage stats collection is very lightweight and should have no impact on your workload in any way.\n\nRequesting removal of collected data\n------------------------------------\n\nTo request removal of collected data, please email us at ``usage_stats@ray.io`` with the ``session_id`` that you can find in ``/tmp/ray/session_xxx/usage_stats.json``.\n\nFrequently Asked Questions (FAQ)\n--------------------------------\n\n**Does the session_id map to personal data?**\n\nNo, the uuid will be a Ray session/job-specific random ID that cannot be used to identify a specific person nor machine. It will not live beyond the lifetime of your Ray session; and is primarily captured to enable us to honor deletion requests.\n\nThe session_id is logged so that deletion requests can be honored.\n\n**Could an enterprise easily configure an additional endpoint or substitute a different endpoint?**\n\nWe definitely see this use case and would love to chat with you to make this work -- email ``usage_stats@ray.io``.\n\n\nContact us\n----------\nIf you have any feedback regarding usage stats collection, please email us at ``usage_stats@ray.io``.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "9eb7f6a6-7852-4efb-8774-70ba5ae920b1": {"__data__": {"id_": "9eb7f6a6-7852-4efb-8774-70ba5ae920b1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/index.md", "file_name": "index.md", "text_hash": "a587cbaf369a564b483eecd0a79a91fa"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "059be000a548ba333d06422639928f37453b7acd", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/index.md", "file_name": "index.md"}, "hash": "326dc058acbdcf7e4a71ccbffa090ab3e434af8df76cb25ec18bff88ffbf6fa6"}}, "hash": "79b5ead9684b6ea026d0775f1b0d6faac3cf6a09bbb5d079718bfda585698080", "text": "(vm-cluster-examples)=\n\n# Examples\n\n:::{note}\nTo learn the basics of Ray on Cloud VMs, we recommend taking a look\nat the {ref}`introductory guide <vm-cluster-quick-start>` first.\n:::\n\nThis section presents example Ray workloads to try out on your cloud cluster.\n\nMore examples will be added in the future. Running the distributed XGBoost example below is a\ngreat way to start experimenting with production Ray workloads in the cloud.\n- {ref}`clusters-vm-ml-example`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6c161615-4bfe-4128-9b9e-ba0feb4d1c9a": {"__data__": {"id_": "6c161615-4bfe-4128-9b9e-ba0feb4d1c9a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "b44bb5fd0af5a3cf2462fa7ae4ea1e19"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3389ca272d7794cef1f1530d852ce7e99e0a8c9b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md"}, "hash": "704fc5037ece03302a95e625ea334d93b8abd79db2d2a089456d9c0511fd2f38"}, "3": {"node_id": "157cdab9-099b-407f-9519-196cd295fafd", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "362d486cd44e877ce49695843dcee760"}, "hash": "5c01b610b85e9d8bd106a650043b4fdd9834c32002a45615654219b173661ac1"}}, "hash": "284ce6a894bc0b9599153fe93eadf344068448cd002536b5f986eed55edb282b", "text": "(clusters-vm-ml-example)=\n\n# Ray Train XGBoostTrainer on VMs\n\n:::{note}\nTo learn the basics of Ray on VMs, we recommend taking a look\nat the {ref}`introductory guide <vm-cluster-quick-start>` first.\n:::\n\n\nIn this guide, we show you how to run a sample Ray machine learning\nworkload on AWS.The similar steps can be used to deploy on GCP or Azure as well.We will run Ray's {ref}`XGBoost training benchmark <xgboost-benchmark>` with a 100 gigabyte training set.To learn more about using Ray's XGBoostTrainer, check out {ref}`the XGBoostTrainer documentation <train-gbdt-guide>`.## VM cluster setup\n\nFor the workload in this guide, it is recommended to use the following setup:\n- 10 nodes total\n- A capacity of 16 CPU and 64 Gi memory per node.For the major cloud providers, suitable instance types include\n    * m5.4xlarge (Amazon Web Services)\n    * Standard_D5_v2 (Azure)\n    * e2-standard-16 (Google Cloud)\n- Each node should be configured with 1000 gigabytes of disk space (to store the training set).The corresponding cluster configuration file is as follows:\n\n```{literalinclude} ../configs/xgboost-benchmark.yaml\n:language: yaml\n```\n\n```{admonition} Optional: Set up an autoscaling cluster\n**If you would like to try running the workload with autoscaling enabled**,\nchange ``min_workers`` of worker nodes to 0.After the workload is submitted, 9 workers nodes will\nscale up to accommodate the workload.These nodes will scale back down after the workload is complete.```\n\n## Deploy a Ray cluster\n\nNow we're ready to deploy the Ray cluster with the configuration that's defined above.Before running the command, make sure your aws credentials are configured correctly.```shell\nray up -y cluster.yaml\n```\n\nA Ray head node and 9 Ray worker nodes will be created.## Run the workload\n\nWe will use {ref}`Ray Job Submission <jobs-overview>` to kick off the workload.### Connect to the cluster\n\nFirst, we connect to the Job server.Run the following blocking command\nin a separate shell.```shell\nray dashboard cluster.yaml\n```\nThis will forward remote port 8265 to port 8265 on localhost.### Submit the workload\n\nWe'll use the {ref}`Ray Job Python SDK <ray-job-sdk>` to submit the XGBoost workload.```{literalinclude} /cluster/doc_code/xgboost_submit.py\n:language: python\n```\n\nTo submit the workload, run the above Python script.The script is available [in the Ray repository][XGBSubmit].```shell\n# Download the above script.curl https://raw.githubusercontent.com/ray-project/ray/releases/2.0.0/doc/source/cluster/doc_code/xgboost_submit.py -o xgboost_submit.py\n# Run the script.python xgboost_submit.py\n```\n\n### Observe progress\n\nThe benchmark may take up to 30 minutes to run.Use the following tools to observe its progress.#### Job logs\n\nTo follow the job's logs, use the command printed by the above submission script.```shell\n# Subsitute the Ray Job's submission id.ray job logs 'raysubmit_xxxxxxxxxxxxxxxx' --address=\"http://localhost:8265\" --follow\n```\n\n#### Ray Dashboard\n\nView `localhost:8265` in your browser to access the Ray Dashboard.#### Ray Status\n\nObserve autoscaling status and Ray resource usage with\n```shell\nray exec cluster.yaml 'ray status'\n```\n\n### Job completion\n\n#### Benchmark results\n\nOnce the benchmark is complete, the job log will display the results:\n\n```\nResults: {'training_time': 1338.488839321999, 'prediction_time': 403.36653568099973}\n```\n\nThe performance of the benchmark is sensitive to the underlying cloud infrastructure --\nyou might not match {ref}`the numbers quoted in the benchmark docs <xgboost-benchmark>`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "157cdab9-099b-407f-9519-196cd295fafd": {"__data__": {"id_": "157cdab9-099b-407f-9519-196cd295fafd", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "362d486cd44e877ce49695843dcee760"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3389ca272d7794cef1f1530d852ce7e99e0a8c9b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md"}, "hash": "704fc5037ece03302a95e625ea334d93b8abd79db2d2a089456d9c0511fd2f38"}, "2": {"node_id": "6c161615-4bfe-4128-9b9e-ba0feb4d1c9a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "b44bb5fd0af5a3cf2462fa7ae4ea1e19"}, "hash": "284ce6a894bc0b9599153fe93eadf344068448cd002536b5f986eed55edb282b"}}, "hash": "5c01b610b85e9d8bd106a650043b4fdd9834c32002a45615654219b173661ac1", "text": "#### Model parameters\nThe file `model.json` in the Ray head node contains the parameters for the trained model.Other result data will be available in the directory `ray_results` in the head node.Refer to the {ref}`XGBoostTrainer documentation <train-gbdt-guide>` for details.```{admonition} Scale-down\nIf autoscaling is enabled, Ray worker nodes will scale down after the specified idle timeout.```\n\n#### Clean-up\nDelete your Ray cluster with the following command:\n```shell\nray down -y cluster.yaml\n```\n\n[XGBSubmit]: https://github.com/ray-project/ray/blob/releases/2.0.0/doc/source/cluster/doc_code/xgboost_submit.py", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c34c8f2b-79bc-4e78-9f99-432e074451b3": {"__data__": {"id_": "c34c8f2b-79bc-4e78-9f99-432e074451b3", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "2b13f2af0921f0ee8ccbb6720ac5ed5a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "673090c62afb00e968ba0187f61b64b41346e7cb714ce5b570e75e82d521ba58"}, "3": {"node_id": "82dcb8f1-44af-499b-80bb-6906495be97b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "1e4be47545ec57bb3a92106cd7544329"}, "hash": "18eb04eb1cbf91b62f06e4dccea1cb5676af07954713fccb9cc92f89e09daa43"}}, "hash": "c5fe0ec146c2e6bae0632c381f932a6f5dd978525608c04f2994d76033aab88b", "text": ".. include:: /_includes/clusters/announcement.rst\n\n.. _vm-cluster-quick-start:\n\nGetting Started\n===============\n\nThis quick start demonstrates the capabilities of the Ray cluster. Using the Ray cluster, we'll take a sample application designed to run on a laptop and scale it up in the cloud. Ray will launch clusters and scale Python with just a few commands.\n\nFor launching a Ray cluster manually, you can refer to the :ref:`on-premise cluster setup <on-prem>` guide.\n\nAbout the demo\n--------------\n\nThis demo will walk through an end-to-end flow:\n\n1. Create a (basic) Python application.\n2. Launch a cluster on a cloud provider.\n3. Run the application in the cloud.\n\nRequirements\n~~~~~~~~~~~~\n\nTo run this demo, you will need:\n\n* Python installed on your development machine (typically your laptop), and\n* an account at your preferred cloud provider (AWS, GCP, Azure, Aliyun, or vSphere).\n\nSetup\n~~~~~\n\nBefore we start, you will need to install some Python dependencies as follows:\n\n.. tabs::\n\n   .. tab:: Ray Team Supported\n\n      .. tabs::\n\n         .. tab:: AWS\n\n            .. code-block:: shell\n\n                $ pip install -U \"ray[default]\" boto3\n\n         .. tab:: GCP\n\n            .. code-block:: shell\n\n                $ pip install -U \"ray[default]\" google-api-python-client\n\n   .. tab:: Community Supported\n\n      .. tabs::\n\n         .. tab:: Azure\n\n            .. code-block:: shell\n\n                $ pip install -U \"ray[default]\" azure-cli azure-core\n\n         .. tab:: Aliyun\n\n            .. code-block:: shell\n\n                $ pip install -U \"ray[default]\" aliyun-python-sdk-core aliyun-python-sdk-ecs\n            \n            Aliyun Cluster Launcher Maintainers (GitHub handles): @zhuangzhuang131419, @chenk008\n\n         .. tab:: vSphere\n\n            .. code-block:: shell\n\n                $ pip install -U \"ray[default]\" vsphere-automation-sdk-python\n\n            vSphere Cluster Launcher Maintainers (GitHub handles): @vinodkri, @LaynePeng", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "82dcb8f1-44af-499b-80bb-6906495be97b": {"__data__": {"id_": "82dcb8f1-44af-499b-80bb-6906495be97b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "1e4be47545ec57bb3a92106cd7544329"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "673090c62afb00e968ba0187f61b64b41346e7cb714ce5b570e75e82d521ba58"}, "2": {"node_id": "c34c8f2b-79bc-4e78-9f99-432e074451b3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "2b13f2af0921f0ee8ccbb6720ac5ed5a"}, "hash": "c5fe0ec146c2e6bae0632c381f932a6f5dd978525608c04f2994d76033aab88b"}, "3": {"node_id": "384bbf17-abd1-4393-b76b-2d61100f54e1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "fe2bd3088c385bbdbb1e197cdc75e339"}, "hash": "15d8adaf2a1e56d45de1a1fdb567659313c2f9e788ece5e58979efb22b43de23"}}, "hash": "18eb04eb1cbf91b62f06e4dccea1cb5676af07954713fccb9cc92f89e09daa43", "text": "Next, if you're not set up to use your cloud provider from the command line, you'll have to configure your credentials:\n\n.. tabs::\n\n   .. tab:: Ray Team Supported\n\n      .. tabs::\n\n         .. tab:: AWS\n\n            Configure your credentials in ``~/.aws/credentials`` as described in `the AWS docs <https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html>`_.\n\n         .. tab:: GCP\n\n            Set the ``GOOGLE_APPLICATION_CREDENTIALS`` environment variable as described in `the GCP docs <https://cloud.google.com/docs/authentication/getting-started>`_.\n\n   .. tab:: Community Supported\n\n      .. tabs::\n\n         .. tab:: Azure\n\n            Log in using ``az login``, then configure your credentials with ``az account set -s <subscription_id>``.\n\n         .. tab:: Aliyun\n\n            Obtain and set the AccessKey pair of the Aliyun account as described in `the docs <https://www.alibabacloud.com/help/en/doc-detail/175967.htm>`__.\n\n            Make sure to grant the necessary permissions to the RAM user and set the AccessKey pair in your cluster config file.\n            Refer to the provided `aliyun/example-full.yaml </ray/python/ray/autoscaler/aliyun/example-full.yaml>`__ for a sample cluster config.\n\n         .. tab:: vSphere\n\n            .. code-block:: shell\n\n                $ export VSPHERE_SERVER=192.168.0.1 # Enter your vSphere IP\n                $ export VSPHERE_USER=user # Enter your username\n                $ export VSPHERE_PASSWORD=password # Enter your password\n\n\nCreate a (basic) Python application\n-----------------------------------\n\nWe will write a simple Python application that tracks the IP addresses of the machines that its tasks are executed on:\n\n.. code-block:: python\n\n    from collections import Counter\n    import socket\n    import time\n\n    def f():\n        time.sleep(0.001)\n        # Return IP address.return socket.gethostbyname(socket.gethostname())\n\n    ip_addresses = [f() for _ in range(10000)]\n    print(Counter(ip_addresses))\n\nSave this application as ``script.py`` and execute it by running the command ``python script.py``.The application should take 10 seconds to run and output something similar to ``Counter({'127.0.0.1': 10000})``.With some small changes, we can make this application run on Ray (for more information on how to do this, refer to :ref:`the Ray Core Walkthrough <core-walkthrough>`):\n\n.. code-block:: python\n\n    from collections import Counter\n    import socket\n    import time\n\n    import ray\n\n    ray.init()\n\n    @ray.remote\n    def f():\n        time.sleep(0.001)\n        # Return IP address.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "384bbf17-abd1-4393-b76b-2d61100f54e1": {"__data__": {"id_": "384bbf17-abd1-4393-b76b-2d61100f54e1", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "fe2bd3088c385bbdbb1e197cdc75e339"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "673090c62afb00e968ba0187f61b64b41346e7cb714ce5b570e75e82d521ba58"}, "2": {"node_id": "82dcb8f1-44af-499b-80bb-6906495be97b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "1e4be47545ec57bb3a92106cd7544329"}, "hash": "18eb04eb1cbf91b62f06e4dccea1cb5676af07954713fccb9cc92f89e09daa43"}, "3": {"node_id": "6083ac45-ab63-465d-9991-d2cc4c009f3a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "de4fa1e014bf378fd137ba3e9e7ff509"}, "hash": "6db6724d2771f5cbf05510ced06b0109f64e392e2a4b4c116d0db0c6d1554186"}}, "hash": "15d8adaf2a1e56d45de1a1fdb567659313c2f9e788ece5e58979efb22b43de23", "text": "return socket.gethostbyname(socket.gethostname())\n\n    object_ids = [f.remote() for _ in range(10000)]\n    ip_addresses = ray.get(object_ids)\n    print(Counter(ip_addresses))\n\nFinally, let's add some code to make the output more interesting:\n\n.. code-block:: python\n\n    from collections import Counter\n    import socket\n    import time\n\n    import ray\n\n    ray.init()\n\n    print('''This cluster consists of\n        {} nodes in total\n        {} CPU resources in total\n    '''.format(len(ray.nodes()), ray.cluster_resources()['CPU']))\n\n    @ray.remote\n    def f():\n        time.sleep(0.001)\n        # Return IP address.return socket.gethostbyname(socket.gethostname())\n\n    object_ids = [f.remote() for _ in range(10000)]\n    ip_addresses = ray.get(object_ids)\n\n    print('Tasks executed')\n    for ip_address, num_tasks in Counter(ip_addresses).items():\n        print('    {} tasks on {}'.format(num_tasks, ip_address))\n\nRunning ``python script.py`` should now output something like:\n\n.. parsed-literal::\n\n    This cluster consists of\n        1 nodes in total\n        4.0 CPU resources in total\n\n    Tasks executed\n        10000 tasks on 127.0.0.1\n\nLaunch a cluster on a cloud provider\n------------------------------------\n\nTo start a Ray Cluster, first we need to define the cluster configuration.The cluster configuration is defined within a YAML file that will be used by the Cluster Launcher to launch the head node, and by the Autoscaler to launch worker nodes.A minimal sample cluster configuration file looks as follows:\n\n.. tabs::\n\n   .. tab:: Ray Team Supported\n\n      .. tabs::\n\n         .. tab:: AWS\n\n            .. literalinclude:: ../../../../python/ray/autoscaler/aws/example-minimal.yaml\n               :language: yaml\n\n         .. tab:: GCP\n\n            .. code-block:: yaml\n\n                # A unique identifier for the head node and workers of this cluster.cluster_name: minimal\n\n                # Cloud-provider specific configuration.provider:\n                    type: gcp\n                    region: us-west1\n\n   .. tab:: Community Supported\n\n      .. tabs::\n\n         .. tab:: Azure\n\n            .. code-block:: yaml\n\n                # An unique identifier for the head node and workers of this cluster.cluster_name: minimal\n\n                # Cloud-provider specific configuration.provider:\n                    type: azure\n                    location: westus2\n                    resource_group: ray-cluster\n\n                # How Ray will authenticate with newly launched nodes.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6083ac45-ab63-465d-9991-d2cc4c009f3a": {"__data__": {"id_": "6083ac45-ab63-465d-9991-d2cc4c009f3a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "de4fa1e014bf378fd137ba3e9e7ff509"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "673090c62afb00e968ba0187f61b64b41346e7cb714ce5b570e75e82d521ba58"}, "2": {"node_id": "384bbf17-abd1-4393-b76b-2d61100f54e1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "fe2bd3088c385bbdbb1e197cdc75e339"}, "hash": "15d8adaf2a1e56d45de1a1fdb567659313c2f9e788ece5e58979efb22b43de23"}, "3": {"node_id": "a4a6cdf4-8a73-4b62-a8c6-20b20506458f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "db368a26a1ddd97163afcf1f90bc5e6a"}, "hash": "381ae6444e90490cea02fc8daf6f88e6c77cceaa057e50aff1fb05320129fc20"}}, "hash": "6db6724d2771f5cbf05510ced06b0109f64e392e2a4b4c116d0db0c6d1554186", "text": "auth:\n                    ssh_user: ubuntu\n                    # you must specify paths to matching private and public key pair files\n                    # use `ssh-keygen -t rsa -b 4096` to generate a new ssh key pair\n                    ssh_private_key: ~/.ssh/id_rsa\n                    # changes to this should match what is specified in file_mounts\n                    ssh_public_key: ~/.ssh/id_rsa.pub\n\n         .. tab:: Aliyun\n\n            Please refer to `example-full.yaml </ray/python/ray/autoscaler/aliyun/example-full.yaml>`__.Make sure your account balance is not less than 100 RMB, otherwise you will receive the error `InvalidAccountStatus.NotEnoughBalance`... tab:: vSphere\n\n            .. code-block:: yaml\n\n                # A unique identifier for the head node and workers of this cluster.cluster_name: minimal\n\n                # Cloud-provider specific configuration.provider:\n                    type: vsphere\n                \n                auth:\n                    ssh_user: ray # The VMs are initialised with an user called ray.available_node_types:\n                    ray.head.default:\n                        node_config:\n                            resource_pool: ray # Resource pool where the Ray cluster will get created\n                            library_item: ray-head-debian # OVF file name from which the head will be created\n\n                    worker:\n                        node_config:\n                            clone: True # If True, all the workers will be instant-cloned from a frozen VM\n                            library_item: ray-frozen-debian # The OVF file from which a frozen VM will be created", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a4a6cdf4-8a73-4b62-a8c6-20b20506458f": {"__data__": {"id_": "a4a6cdf4-8a73-4b62-a8c6-20b20506458f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "db368a26a1ddd97163afcf1f90bc5e6a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst"}, "hash": "673090c62afb00e968ba0187f61b64b41346e7cb714ce5b570e75e82d521ba58"}, "2": {"node_id": "6083ac45-ab63-465d-9991-d2cc4c009f3a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "de4fa1e014bf378fd137ba3e9e7ff509"}, "hash": "6db6724d2771f5cbf05510ced06b0109f64e392e2a4b4c116d0db0c6d1554186"}}, "hash": "381ae6444e90490cea02fc8daf6f88e6c77cceaa057e50aff1fb05320129fc20", "text": "Save this configuration file as ``config.yaml``. You can specify a lot more details in the configuration file: instance types to use, minimum and maximum number of workers to start, autoscaling strategy, files to sync, and more. For a full reference on the available configuration properties, please refer to the :ref:`cluster YAML configuration options reference <cluster-config>`.\n\nAfter defining our configuration, we will use the Ray cluster launcher to start a cluster on the cloud, creating a designated \"head node\" and worker nodes. To start the Ray cluster, we will use the :ref:`Ray CLI <ray-cluster-cli>`. Run the following command:\n\n.. code-block:: shell\n\n    $ ray up -y config.yaml\n\nRunning applications on a Ray Cluster\n-------------------------------------\n\nWe are now ready to execute an application on our Ray Cluster.\n``ray.init()`` will now automatically connect to the newly created cluster.\n\nAs a quick example, we execute a Python command on the Ray Cluster that connects to Ray and exits:\n\n.. code-block:: shell\n\n    $ ray exec config.yaml 'python -c \"import ray; ray.init()\"'\n    2022-08-10 11:23:17,093 INFO worker.py:1312 -- Connecting to existing Ray cluster at address: <remote IP address>:6379...\n    2022-08-10 11:23:17,097 INFO worker.py:1490 -- Connected to Ray cluster.\n\nYou can also optionally get a remote shell using ``ray attach`` and run commands directly on the cluster. This command will create an SSH connection to the head node of the Ray Cluster.\n\n.. code-block:: shell\n\n    # From a remote client:\n    $ ray attach config.yaml\n\n    # Now on the head node...\n    $ python -c \"import ray; ray.init()\"\n\nFor a full reference on the Ray Cluster CLI tools, please refer to :ref:`the cluster commands reference <cluster-commands>`.\n\nWhile these tools are useful for ad-hoc execution on the Ray Cluster, the recommended way to execute an application on a Ray Cluster is to use :ref:`Ray Jobs <jobs-quickstart>`. Check out the :ref:`quickstart guide <jobs-quickstart>` to get started!\n\nDeleting a Ray Cluster\n----------------------\n\nTo shut down your cluster, run the following command:\n\n.. code-block:: shell\n\n    $ ray down -y config.yaml", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6392923d-dbf2-4a53-a55c-f4f14353a2ff": {"__data__": {"id_": "6392923d-dbf2-4a53-a55c-f4f14353a2ff", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/index.md", "file_name": "index.md", "text_hash": "1f362d5cac986f1b087e9fc353b22717"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "79b3017bdc6c936583e006b5d4ebbc6c65c8adcb", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/index.md", "file_name": "index.md"}, "hash": "a6ba6e8d6c23d756a1108439fc0788a961817b16d32b208dde4d7d4324daf50a"}}, "hash": "c7670b4ee63e05079085696ca0243d0c66e2a20ac4c5c95f40c54e099463fd33", "text": "# Ray on Cloud VMs\n(cloud-vm-index)=\n\n## Overview\n\nIn this section we cover how to launch Ray clusters on Cloud VMs. Ray ships with built-in support\nfor launching AWS and GCP clusters, and also has community-maintained integrations for Azure, Aliyun and vSphere.\nEach Ray cluster consists of a head node and a collection of worker nodes. Optional\n[autoscaling](vms-autoscaling) support allows the Ray cluster to be sized according to the\nrequirements of your Ray workload, adding and removing worker nodes as needed. Ray supports\nclusters composed of multiple heterogeneous compute nodes (including GPU nodes).\n\nConcretely, you will learn how to:\n\n- Set up and configure Ray in public clouds\n- Deploy applications and monitor your cluster\n\n## Learn More\n\nThe Ray docs present all the information you need to start running Ray workloads on VMs.\n\n```{eval-rst}\n.. grid:: 1 2 2 2\n    :gutter: 1\n    :class-container: container pb-3\n    \n    .. grid-item-card::\n    \n        **Getting Started**\n        ^^^\n    \n        Learn how to start a Ray cluster and deploy Ray applications in the cloud.\n    \n        +++\n        .. button-ref:: vm-cluster-quick-start\n            :color: primary\n            :outline:\n            :expand:\n\n            Get Started with Ray on Cloud VMs\n    \n    .. grid-item-card::\n\n        **Examples**\n        ^^^\n    \n        Try example Ray workloads in the Cloud\n    \n        +++\n        .. button-ref:: vm-cluster-examples\n            :color: primary\n            :outline:\n            :expand:\n\n            Try example workloads\n    \n    .. grid-item-card::\n\n        **User Guides**\n        ^^^\n    \n        Learn best practices for configuring cloud clusters\n    \n        +++\n        .. button-ref:: vm-cluster-guides\n            :color: primary\n            :outline:\n            :expand:\n\n            Read the User Guides\n    \n    .. grid-item-card::\n\n        **API Reference**\n        ^^^\n    \n        Find API references for cloud clusters\n    \n        +++\n        .. button-ref:: vm-cluster-api-references\n            :color: primary\n            :outline:\n            :expand:\n\n            Check API references\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8321234e-5b11-4ca6-b7cd-6351ccdedd3c": {"__data__": {"id_": "8321234e-5b11-4ca6-b7cd-6351ccdedd3c", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/index.md", "file_name": "index.md", "text_hash": "30e4027926ca7711ef4bbadaf7f06cd4"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "42f28b2ddc1835b66055b3fcf000b0f239bfa174", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/index.md", "file_name": "index.md"}, "hash": "6fbfaf22a1062e28d2076f14debecf05f1ded536057b6792d7df14f3089ff507"}}, "hash": "3dd1c4b2a2fadb08047f9ca3d13e44712947ac8f400383c14dc1ed21133be57a", "text": "(vm-cluster-api-references)=\n\n# API References\n\nThe following pages provide reference documentation for using Ray Clusters on virtual machines.\n\n```{toctree}\n:caption: \"Reference documentation for Ray Clusters on VMs:\"\n:maxdepth: '2'\n:name: ray-clusters-vms-reference\n\nray-cluster-cli\nray-cluster-configuration\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e4f9a169-2d53-4e4c-a08f-335d79196592": {"__data__": {"id_": "e4f9a169-2d53-4e4c-a08f-335d79196592", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "699852c4e782c4911bcf3db826287f5e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst"}, "hash": "07190f1b5fe1232ca4562ee21d7b87819510e4fe25dd49bac39283f5bc16637b"}, "3": {"node_id": "e795e777-11ba-42a7-a975-9316b7469ddd", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "e7b6567077facc4f62f59edb88ac167c"}, "hash": "266578864b00b19dd2f151718ba20acd80a43443e4602577ec1208c3a2919434"}}, "hash": "533ae5ccf4c7f8630b0161c78f2a9d3d8808fdbd49bc39b3952f2444b37db104", "text": ".. _cluster-commands:\n\nCluster Launcher Commands\n=========================\n\nThis document overviews common commands for using the Ray cluster launcher.\nSee the :ref:`Cluster Configuration <cluster-config>` docs on how to customize the configuration file.\n\nLaunching a cluster (``ray up``)\n--------------------------------\n\nThis will start up the machines in the cloud, install your dependencies and run\nany setup commands that you have, configure the Ray cluster automatically, and\nprepare you to scale your distributed system. See :ref:`the documentation\n<ray-up-doc>` for ``ray up``. The example config files can be accessed `here <https://github.com/ray-project/ray/tree/master/python/ray/autoscaler>`_.\n\n.. tip:: The worker nodes will start only after the head node has finished\n         starting. To monitor the progress of the cluster setup, you can run\n         `ray monitor <cluster yaml>`.\n\n.. code-block:: shell\n\n    # Replace '<your_backend>' with one of: 'aws', 'gcp', 'kubernetes', or 'local'.\n    $ BACKEND=<your_backend>\n\n    # Create or update the cluster.\n    $ ray up ray/python/ray/autoscaler/$BACKEND/example-full.yaml\n\n    # Tear down the cluster.\n    $ ray down ray/python/ray/autoscaler/$BACKEND/example-full.yaml\n\nUpdating an existing cluster (``ray up``)\n-----------------------------------------\n\nIf you want to update your cluster configuration (add more files, change dependencies), run ``ray up`` again on the existing cluster.\n\nThis command checks if the local configuration differs from the applied\nconfiguration of the cluster. This includes any changes to synced files\nspecified in the ``file_mounts`` section of the config. If so, the new files\nand config will be uploaded to the cluster. Following that, Ray\nservices/processes will be restarted.\n\n.. tip:: Don't do this for the cloud provider specifications (e.g., change from\n         AWS to GCP on a running cluster) or change the cluster name (as this\n         will just start a new cluster and orphan the original one).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e795e777-11ba-42a7-a975-9316b7469ddd": {"__data__": {"id_": "e795e777-11ba-42a7-a975-9316b7469ddd", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "e7b6567077facc4f62f59edb88ac167c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst"}, "hash": "07190f1b5fe1232ca4562ee21d7b87819510e4fe25dd49bac39283f5bc16637b"}, "2": {"node_id": "e4f9a169-2d53-4e4c-a08f-335d79196592", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "699852c4e782c4911bcf3db826287f5e"}, "hash": "533ae5ccf4c7f8630b0161c78f2a9d3d8808fdbd49bc39b3952f2444b37db104"}, "3": {"node_id": "683f5197-0867-4850-8f4e-a2c6456a3400", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "9ef22973dea1fcc0eb63e77971364379"}, "hash": "c4efc2aa05ecba45585f49091b1bad5e62a8d122ad3eaa33c878a2fa91893d48"}}, "hash": "266578864b00b19dd2f151718ba20acd80a43443e4602577ec1208c3a2919434", "text": "You can also run ``ray up`` to restart a cluster if it seems to be in a bad\nstate (this will restart all Ray services even if there are no config changes).\n\nRunning ``ray up`` on an existing cluster will do all the following:\n\n* If the head node matches the cluster specification, the filemounts will be\n  reapplied and the ``setup_commands`` and ``ray start`` commands will be run.\n  There may be some caching behavior here to skip setup/file mounts.\n* If the head node is out of date from the specified YAML (e.g.,\n  ``head_node_type`` has changed on the YAML), then the out-of-date node will\n  be terminated and a new node will be provisioned to replace it. Setup/file\n  mounts/``ray start`` will be applied.\n* After the head node reaches a consistent state (after ``ray start`` commands\n  are finished), the same above procedure will be applied to all the worker\n  nodes. The ``ray start`` commands tend to run a ``ray stop`` + ``ray start``,\n  so this will kill currently working jobs.\n\nIf you don't want the update to restart services (e.g. because the changes\ndon't require a restart), pass ``--no-restart`` to the update call.\n\nIf you want to force re-generation of the config to pick up possible changes in\nthe cloud environment, pass ``--no-config-cache`` to the update call.\n\nIf you want to skip the setup commands and only run ``ray stop``/``ray start``\non all nodes, pass ``--restart-only`` to the update call.\n\nSee :ref:`the documentation <ray-up-doc>` for ``ray up``.\n\n.. code-block:: shell\n\n    # Reconfigure autoscaling behavior without interrupting running jobs.\n    $ ray up ray/python/ray/autoscaler/$BACKEND/example-full.yaml \\\n        --max-workers=N --no-restart\n\nRunning shell commands on the cluster (``ray exec``)\n----------------------------------------------------\n\nYou can use ``ray exec`` to conveniently run commands on clusters. See :ref:`the documentation <ray-exec-doc>` for ``ray exec``.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "683f5197-0867-4850-8f4e-a2c6456a3400": {"__data__": {"id_": "683f5197-0867-4850-8f4e-a2c6456a3400", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "9ef22973dea1fcc0eb63e77971364379"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst"}, "hash": "07190f1b5fe1232ca4562ee21d7b87819510e4fe25dd49bac39283f5bc16637b"}, "2": {"node_id": "e795e777-11ba-42a7-a975-9316b7469ddd", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "e7b6567077facc4f62f59edb88ac167c"}, "hash": "266578864b00b19dd2f151718ba20acd80a43443e4602577ec1208c3a2919434"}, "3": {"node_id": "897574e7-4c29-4fc1-a44d-1e539cd2bf80", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "bb487436e3a7ff67b24d24fd75b4e48e"}, "hash": "c3e583acc0ae2970c573480f2fbbe97cdfbfd2ae1cb15e4c6fe54b2651f1826c"}}, "hash": "c4efc2aa05ecba45585f49091b1bad5e62a8d122ad3eaa33c878a2fa91893d48", "text": ".. code-block:: shell\n\n    # Run a command on the cluster\n    $ ray exec cluster.yaml 'echo \"hello world\"'\n\n    # Run a command on the cluster, starting it if needed\n    $ ray exec cluster.yaml 'echo \"hello world\"' --start\n\n    # Run a command on the cluster, stopping the cluster after it finishes\n    $ ray exec cluster.yaml 'echo \"hello world\"' --stop\n\n    # Run a command on a new cluster called 'experiment-1', stopping it after\n    $ ray exec cluster.yaml 'echo \"hello world\"' \\\n        --start --stop --cluster-name experiment-1\n\n    # Run a command in a detached tmux session\n    $ ray exec cluster.yaml 'echo \"hello world\"' --tmux\n\n    # Run a command in a screen (experimental)\n    $ ray exec cluster.yaml 'echo \"hello world\"' --screen\n\nIf you want to run applications on the cluster that are accessible from a web\nbrowser (e.g., Jupyter notebook), you can use the ``--port-forward``. The local\nport opened is the same as the remote port.\n\n.. code-block:: shell\n\n    $ ray exec cluster.yaml --port-forward=8899 'source ~/anaconda3/bin/activate tensorflow_p36 && jupyter notebook --port=8899'\n\n.. note:: For Kubernetes clusters, the ``port-forward`` option cannot be used\n          while executing a command. To port forward and run a command you need\n          to call ``ray exec`` twice separately.\n\nRunning Ray scripts on the cluster (``ray submit``)\n---------------------------------------------------\n\nYou can also use ``ray submit`` to execute Python scripts on clusters. This\nwill ``rsync`` the designated file onto the head node cluster and execute it\nwith the given arguments. See :ref:`the documentation <ray-submit-doc>` for\n``ray submit``.\n\n.. code-block:: shell\n\n    # Run a Python script in a detached tmux session\n    $ ray submit cluster.yaml --tmux --start --stop tune_experiment.py\n\n    # Run a Python script with arguments.\n    # This executes script.py on the head node of the cluster, using\n    # the command: python ~/script.py --arg1 --arg2 --arg3\n    $ ray submit cluster.yaml script.py -- --arg1 --arg2 --arg3", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "897574e7-4c29-4fc1-a44d-1e539cd2bf80": {"__data__": {"id_": "897574e7-4c29-4fc1-a44d-1e539cd2bf80", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "bb487436e3a7ff67b24d24fd75b4e48e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst"}, "hash": "07190f1b5fe1232ca4562ee21d7b87819510e4fe25dd49bac39283f5bc16637b"}, "2": {"node_id": "683f5197-0867-4850-8f4e-a2c6456a3400", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "9ef22973dea1fcc0eb63e77971364379"}, "hash": "c4efc2aa05ecba45585f49091b1bad5e62a8d122ad3eaa33c878a2fa91893d48"}}, "hash": "c3e583acc0ae2970c573480f2fbbe97cdfbfd2ae1cb15e4c6fe54b2651f1826c", "text": "Attaching to a running cluster (``ray attach``)\n-----------------------------------------------\n\nYou can use ``ray attach`` to attach to an interactive screen session on the\ncluster. See :ref:`the documentation <ray-attach-doc>` for ``ray attach`` or\nrun ``ray attach --help``.\n\n.. code-block:: shell\n\n    # Open a screen on the cluster\n    $ ray attach cluster.yaml\n\n    # Open a screen on a new cluster called 'session-1'\n    $ ray attach cluster.yaml --start --cluster-name=session-1\n\n    # Attach to tmux session on cluster (creates a new one if none available)\n    $ ray attach cluster.yaml --tmux\n\n.. _ray-rsync:\n\nSynchronizing files from the cluster (``ray rsync-up/down``)\n------------------------------------------------------------\n\nTo download or upload files to the cluster head node, use ``ray rsync_down`` or\n``ray rsync_up``:\n\n.. code-block:: shell\n\n    $ ray rsync_down cluster.yaml '/path/on/cluster' '/local/path'\n    $ ray rsync_up cluster.yaml '/local/path' '/path/on/cluster'\n\n.. _monitor-cluster:\n\nMonitoring cluster status (``ray dashboard/status``)\n-----------------------------------------------------\n\nThe Ray also comes with an online dashboard. The dashboard is accessible via\nHTTP on the head node (by default it listens on ``localhost:8265``). You can\nalso use the built-in ``ray dashboard`` to set up port forwarding\nautomatically, making the remote dashboard viewable in your local browser at\n``localhost:8265``.\n\n.. code-block:: shell\n\n    $ ray dashboard cluster.yaml\n\nYou can monitor cluster usage and auto-scaling status by running (on the head node):\n\n.. code-block:: shell\n\n    $ ray status\n\nTo see live updates to the status:\n\n.. code-block:: shell\n\n    $ watch -n 1 ray status\n\nThe Ray autoscaler also reports per-node status in the form of instance tags.\nIn your cloud provider console, you can click on a Node, go to the \"Tags\" pane,\nand add the ``ray-node-status`` tag as a column. This lets you see per-node\nstatuses at a glance:\n\n.. image:: /images/autoscaler-status.png\n\nCommon Workflow: Syncing git branches\n-------------------------------------\n\nA common use case is syncing a particular local git branch to all workers of\nthe cluster. However, if you just put a `git checkout <branch>` in the setup\ncommands, the autoscaler won't know when to rerun the command to pull in\nupdates. There is a nice workaround for this by including the git SHA in the\ninput (the hash of the file will change if the branch is updated):\n\n.. code-block:: yaml\n\n    file_mounts: {\n        \"/tmp/current_branch_sha\": \"/path/to/local/repo/.git/refs/heads/<YOUR_BRANCH_NAME>\",\n    }\n\n    setup_commands:\n        - test -e <REPO_NAME> || git clone https://github.com/<REPO_ORG>/<REPO_NAME>.git\n        - cd <REPO_NAME> && git fetch && git checkout `cat /tmp/current_branch_sha`\n\nThis tells ``ray up`` to sync the current git branch SHA from your personal\ncomputer to a temporary file on the cluster (assuming you've pushed the branch\nhead already). Then, the setup commands read that file to figure out which SHA\nthey should checkout on the nodes. Note that each command runs in its own\nsession. The final workflow to update the cluster then becomes just this:\n\n1. Make local changes to a git branch\n2. Commit the changes with ``git commit`` and ``git push``\n3. Update files on your Ray cluster with ``ray up``", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4c0eb79-eb7a-4f25-a882-fd5052ab7ae0": {"__data__": {"id_": "c4c0eb79-eb7a-4f25-a882-fd5052ab7ae0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "3963e4e4c4c2e53cd687616868f26778"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "3": {"node_id": "46991261-a958-4d42-826d-fe239c2486dc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "ecb2d03715bfc85a56a1db3608bd2daf"}, "hash": "194f6e641287f6efe0af11f26a893073b10854441e67e1e2af7d601f2c21152a"}}, "hash": "c03a17f959f2910c39b9dbbe737db7fa58e1f65b3306e38c74811358323ab9f3", "text": ".. _cluster-config:\n\nCluster YAML Configuration Options\n==================================\n\nThe cluster configuration is defined within a YAML file that will be used by the Cluster Launcher to launch the head node, and by the Autoscaler to launch worker nodes.Once the cluster configuration is defined, you will need to use the :ref:`Ray CLI <ray-cluster-cli>` to perform any operations such as starting and stopping the cluster.Syntax\n------\n\n. parsed-literal::\n\n    :ref:`cluster_name <cluster-configuration-cluster-name>`: str\n    :ref:`max_workers <cluster-configuration-max-workers>`: int\n    :ref:`upscaling_speed <cluster-configuration-upscaling-speed>`: float\n    :ref:`idle_timeout_minutes <cluster-configuration-idle-timeout-minutes>`: int\n    :ref:`docker <cluster-configuration-docker>`:\n        :ref:`docker <cluster-configuration-docker-type>`\n    :ref:`provider <cluster-configuration-provider>`:\n        :ref:`provider <cluster-configuration-provider-type>`\n    :ref:`auth <cluster-configuration-auth>`:\n        :ref:`auth <cluster-configuration-auth-type>`\n    :ref:`available_node_types <cluster-configuration-available-node-types>`:\n        :ref:`node_types <cluster-configuration-node-types-type>`\n    :ref:`head_node_type <cluster-configuration-head-node-type>`: str\n    :ref:`file_mounts <cluster-configuration-file-mounts>`:\n        :ref:`file_mounts <cluster-configuration-file-mounts-type>`\n    :ref:`cluster_synced_files <cluster-configuration-cluster-synced-files>`:\n        - str\n    :ref:`rsync_exclude <cluster-configuration-rsync-exclude>`:\n        - str\n    :ref:`rsync_filter <cluster-configuration-rsync-filter>`:\n        - str\n    :ref:`initialization_commands <cluster-configuration-initialization-commands>`:\n        - str\n    :ref:`setup_commands <cluster-configuration-setup-commands>`:\n        - str\n    :ref:`head_setup_commands <cluster-configuration-head-setup-commands>`:\n        - str\n    :ref:`worker_setup_commands <cluster-configuration-worker-setup-commands>`:\n        - str\n    :ref:`head_start_ray_commands <cluster-configuration-head-start-ray-commands>`:\n        - str\n    :ref:`worker_start_ray_commands <cluster-configuration-worker-start-ray-commands>`:\n        - str\n\nCustom types\n------------\n\n. _cluster-configuration-docker-type:\n\nDocker\n~~~~~~\n\n.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "46991261-a958-4d42-826d-fe239c2486dc": {"__data__": {"id_": "46991261-a958-4d42-826d-fe239c2486dc", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "ecb2d03715bfc85a56a1db3608bd2daf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "c4c0eb79-eb7a-4f25-a882-fd5052ab7ae0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "3963e4e4c4c2e53cd687616868f26778"}, "hash": "c03a17f959f2910c39b9dbbe737db7fa58e1f65b3306e38c74811358323ab9f3"}, "3": {"node_id": "a71514b5-1b95-47f4-855e-beb1f35ee24a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "298c03207565b27d6f098267c72fe388"}, "hash": "882388a8fd22bff2a17f7329a2352de75fbee5cd16034ee8a2f05d5dbfac8ef7"}}, "hash": "194f6e641287f6efe0af11f26a893073b10854441e67e1e2af7d601f2c21152a", "text": "parsed-literal::\n    :ref:`image <cluster-configuration-image>`: str\n    :ref:`head_image <cluster-configuration-head-image>`: str\n    :ref:`worker_image <cluster-configuration-worker-image>`: str\n    :ref:`container_name <cluster-configuration-container-name>`: str\n    :ref:`pull_before_run <cluster-configuration-pull-before-run>`: bool\n    :ref:`run_options <cluster-configuration-run-options>`:\n        - str\n    :ref:`head_run_options <cluster-configuration-head-run-options>`:\n        - str\n    :ref:`worker_run_options <cluster-configuration-worker-run-options>`:\n        - str\n    :ref:`disable_automatic_runtime_detection <cluster-configuration-disable-automatic-runtime-detection>`: bool\n    :ref:`disable_shm_size_detection <cluster-configuration-disable-shm-size-detection>`: bool\n\n. _cluster-configuration-auth-type:\n\nAuth\n~~~~\n\n. tab-set::\n\n    . tab-item:: AWS\n\n        . parsed-literal::\n\n            :ref:`ssh_user <cluster-configuration-ssh-user>`: str\n            :ref:`ssh_private_key <cluster-configuration-ssh-private-key>`: str\n\n    . tab-item:: Azure\n\n        . parsed-literal::\n\n            :ref:`ssh_user <cluster-configuration-ssh-user>`: str\n            :ref:`ssh_private_key <cluster-configuration-ssh-private-key>`: str\n            :ref:`ssh_public_key <cluster-configuration-ssh-public-key>`: str\n\n    . tab-item:: GCP\n\n        . parsed-literal::\n\n            :ref:`ssh_user <cluster-configuration-ssh-user>`: str\n            :ref:`ssh_private_key <cluster-configuration-ssh-private-key>`: str\n\n. _cluster-configuration-provider-type:\n\nProvider\n~~~~~~~~\n\n. tab-set::\n\n    . tab-item:: AWS\n\n        . parsed-literal::\n\n            :ref:`type <cluster-configuration-type>`: str\n            :ref:`region <cluster-configuration-region>`: str\n            :ref:`availability_zone <cluster-configuration-availability-zone>`: str\n            :ref:`cache_stopped_nodes <cluster-configuration-cache-stopped-nodes>`: bool\n            :ref:`security_group <cluster-configuration-security-group>`:\n                :ref:`Security Group <cluster-configuration-security-group-type>`\n            :ref:`use_internal_ips <cluster-configuration-use-internal-ips>`: bool\n\n    . tab-item:: Azure\n\n        .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a71514b5-1b95-47f4-855e-beb1f35ee24a": {"__data__": {"id_": "a71514b5-1b95-47f4-855e-beb1f35ee24a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "298c03207565b27d6f098267c72fe388"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "46991261-a958-4d42-826d-fe239c2486dc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "ecb2d03715bfc85a56a1db3608bd2daf"}, "hash": "194f6e641287f6efe0af11f26a893073b10854441e67e1e2af7d601f2c21152a"}, "3": {"node_id": "45310706-fff1-46cf-a38f-9489efa2d14f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "6547a4dda3156a213decc1aaeca32c9e"}, "hash": "9fce3c0431f29fcc3cea780317bfe370a10febf456f52f4737dc9d31c3225c33"}}, "hash": "882388a8fd22bff2a17f7329a2352de75fbee5cd16034ee8a2f05d5dbfac8ef7", "text": "parsed-literal::\n\n            :ref:`type <cluster-configuration-type>`: str\n            :ref:`location <cluster-configuration-location>`: str\n            :ref:`resource_group <cluster-configuration-resource-group>`: str\n            :ref:`subscription_id <cluster-configuration-subscription-id>`: str\n            :ref:`cache_stopped_nodes <cluster-configuration-cache-stopped-nodes>`: bool\n            :ref:`use_internal_ips <cluster-configuration-use-internal-ips>`: bool\n\n    . tab-item:: GCP\n\n        . parsed-literal::\n\n            :ref:`type <cluster-configuration-type>`: str\n            :ref:`region <cluster-configuration-region>`: str\n            :ref:`availability_zone <cluster-configuration-availability-zone>`: str\n            :ref:`project_id <cluster-configuration-project-id>`: str\n            :ref:`cache_stopped_nodes <cluster-configuration-cache-stopped-nodes>`: bool\n            :ref:`use_internal_ips <cluster-configuration-use-internal-ips>`: bool\n\n. _cluster-configuration-security-group-type:\n\nSecurity Group\n~~~~~~~~~~~~~~\n\n. tab-set::\n\n    . tab-item:: AWS\n\n        . parsed-literal::\n\n            :ref:`GroupName <cluster-configuration-group-name>`: str\n            :ref:`IpPermissions <cluster-configuration-ip-permissions>`:\n                - `IpPermission <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_IpPermission.html>`_\n\n. _cluster-configuration-node-types-type:\n\nNode types\n~~~~~~~~~~\n\nThe ``available_nodes_types`` object's keys represent the names of the different node types.Deleting a node type from ``available_node_types`` and updating with :ref:`ray up <ray-up-doc>` will cause the autoscaler to scale down all nodes of that type.In particular, changing the key of a node type object will\nresult in removal of nodes corresponding to the old key; nodes with the new key name will then be\ncreated according to cluster configuration and Ray resource demands.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "45310706-fff1-46cf-a38f-9489efa2d14f": {"__data__": {"id_": "45310706-fff1-46cf-a38f-9489efa2d14f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "6547a4dda3156a213decc1aaeca32c9e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "a71514b5-1b95-47f4-855e-beb1f35ee24a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "298c03207565b27d6f098267c72fe388"}, "hash": "882388a8fd22bff2a17f7329a2352de75fbee5cd16034ee8a2f05d5dbfac8ef7"}, "3": {"node_id": "81e5446d-43ec-483e-afbb-98275bbda71e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "31839a74a48db9207ce5e7680d116dbf"}, "hash": "1bc40e37691303f5ba4538b7bfc528831a7a3e6cf83bee00d0af834b9b2d224b"}}, "hash": "9fce3c0431f29fcc3cea780317bfe370a10febf456f52f4737dc9d31c3225c33", "text": ".. parsed-literal::\n    <node_type_1_name>:\n        :ref:`node_config <cluster-configuration-node-config>`:\n            :ref:`Node config <cluster-configuration-node-config-type>`\n        :ref:`resources <cluster-configuration-resources>`:\n            :ref:`Resources <cluster-configuration-resources-type>`\n        :ref:`min_workers <cluster-configuration-node-min-workers>`: int\n        :ref:`max_workers <cluster-configuration-node-max-workers>`: int\n        :ref:`worker_setup_commands <cluster-configuration-node-type-worker-setup-commands>`:\n            - str\n        :ref:`docker <cluster-configuration-node-docker>`:\n            :ref:`Node Docker <cluster-configuration-node-docker-type>`\n    <node_type_2_name>:\n        ...\n    ...\n\n.. _cluster-configuration-node-config-type:\n\nNode config\n~~~~~~~~~~~\n\nCloud-specific configuration for nodes of a given node type.Modifying the ``node_config`` and updating with :ref:`ray up <ray-up-doc>` will cause the autoscaler to scale down all existing nodes of the node type;\nnodes with the newly applied ``node_config`` will then be created according to cluster configuration and Ray resource demands... tab-set::\n\n    .. tab-item:: AWS\n\n        A YAML object which conforms to the EC2 ``create_instances`` API in `the AWS docs <https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/ec2.html#EC2.ServiceResource.create_instances>`_... tab-item:: Azure\n\n        A YAML object as defined in `the deployment template <https://docs.microsoft.com/en-us/azure/templates/microsoft.compute/virtualmachines>`_ whose resources are defined in `the Azure docs <https://docs.microsoft.com/en-us/azure/templates/>`_... tab-item:: GCP\n\n        A YAML object as defined in `the GCP docs <https://cloud.google.com/compute/docs/reference/rest/v1/instances>`_.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "81e5446d-43ec-483e-afbb-98275bbda71e": {"__data__": {"id_": "81e5446d-43ec-483e-afbb-98275bbda71e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "31839a74a48db9207ce5e7680d116dbf"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "45310706-fff1-46cf-a38f-9489efa2d14f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "6547a4dda3156a213decc1aaeca32c9e"}, "hash": "9fce3c0431f29fcc3cea780317bfe370a10febf456f52f4737dc9d31c3225c33"}, "3": {"node_id": "319705b4-0fe4-4341-aa1e-0976a4a3262b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "ae34bd4f880a27d16626e5694d7c48c6"}, "hash": "85df16b59d6e52d8561501a9e0f5ff96339577cd5ca97a922daaa780aa393074"}}, "hash": "1bc40e37691303f5ba4538b7bfc528831a7a3e6cf83bee00d0af834b9b2d224b", "text": ".. _cluster-configuration-node-docker-type:\n\nNode Docker\n~~~~~~~~~~~\n\n.. parsed-literal::\n\n    :ref:`worker_image <cluster-configuration-image>`: str\n    :ref:`pull_before_run <cluster-configuration-pull-before-run>`: bool\n    :ref:`worker_run_options <cluster-configuration-worker-run-options>`:\n        - str\n    :ref:`disable_automatic_runtime_detection <cluster-configuration-disable-automatic-runtime-detection>`: bool\n    :ref:`disable_shm_size_detection <cluster-configuration-disable-shm-size-detection>`: bool\n\n.. _cluster-configuration-resources-type:\n\nResources\n~~~~~~~~~\n\n.. parsed-literal::\n\n    :ref:`CPU <cluster-configuration-CPU>`: int\n    :ref:`GPU <cluster-configuration-GPU>`: int\n    :ref:`object_store_memory <cluster-configuration-object-store-memory>`: int\n    :ref:`memory <cluster-configuration-memory>`: int\n    <custom_resource1>: int\n    <custom_resource2>: int\n    ...\n\n.. _cluster-configuration-file-mounts-type:\n\nFile mounts\n~~~~~~~~~~~\n\n.. parsed-literal::\n    <path1_on_remote_machine>: str # Path 1 on local machine\n    <path2_on_remote_machine>: str # Path 2 on local machine\n    ...\n\nProperties and Definitions\n--------------------------\n\n.. _cluster-configuration-cluster-name:\n\n``cluster_name``\n~~~~~~~~~~~~~~~~\n\nThe name of the cluster.This is the namespace of the cluster.* **Required:** Yes\n* **Importance:** High\n* **Type:** String\n* **Default:** \"default\"\n* **Pattern:** ``[a-zA-Z0-9_]+``\n\n.. _cluster-configuration-max-workers:\n\n``max_workers``\n~~~~~~~~~~~~~~~\n\nThe maximum number of workers the cluster will have at any given time.* **Required:** No\n* **Importance:** High\n* **Type:** Integer\n* **Default:** ``2``\n* **Minimum:** ``0``\n* **Maximum:** Unbounded\n\n.. _cluster-configuration-upscaling-speed:\n\n``upscaling_speed``\n~~~~~~~~~~~~~~~~~~~\n\nThe number of nodes allowed to be pending as a multiple of the current number of nodes.For example, if set to 1.0, the cluster can grow in size by at most 100% at any time, so if the cluster currently has 20 nodes, at most 20 pending launches are allowed.Note that although the autoscaler will scale down to `min_workers` (which could be 0), it will always scale up to 5 nodes at a minimum when scaling up.* **Required:** No\n* **Importance:** Medium\n* **Type:** Float\n* **Default:** ``1.0``\n* **Minimum:** ``0.0``\n* **Maximum:** Unbounded\n\n.. _cluster-configuration-idle-timeout-minutes:\n\n``idle_timeout_minutes``\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe number of minutes that need to pass before an idle worker node is removed by the Autoscaler.* **Required:** No\n* **Importance:** Medium\n* **Type:** Integer\n* **Default:** ``5``\n* **Minimum:** ``0``\n* **Maximum:** Unbounded\n\n.. _cluster-configuration-docker:\n\n``docker``\n~~~~~~~~~~\n\nConfigure Ray to run in Docker containers.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "319705b4-0fe4-4341-aa1e-0976a4a3262b": {"__data__": {"id_": "319705b4-0fe4-4341-aa1e-0976a4a3262b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "ae34bd4f880a27d16626e5694d7c48c6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "81e5446d-43ec-483e-afbb-98275bbda71e", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "31839a74a48db9207ce5e7680d116dbf"}, "hash": "1bc40e37691303f5ba4538b7bfc528831a7a3e6cf83bee00d0af834b9b2d224b"}, "3": {"node_id": "351ff147-587a-45b7-8fe8-355ec7db3253", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "fc7e853859f28b254a26959698ff823b"}, "hash": "7dfebbe0a4e13dc56b79bc72f260477455a428b57ecae7de51549903f1ca3924"}}, "hash": "85df16b59d6e52d8561501a9e0f5ff96339577cd5ca97a922daaa780aa393074", "text": "* **Required:** No\n* **Importance:** High\n* **Type:** :ref:`Docker <cluster-configuration-docker-type>`\n* **Default:** ``{}``\n\nIn rare cases when Docker is not available on the system by default (e.g., bad AMI), add the following commands to :ref:`initialization_commands <cluster-configuration-initialization-commands>` to install it... code-block:: yaml\n\n    initialization_commands:\n        - curl -fsSL https://get.docker.com -o get-docker.sh\n        - sudo sh get-docker.sh\n        - sudo usermod -aG docker $USER\n        - sudo systemctl restart docker -f\n\n.. _cluster-configuration-provider:\n\n``provider``\n~~~~~~~~~~~~\n\nThe cloud provider-specific configuration properties.* **Required:** Yes\n* **Importance:** High\n* **Type:** :ref:`Provider <cluster-configuration-provider-type>`\n\n.. _cluster-configuration-auth:\n\n``auth``\n~~~~~~~~\n\nAuthentication credentials that Ray will use to launch nodes.* **Required:** Yes\n* **Importance:** High\n* **Type:** :ref:`Auth <cluster-configuration-auth-type>`\n\n.. _cluster-configuration-available-node-types:\n\n``available_node_types``\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nTells the autoscaler the allowed node types and the resources they provide.Each node type is identified by a user-specified key.* **Required:** No\n* **Importance:** High\n* **Type:** :ref:`Node types <cluster-configuration-node-types-type>`\n* **Default:**\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        .. code-block:: yaml\n\n          available_node_types:\n            ray.head.default:\n                node_config:\n                  InstanceType: m5.large\n                  BlockDeviceMappings:\n                      - DeviceName: /dev/sda1\n                        Ebs:\n                            VolumeSize: 140\n                resources: {\"CPU\": 2}\n            ray.worker.default:\n                node_config:\n                  InstanceType: m5.large\n                  InstanceMarketOptions:\n                      MarketType: spot\n                resources: {\"CPU\": 2}\n                min_workers: 0\n\n.. _cluster-configuration-head-node-type:\n\n``head_node_type``\n~~~~~~~~~~~~~~~~~~\n\nThe key for one of the node types in :ref:`available_node_types <cluster-configuration-available-node-types>`.This node type will be used to launch the head node.If the field ``head_node_type`` is changed and an update is executed with :ref:`ray up <ray-up-doc>`, the currently running head node will\nbe considered outdated.The user will receive a prompt asking to confirm scale-down of the outdated head node, and the cluster will restart with a new\nhead node.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "351ff147-587a-45b7-8fe8-355ec7db3253": {"__data__": {"id_": "351ff147-587a-45b7-8fe8-355ec7db3253", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "fc7e853859f28b254a26959698ff823b"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "319705b4-0fe4-4341-aa1e-0976a4a3262b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "ae34bd4f880a27d16626e5694d7c48c6"}, "hash": "85df16b59d6e52d8561501a9e0f5ff96339577cd5ca97a922daaa780aa393074"}, "3": {"node_id": "71701ef5-3759-4b70-91a3-fc309d1c1543", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "15843d714dab62269c2fa094f7ef1b42"}, "hash": "e95f6d2e2031ebfb7b050fbe26555a1b0400258f1dc3e24bb78cd0068e297d74"}}, "hash": "7dfebbe0a4e13dc56b79bc72f260477455a428b57ecae7de51549903f1ca3924", "text": "Changing the :ref:`node_config<cluster-configuration-node-config>` of the :ref:`node_type<cluster-configuration-node-types-type>` with key ``head_node_type`` will also result in cluster restart after a user prompt.\n\n\n\n* **Required:** Yes\n* **Importance:** High\n* **Type:** String\n* **Pattern:** ``[a-zA-Z0-9_]+``\n\n.. _cluster-configuration-file-mounts:\n\n``file_mounts``\n~~~~~~~~~~~~~~~\n\nThe files or directories to copy to the head and worker nodes.* **Required:** No\n* **Importance:** High\n* **Type:** :ref:`File mounts <cluster-configuration-file-mounts-type>`\n* **Default:** ``[]``\n\n.. _cluster-configuration-cluster-synced-files:\n\n``cluster_synced_files``\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nA list of paths to the files or directories to copy from the head node to the worker nodes.The same path on the head node will be copied to the worker node.This behavior is a subset of the file_mounts behavior, so in the vast majority of cases one should just use :ref:`file_mounts <cluster-configuration-file-mounts>`.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-rsync-exclude:\n\n``rsync_exclude``\n~~~~~~~~~~~~~~~~~\n\nA list of patterns for files to exclude when running ``rsync up`` or ``rsync down``.The filter is applied on the source directory only.Example for a pattern in the list: ``**/.git/**``.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-rsync-filter:\n\n``rsync_filter``\n~~~~~~~~~~~~~~~~\n\nA list of patterns for files to exclude when running ``rsync up`` or ``rsync down``.The filter is applied on the source directory and recursively through all subdirectories.Example for a pattern in the list: ``.gitignore``.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-initialization-commands:\n\n``initialization_commands``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA list of commands that will be run before the :ref:`setup commands <cluster-configuration-setup-commands>`.If Docker is enabled, these commands will run outside the container and before Docker is setup.* **Required:** No\n* **Importance:** Medium\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-setup-commands:\n\n``setup_commands``\n~~~~~~~~~~~~~~~~~~\n\nA list of commands to run to set up nodes.These commands will always run on the head and worker nodes and will be merged with :ref:`head setup commands <cluster-configuration-head-setup-commands>` for head and with :ref:`worker setup commands <cluster-configuration-worker-setup-commands>` for workers.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "71701ef5-3759-4b70-91a3-fc309d1c1543": {"__data__": {"id_": "71701ef5-3759-4b70-91a3-fc309d1c1543", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "15843d714dab62269c2fa094f7ef1b42"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "351ff147-587a-45b7-8fe8-355ec7db3253", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "fc7e853859f28b254a26959698ff823b"}, "hash": "7dfebbe0a4e13dc56b79bc72f260477455a428b57ecae7de51549903f1ca3924"}, "3": {"node_id": "57d9be46-033b-4cdf-95cb-b5d4e872d4ce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "6c18552909d4e67dc16b8136acc4c012"}, "hash": "0b17b515ab930312fe2da46ace73f59d405dff5b33efbcba46c9e6492d58c0db"}}, "hash": "e95f6d2e2031ebfb7b050fbe26555a1b0400258f1dc3e24bb78cd0068e297d74", "text": "* **Required:** No\n* **Importance:** Medium\n* **Type:** List of String\n* **Default:**\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        .. code-block:: yaml\n\n            # Default setup_commands:\n            setup_commands:\n              - echo 'export PATH=\"$HOME/anaconda3/envs/tensorflow_p36/bin:$PATH\"' >> ~/.bashrc\n              - pip install -U https://s3-us-west-2.amazonaws.com/ray-wheels/latest/ray-3.0.0.dev0-cp37-cp37m-manylinux2014_x86_64.whl\n\n- Setup commands should ideally be *idempotent* (i.e., can be run multiple times without changing the result); this allows Ray to safely update nodes after they have been created.You can usually make commands idempotent with small modifications, e.g.``git clone foo`` can be rewritten as ``test -e foo || git clone foo`` which checks if the repo is already cloned first.- Setup commands are run sequentially but separately.For example, if you are using anaconda, you need to run ``conda activate env && pip install -U ray`` because splitting the command into two setup commands will not work.- Ideally, you should avoid using setup_commands by creating a docker image with all the dependencies preinstalled to minimize startup time.- **Tip**: if you also want to run apt-get commands during setup add the following list of commands:\n\n    .. code-block:: yaml\n\n        setup_commands:\n          - sudo pkill -9 apt-get || true\n          - sudo pkill -9 dpkg || true\n          - sudo dpkg --configure -a\n\n.. _cluster-configuration-head-setup-commands:\n\n``head_setup_commands``\n~~~~~~~~~~~~~~~~~~~~~~~\n\nA list of commands to run to set up the head node.These commands will be merged with the general :ref:`setup commands <cluster-configuration-setup-commands>`.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-worker-setup-commands:\n\n``worker_setup_commands``\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA list of commands to run to set up the worker nodes.These commands will be merged with the general :ref:`setup commands <cluster-configuration-setup-commands>`.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-head-start-ray-commands:\n\n``head_start_ray_commands``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCommands to start ray on the head node.You don't need to change this.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:**\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        .. code-block:: yaml\n\n            head_start_ray_commands:\n              - ray stop\n              - ulimit -n 65536; ray start --head --port=6379 --object-manager-port=8076 --autoscaling-config=~/ray_bootstrap_config.yaml\n\n.. _cluster-configuration-worker-start-ray-commands:\n\n``worker_start_ray_commands``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCommand to start ray on worker nodes.You don't need to change this.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "57d9be46-033b-4cdf-95cb-b5d4e872d4ce": {"__data__": {"id_": "57d9be46-033b-4cdf-95cb-b5d4e872d4ce", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "6c18552909d4e67dc16b8136acc4c012"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "71701ef5-3759-4b70-91a3-fc309d1c1543", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "15843d714dab62269c2fa094f7ef1b42"}, "hash": "e95f6d2e2031ebfb7b050fbe26555a1b0400258f1dc3e24bb78cd0068e297d74"}, "3": {"node_id": "3bed3ea9-56b4-4a13-ba76-48839b19cbcc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "36aeb503d12c47ef682885c8065f41c6"}, "hash": "6ddbd643058b8fbffb952102b156e83252b4dc6099f44cc4a4382862d99bd0f5"}}, "hash": "0b17b515ab930312fe2da46ace73f59d405dff5b33efbcba46c9e6492d58c0db", "text": "* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:**\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        .. code-block:: yaml\n\n            worker_start_ray_commands:\n              - ray stop\n              - ulimit -n 65536; ray start --address=$RAY_HEAD_IP:6379 --object-manager-port=8076\n\n.. _cluster-configuration-image:\n\n``docker.image``\n~~~~~~~~~~~~~~~~\n\nThe default Docker image to pull in the head and worker nodes.This can be overridden by the :ref:`head_image <cluster-configuration-head-image>` and :ref:`worker_image <cluster-configuration-worker-image>` fields.If neither `image` nor (:ref:`head_image <cluster-configuration-head-image>` and :ref:`worker_image <cluster-configuration-worker-image>`) are specified, Ray will not use Docker.* **Required:** Yes (If Docker is in use.)* **Importance:** High\n* **Type:** String\n\nThe Ray project provides Docker images on `DockerHub <https://hub.docker.com/u/rayproject>`_.The repository includes following images:\n\n* ``rayproject/ray-ml:latest-gpu``: CUDA support, includes ML dependencies.* ``rayproject/ray:latest-gpu``: CUDA support, no ML dependencies.* ``rayproject/ray-ml:latest``: No CUDA support, includes ML dependencies.* ``rayproject/ray:latest``: No CUDA support, no ML dependencies... _cluster-configuration-head-image:\n\n``docker.head_image``\n~~~~~~~~~~~~~~~~~~~~~\nDocker image for the head node to override the default :ref:`docker image <cluster-configuration-image>`.* **Required:** No\n* **Importance:** Low\n* **Type:** String\n\n.. _cluster-configuration-worker-image:\n\n``docker.worker_image``\n~~~~~~~~~~~~~~~~~~~~~~~\nDocker image for the worker nodes to override the default :ref:`docker image <cluster-configuration-image>`.* **Required:** No\n* **Importance:** Low\n* **Type:** String\n\n.. _cluster-configuration-container-name:\n\n``docker.container_name``\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe name to use when starting the Docker container.* **Required:** Yes (If Docker is in use.)* **Importance:** Low\n* **Type:** String\n* **Default:** ray_container\n\n.. _cluster-configuration-pull-before-run:\n\n``docker.pull_before_run``\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf enabled, the latest version of image will be pulled when starting Docker.If disabled, ``docker run`` will only pull the image if no cached version is present.* **Required:** No\n* **Importance:** Medium\n* **Type:** Boolean\n* **Default:** ``True``\n\n.. _cluster-configuration-run-options:\n\n``docker.run_options``\n~~~~~~~~~~~~~~~~~~~~~~\n\nThe extra options to pass to ``docker run``.* **Required:** No\n* **Importance:** Medium\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-head-run-options:\n\n``docker.head_run_options``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe extra options to pass to ``docker run`` for head node only.* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-worker-run-options:\n\n``docker.worker_run_options``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe extra options to pass to ``docker run`` for worker nodes only.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3bed3ea9-56b4-4a13-ba76-48839b19cbcc": {"__data__": {"id_": "3bed3ea9-56b4-4a13-ba76-48839b19cbcc", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "36aeb503d12c47ef682885c8065f41c6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "57d9be46-033b-4cdf-95cb-b5d4e872d4ce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "6c18552909d4e67dc16b8136acc4c012"}, "hash": "0b17b515ab930312fe2da46ace73f59d405dff5b33efbcba46c9e6492d58c0db"}, "3": {"node_id": "10f315cd-09be-4e10-bba5-db96872ae438", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "66964824bd1d74cc0fafae0ff47ebe1a"}, "hash": "0dd66b55f03857df740ba412de4e4c1d69eba4ec67066f405ac4a8c9925db370"}}, "hash": "6ddbd643058b8fbffb952102b156e83252b4dc6099f44cc4a4382862d99bd0f5", "text": "* **Required:** No\n* **Importance:** Low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-disable-automatic-runtime-detection:\n\n``docker.disable_automatic_runtime_detection``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf enabled, Ray will not try to use the NVIDIA Container Runtime if GPUs are present.* **Required:** No\n* **Importance:** Low\n* **Type:** Boolean\n* **Default:** ``False``\n\n\n.. _cluster-configuration-disable-shm-size-detection:\n\n``docker.disable_shm_size_detection``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf enabled, Ray will not automatically specify the size ``/dev/shm`` for the started container and the runtime's default value (64MiB for Docker) will be used.\nIf ``--shm-size=<>`` is manually added to ``run_options``, this is *automatically* set to ``True``, meaning that Ray will defer to the user-provided value.\n\n* **Required:** No\n* **Importance:** Low\n* **Type:** Boolean\n* **Default:** ``False``\n\n\n.. _cluster-configuration-ssh-user:\n\n``auth.ssh_user``\n~~~~~~~~~~~~~~~~~\n\nThe user that Ray will authenticate with when launching new nodes.* **Required:** Yes\n* **Importance:** High\n* **Type:** String\n\n.. _cluster-configuration-ssh-private-key:\n\n``auth.ssh_private_key``\n~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The path to an existing private key for Ray to use.If not configured, Ray will create a new private keypair (default behavior).If configured, the key must be added to the project-wide metadata and ``KeyName`` has to be defined in the :ref:`node configuration <cluster-configuration-node-config>`.* **Required:** No\n        * **Importance:** Low\n        * **Type:** String\n\n    .. tab-item:: Azure\n\n        The path to an existing private key for Ray to use.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n\n        You may use ``ssh-keygen -t rsa -b 4096`` to generate a new ssh keypair... tab-item:: GCP\n\n        The path to an existing private key for Ray to use.If not configured, Ray will create a new private keypair (default behavior).If configured, the key must be added to the project-wide metadata and ``KeyName`` has to be defined in the :ref:`node configuration <cluster-configuration-node-config>`.* **Required:** No\n        * **Importance:** Low\n        * **Type:** String\n\n.. _cluster-configuration-ssh-public-key:\n\n``auth.ssh_public_key``\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        Not available... tab-item:: Azure\n\n        The path to an existing public key for Ray to use.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n\n    .. tab-item:: GCP\n\n        Not available... _cluster-configuration-type:\n\n``provider.type``\n~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The cloud service provider.For AWS, this must be set to ``aws``.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n\n    .. tab-item:: Azure\n\n        The cloud service provider.For Azure, this must be set to ``azure``.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "10f315cd-09be-4e10-bba5-db96872ae438": {"__data__": {"id_": "10f315cd-09be-4e10-bba5-db96872ae438", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "66964824bd1d74cc0fafae0ff47ebe1a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "3bed3ea9-56b4-4a13-ba76-48839b19cbcc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "36aeb503d12c47ef682885c8065f41c6"}, "hash": "6ddbd643058b8fbffb952102b156e83252b4dc6099f44cc4a4382862d99bd0f5"}, "3": {"node_id": "c4efdbc4-6845-4071-91dc-225abe067992", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "3397a857c5b8370d9b5b514021ffe9a8"}, "hash": "795ca270c9e455f6f990ee3455d3bdd8377c89ecb274260473fbec3d65b366a7"}}, "hash": "0dd66b55f03857df740ba412de4e4c1d69eba4ec67066f405ac4a8c9925db370", "text": "* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n\n    .. tab-item:: GCP\n\n        The cloud service provider.For GCP, this must be set to ``gcp``.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n\n.. _cluster-configuration-region:\n\n``provider.region``\n~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The region to use for deployment of the Ray cluster.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n        * **Default:** us-west-2\n\n    .. tab-item:: Azure\n\n        Not available... tab-item:: GCP\n\n        The region to use for deployment of the Ray cluster.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n        * **Default:** us-west1\n\n.. _cluster-configuration-availability-zone:\n\n``provider.availability_zone``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        A string specifying a comma-separated list of availability zone(s) that nodes may be launched in.Nodes will be launched in the first listed availability zone and will be tried in the following availability\n        zones if launching fails.* **Required:** No\n        * **Importance:** Low\n        * **Type:** String\n        * **Default:** us-west-2a,us-west-2b\n\n    .. tab-item:: Azure\n\n        Not available... tab-item:: GCP\n\n        A string specifying a comma-separated list of availability zone(s) that nodes may be launched in.* **Required:** No\n        * **Importance:** Low\n        * **Type:** String\n        * **Default:** us-west1-a\n\n.. _cluster-configuration-location:\n\n``provider.location``\n~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        Not available... tab-item:: Azure\n\n        The location to use for deployment of the Ray cluster.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n        * **Default:** westus2\n\n    .. tab-item:: GCP\n\n        Not available... _cluster-configuration-resource-group:\n\n``provider.resource_group``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        Not available... tab-item:: Azure\n\n        The resource group to use for deployment of the Ray cluster.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** String\n        * **Default:** ray-cluster\n\n    .. tab-item:: GCP\n\n        Not available... _cluster-configuration-subscription-id:\n\n``provider.subscription_id``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        Not available... tab-item:: Azure\n\n        The subscription ID to use for deployment of the Ray cluster.If not specified, Ray will use the default from the Azure CLI.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c4efdbc4-6845-4071-91dc-225abe067992": {"__data__": {"id_": "c4efdbc4-6845-4071-91dc-225abe067992", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "3397a857c5b8370d9b5b514021ffe9a8"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "10f315cd-09be-4e10-bba5-db96872ae438", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "66964824bd1d74cc0fafae0ff47ebe1a"}, "hash": "0dd66b55f03857df740ba412de4e4c1d69eba4ec67066f405ac4a8c9925db370"}, "3": {"node_id": "bb30ffd7-a50d-4b0d-923b-055e224a50e0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "5471af4ecb410ebdad650179f5f9b004"}, "hash": "539e0a00a5afe983a28586340df4da2890d096de626761fd808cbac93d765f11"}}, "hash": "795ca270c9e455f6f990ee3455d3bdd8377c89ecb274260473fbec3d65b366a7", "text": "* **Required:** No\n        * **Importance:** High\n        * **Type:** String\n        * **Default:** ``\"\"``\n\n    .. tab-item:: GCP\n\n        Not available... _cluster-configuration-project-id:\n\n``provider.project_id``\n~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        Not available... tab-item:: Azure\n\n        Not available... tab-item:: GCP\n\n        The globally unique project ID to use for deployment of the Ray cluster.* **Required:** Yes\n        * **Importance:** Low\n        * **Type:** String\n        * **Default:** ``null``\n\n.. _cluster-configuration-cache-stopped-nodes:\n\n``provider.cache_stopped_nodes``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf enabled, nodes will be *stopped* when the cluster scales down.If disabled, nodes will be *terminated* instead.Stopped nodes launch faster than terminated nodes.\n\n\n* **Required:** No\n* **Importance:** Low\n* **Type:** Boolean\n* **Default:** ``True``\n\n.. _cluster-configuration-use-internal-ips:\n\n``provider.use_internal_ips``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf enabled, Ray will use private IP addresses for communication between nodes.\nThis should be omitted if your network interfaces use public IP addresses.\n\nIf enabled, Ray CLI commands (e.g. ``ray up``) will have to be run from a machine\nthat is part of the same VPC as the cluster. \n\nThis option does not affect the existence of public IP addresses for the nodes, it only\naffects which IP addresses are used by Ray. The existence of public IP addresses is\ncontrolled by your cloud provider's configuration.\n\n\n* **Required:** No\n* **Importance:** Low\n* **Type:** Boolean\n* **Default:** ``False``\n\n.. _cluster-configuration-security-group:\n\n``provider.security_group``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        A security group that can be used to specify custom inbound rules.\n\n        * **Required:** No\n        * **Importance:** Medium\n        * **Type:** :ref:`Security Group <cluster-configuration-security-group-type>`\n\n    .. tab-item:: Azure\n\n        Not available.\n\n    .. tab-item:: GCP\n\n        Not available.\n\n\n.. _cluster-configuration-group-name:\n\n``security_group.GroupName``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe name of the security group.This name must be unique within the VPC.* **Required:** No\n* **Importance:** Low\n* **Type:** String\n* **Default:** ``\"ray-autoscaler-{cluster-name}\"``\n\n.. _cluster-configuration-ip-permissions:\n\n``security_group.IpPermissions``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe inbound rules associated with the security group.* **Required:** No\n* **Importance:** Medium\n* **Type:** `IpPermission <https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_IpPermission.html>`_\n\n.. _cluster-configuration-node-config:\n\n``available_node_types.<node_type_name>.node_type.node_config``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe configuration to be used to launch the nodes on the cloud service provider.Among other things, this will specify the instance type to be launched.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bb30ffd7-a50d-4b0d-923b-055e224a50e0": {"__data__": {"id_": "bb30ffd7-a50d-4b0d-923b-055e224a50e0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "5471af4ecb410ebdad650179f5f9b004"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "c4efdbc4-6845-4071-91dc-225abe067992", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "3397a857c5b8370d9b5b514021ffe9a8"}, "hash": "795ca270c9e455f6f990ee3455d3bdd8377c89ecb274260473fbec3d65b366a7"}, "3": {"node_id": "bc0eddd8-2564-4de1-b3b5-2e23e0212663", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "83809b10241a3446b869de1b3debe9d6"}, "hash": "2747614fa63b7e833e6d14ad9ee6615d0cbf150c5757e68ee8f25e95e205f69a"}}, "hash": "539e0a00a5afe983a28586340df4da2890d096de626761fd808cbac93d765f11", "text": "* **Required:** Yes\n* **Importance:** High\n* **Type:** :ref:`Node config <cluster-configuration-node-config-type>`\n\n.. _cluster-configuration-resources:\n\n``available_node_types.<node_type_name>.node_type.resources``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe resources that a node type provides, which enables the autoscaler to automatically select the right type of nodes to launch given the resource demands of the application.The resources specified will be automatically passed to the ``ray start`` command for the node via an environment variable.If not provided, Autoscaler can automatically detect them only for AWS/Kubernetes cloud providers.For more information, see also the `resource demand scheduler <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/resource_demand_scheduler.py>`_\n\n* **Required:** Yes (except for AWS/K8s)\n* **Importance:** High\n* **Type:** :ref:`Resources <cluster-configuration-resources-type>`\n* **Default:** ``{}``\n\nIn some cases, adding special nodes without any resources may be desirable.Such nodes can be used as a driver which connects to the cluster to launch jobs.In order to manually add a node to an autoscaled cluster, the *ray-cluster-name* tag should be set and *ray-node-type* tag should be set to unmanaged.Unmanaged nodes can be created by setting the resources to ``{}`` and the :ref:`maximum workers <cluster-configuration-node-min-workers>` to 0.The Autoscaler will not attempt to start, stop, or update unmanaged nodes.The user is responsible for properly setting up and cleaning up unmanaged nodes... _cluster-configuration-node-min-workers:\n\n``available_node_types.<node_type_name>.node_type.min_workers``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe minimum number of workers to maintain for this node type regardless of utilization.* **Required:** No\n* **Importance:** High\n* **Type:** Integer\n* **Default:** ``0``\n* **Minimum:** ``0``\n* **Maximum:** Unbounded\n\n.. _cluster-configuration-node-max-workers:\n\n``available_node_types.<node_type_name>.node_type.max_workers``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe maximum number of workers to have in the cluster for this node type regardless of utilization.This takes precedence over :ref:`minimum workers <cluster-configuration-node-min-workers>`.By default, the number of workers of a node type is unbounded, constrained only by the cluster-wide :ref:`max_workers <cluster-configuration-max-workers>`.(Prior to Ray 1.3.0, the default value for this field was 0.)Note, for the nodes of type ``head_node_type`` the default number of max workers is 0.* **Required:** No\n* **Importance:** High\n* **Type:** Integer\n* **Default:** cluster-wide :ref:`max_workers <cluster-configuration-max-workers>`\n* **Minimum:** ``0``\n* **Maximum:** cluster-wide :ref:`max_workers <cluster-configuration-max-workers>`\n\n.. _cluster-configuration-node-type-worker-setup-commands:\n\n``available_node_types.<node_type_name>.node_type.worker_setup_commands``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA list of commands to run to set up worker nodes of this type.These commands will replace the general :ref:`worker setup commands <cluster-configuration-worker-setup-commands>` for the node.* **Required:** No\n* **Importance:** low\n* **Type:** List of String\n* **Default:** ``[]``\n\n.. _cluster-configuration-cpu:\n\n``available_node_types.<node_type_name>.node_type.resources.CPU``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The number of CPUs made available by this node.If not configured, Autoscaler can automatically detect them only for AWS/Kubernetes cloud providers.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bc0eddd8-2564-4de1-b3b5-2e23e0212663": {"__data__": {"id_": "bc0eddd8-2564-4de1-b3b5-2e23e0212663", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "83809b10241a3446b869de1b3debe9d6"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "bb30ffd7-a50d-4b0d-923b-055e224a50e0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "5471af4ecb410ebdad650179f5f9b004"}, "hash": "539e0a00a5afe983a28586340df4da2890d096de626761fd808cbac93d765f11"}, "3": {"node_id": "d005637e-bd3c-46c8-a141-bddb7f3e0457", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "8b91c93322943e93a233ca14b0e23cb1"}, "hash": "4d9051c5f853f2c80c70e13ecff5240795b096ef8616feff61c3387502c99051"}}, "hash": "2747614fa63b7e833e6d14ad9ee6615d0cbf150c5757e68ee8f25e95e205f69a", "text": "* **Required:** Yes (except for AWS/K8s)\n        * **Importance:** High\n        * **Type:** Integer\n\n    .. tab-item:: Azure\n\n        The number of CPUs made available by this node.* **Required:** Yes\n        * **Importance:** High\n        * **Type:** Integer\n\n    .. tab-item:: GCP\n\n        The number of CPUs made available by this node.* **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n\n.. _cluster-configuration-gpu:\n\n``available_node_types.<node_type_name>.node_type.resources.GPU``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The number of GPUs made available by this node. If not configured, Autoscaler can automatically detect them only for AWS/Kubernetes cloud providers.\n\n        * **Required:** No\n        * **Importance:** Low\n        * **Type:** Integer\n\n    .. tab-item:: Azure\n\n        The number of GPUs made available by this node.\n\n        * **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n    .. tab-item:: GCP\n\n        The number of GPUs made available by this node.\n\n        * **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n.. _cluster-configuration-memory:\n\n``available_node_types.<node_type_name>.node_type.resources.memory``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The memory in bytes allocated for python worker heap memory on the node.\n        If not configured, Autoscaler will automatically detect the amount of RAM on\n        the node for AWS/Kubernetes and allocate 70% of it for the heap.\n\n        * **Required:** No\n        * **Importance:** Low\n        * **Type:** Integer\n\n    .. tab-item:: Azure\n\n        The memory in bytes allocated for python worker heap memory on the node.\n\n        * **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n    .. tab-item:: GCP\n\n        The memory in bytes allocated for python worker heap memory on the node.\n\n        * **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n\n.. _cluster-configuration-object-store-memory:\n\n``available_node_types.<node_type_name>.node_type.resources.object-store-memory``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        The memory in bytes allocated for the object store on the node.If not configured, Autoscaler will automatically detect the amount of RAM on the node for AWS/Kubernetes and allocate 30% of it for the object store.* **Required:** No\n        * **Importance:** Low\n        * **Type:** Integer\n\n    .. tab-item:: Azure\n\n        The memory in bytes allocated for the object store on the node.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d005637e-bd3c-46c8-a141-bddb7f3e0457": {"__data__": {"id_": "d005637e-bd3c-46c8-a141-bddb7f3e0457", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "8b91c93322943e93a233ca14b0e23cb1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst"}, "hash": "15ab02ba84beaddcca6fa3fc711e6e6ac92635e9f2250a9ab1fecc3fd6144860"}, "2": {"node_id": "bc0eddd8-2564-4de1-b3b5-2e23e0212663", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "83809b10241a3446b869de1b3debe9d6"}, "hash": "2747614fa63b7e833e6d14ad9ee6615d0cbf150c5757e68ee8f25e95e205f69a"}}, "hash": "4d9051c5f853f2c80c70e13ecff5240795b096ef8616feff61c3387502c99051", "text": "* **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n    .. tab-item:: GCP\n\n        The memory in bytes allocated for the object store on the node.* **Required:** No\n        * **Importance:** High\n        * **Type:** Integer\n\n.. _cluster-configuration-node-docker:\n\n``available_node_types.<node_type_name>.docker``\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA set of overrides to the top-level :ref:`Docker <cluster-configuration-docker>` configuration.* **Required:** No\n* **Importance:** Low\n* **Type:** :ref:`docker <cluster-configuration-node-docker-type>`\n* **Default:** ``{}``\n\nExamples\n--------\n\nMinimal configuration\n~~~~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/aws/example-minimal.yaml\n            :language: yaml\n\n    .. tab-item:: Azure\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/azure/example-minimal.yaml\n            :language: yaml\n\n    .. tab-item:: GCP\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/gcp/example-minimal.yaml\n            :language: yaml\n\nFull configuration\n~~~~~~~~~~~~~~~~~~\n\n.. tab-set::\n\n    .. tab-item:: AWS\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/aws/example-full.yaml\n            :language: yaml\n\n    .. tab-item:: Azure\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/azure/example-full.yaml\n            :language: yaml\n\n    .. tab-item:: GCP\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/gcp/example-full.yaml\n            :language: yaml\n\nTPU Configuration\n~~~~~~~~~~~~~~~~~\n\nIt is possible to use `TPU VMs <https://cloud.google.com/tpu/docs/users-guide-tpu-vm>`_ on GCP.Currently, `TPU pods <https://cloud.google.com/tpu/docs/system-architecture-tpu-vm#pods>`_ (TPUs other than v2-8, v3-8 and v4-8) are not supported.Before using a config with TPUs, ensure that the `TPU API is enabled for your GCP project <https://cloud.google.com/tpu/docs/users-guide-tpu-vm#enable_the_cloud_tpu_api>`_... tab-set::\n\n    .. tab-item:: GCP\n\n        .. literalinclude:: ../../../../../python/ray/autoscaler/gcp/tpu.yaml\n            :language: yaml", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0d89b6c6-276b-452d-94fe-e3f3d08369f6": {"__data__": {"id_": "0d89b6c6-276b-452d-94fe-e3f3d08369f6", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/index.rst", "file_name": "index.rst", "text_hash": "dcb884fc3dfc85cf80bced085bb6533a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "5f72eedd6db99177e353e656c114bdd778cb25b1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/index.rst", "file_name": "index.rst"}, "hash": "3f15523ae4e71e59d8be3f4c2fac3122a52c04e9d8391b4f26ead754e987bccf"}}, "hash": "781e54f756d5785c50c9af8abd3434df0646630386902fa7ec27e3b981919387", "text": ".. _ref-cluster-setup:\n\nCommunity Supported Cluster Managers\n====================================\n\n.. note::\n\n    If you're using AWS, Azure or GCP you can use the :ref:`Ray cluster launcher <cluster-index>` to simplify the cluster setup process.\n\nThe following is a list of community supported cluster managers.\n\n.. toctree::\n   :maxdepth: 2\n\n   yarn.rst\n   slurm.rst\n   lsf.rst\n   spark.rst\n\n.. _ref-additional-cloud-providers:\n\nUsing a custom cloud or cluster manager\n=======================================\n\nThe Ray cluster launcher currently supports AWS, Azure, GCP, Aliyun and Kuberay out of the box. To use the Ray cluster launcher and Autoscaler on other cloud providers or cluster managers, you can implement the `node_provider.py <https://github.com/ray-project/ray/tree/master/python/ray/autoscaler/node_provider.py>`_ interface (100 LOC).\nOnce the node provider is implemented, you can register it in the `provider section <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/local/example-full.yaml#L18>`_ of the cluster launcher config.\n\n.. code-block:: yaml\n\n    provider:\n      type: \"external\"\n      module: \"my.module.MyCustomNodeProvider\"\n\nYou can refer to `AWSNodeProvider <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/aws/node_provider.py#L95>`_, `KuberayNodeProvider <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/kuberay/node_provider.py#L148>`_ and\n `LocalNodeProvider <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/_private/local/node_provider.py#L166>`_ for more examples.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e5fdc0ec-92be-471a-97d0-e6b8b9397d8e": {"__data__": {"id_": "e5fdc0ec-92be-471a-97d0-e6b8b9397d8e", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/lsf.rst", "file_name": "lsf.rst", "text_hash": "996544bf5ad9c4b4e79fe228402a5b89"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "9d96b1ae7e208a0ba069d1d6f3e12027358f5ec2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/lsf.rst", "file_name": "lsf.rst"}, "hash": "ae480e104f8c1739345663418d32b46e863c26691ee20f88138e6dcc7123efc0"}}, "hash": "f7e5a4728e02e3b2263b9d66748d70612d2e3bed84d4d5aaef72a7f8812066f1", "text": ".. _ray-LSF-deploy:\n\nDeploying on LSF\n================\n\nThis document describes a couple high-level steps to run Ray clusters on LSF.\n\n1) Obtain desired nodes from LSF scheduler using bsub directives.\n2) Obtain free ports on the desired nodes to start ray services like dashboard, GCS etc.\n3) Start ray head node on one of the available nodes.\n4) Connect all the worker nodes to the head node.\n5) Perform port forwarding to access ray dashboard.\n\nSteps 1-4 have been automated and can be easily run as a script, please refer to below github repo to access script and run sample workloads:\n\n- `ray_LSF`_ Ray with LSF. Users can start up a Ray cluster on LSF, and run DL workloads through that either in a batch or interactive mode.\n\n.. _`ray_LSF`: https://github.com/IBMSpectrumComputing/ray-integration", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "dd3f68ae-eaa5-4a34-a5b5-c02e495ccdd9": {"__data__": {"id_": "dd3f68ae-eaa5-4a34-a5b5-c02e495ccdd9", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-basic.rst", "file_name": "slurm-basic.rst", "text_hash": "03db2f6b72b5adf83b0c2754069199e2"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "13cf6ed3b08d89431287ced9df879d64790b6ece", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-basic.rst", "file_name": "slurm-basic.rst"}, "hash": "956bc909cd765dd56cd696afe91cb774f12ca1c2678036423169bbf4f2af13c9"}}, "hash": "ea62a6b22c4558b9676cbc887787b5403868d93a237f3894e5168f607f2e766e", "text": ":orphan:\n\n.. _slurm-basic:\n\nslurm-basic.sh\n~~~~~~~~~~~~~~\n\n.. literalinclude:: /cluster/doc_code/slurm-basic.sh\n   :language: bash", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "243c751f-e27c-4caf-abff-f07b144ccb66": {"__data__": {"id_": "243c751f-e27c-4caf-abff-f07b144ccb66", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-launch.rst", "file_name": "slurm-launch.rst", "text_hash": "2de1302653ddb6bc3336a904d1f5b032"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2b3d2f49ed4318d827b14b29988b549413ab253d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-launch.rst", "file_name": "slurm-launch.rst"}, "hash": "aef0f2bf8d2d0b86a5223555356894f782fd49f9d5d568c7cdf731dfa6219ceb"}}, "hash": "4eb86f38e62d93c46f2d0352a153868a2638214a72b6155c672c57c984d9f814", "text": ":orphan:\n\n.. _slurm-launch:\n\nslurm-launch.py\n~~~~~~~~~~~~~~~\n\n.. literalinclude:: /cluster/doc_code/slurm-launch.py", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1754c6fe-7d8a-4ff4-9d7c-21b6afe03d87": {"__data__": {"id_": "1754c6fe-7d8a-4ff4-9d7c-21b6afe03d87", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-template.rst", "file_name": "slurm-template.rst", "text_hash": "587b8e7f57b3f2bb07f59fc8bdcea0e3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d669e36152f5242b38b20986b896012d5056b2f6", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-template.rst", "file_name": "slurm-template.rst"}, "hash": "4fff02776982a52e969a0a0f09198c673b1887a8c58800d4a9fd261d29c9184a"}}, "hash": "ec39522d5311a21cdd52fe449068cb2d4007225d93f55684ad8b41a25a00ea8f", "text": ":orphan:\n\n.. _slurm-template:\n\nslurm-template.sh\n~~~~~~~~~~~~~~~~~\n\n.. literalinclude:: /cluster/doc_code/slurm-template.sh\n    :language: bash", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a761e05b-7ad9-4db3-924a-452fce5a0444": {"__data__": {"id_": "a761e05b-7ad9-4db3-924a-452fce5a0444", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "ef14700397f13a4948595066981c8152"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst"}, "hash": "697b5bfa715a15819f57834f209e7a0ad9c9db672969b5ca1549d6408757beb9"}, "3": {"node_id": "70a61247-6af1-4f88-afa2-b7a5f97df653", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "2515ff6607a656573ebb22a9c4a96d4e"}, "hash": "99e9111810379f68d59679d7cdd3ba92ba769581acbccf627a35a1a17e6c4bd3"}}, "hash": "99c8e45b35e138fdbcc1f8b7a7708d1f597e96bc0186fab9d3c90a91798bfb47", "text": ".. _ray-slurm-deploy:\n\nDeploying on Slurm\n==================\n\nSlurm usage with Ray can be a little bit unintuitive.\n\n* SLURM requires multiple copies of the same program are submitted multiple times to the same cluster to do cluster programming. This is particularly well-suited for MPI-based workloads.\n* Ray, on the other hand, expects a head-worker architecture with a single point of entry. That is, you'll need to start a Ray head node, multiple Ray worker nodes, and run your Ray script on the head node.\n\n.. warning::\n\n    SLURM support is still a work in progress. SLURM users should be aware\n    of current limitations regarding networking.\n    See :ref:`here <slurm-network-ray>` for more explanations.\n\n    SLURM support is community-maintained. Maintainer GitHub handle: tupui.\n\nThis document aims to clarify how to run Ray on SLURM.\n\n.. contents::\n  :local:\n\n\nWalkthrough using Ray with SLURM\n--------------------------------\n\nMany SLURM deployments require you to interact with slurm via ``sbatch``, which executes a batch script on SLURM.\n\nTo run a Ray job with ``sbatch``, you will want to start a Ray cluster in the sbatch job with multiple ``srun`` commands (tasks), and then execute your python script that uses Ray. Each task will run on a separate node and start/connect to a Ray runtime.\n\nThe below walkthrough will do the following:\n\n1. Set the proper headers for the ``sbatch`` script.\n2. Load the proper environment/modules.\n3. Fetch a list of available computing nodes and their IP addresses.\n4. Launch a head ray process in one of the node (called the head node).\n5. Launch Ray processes in (n-1) worker nodes and connects them to the head node by providing the head node address.\n6. After the underlying ray cluster is ready, submit the user specified task.\n\nSee :ref:`slurm-basic.sh <slurm-basic>` for an end-to-end example.\n\n.. _ray-slurm-headers:\n\nsbatch directives\n~~~~~~~~~~~~~~~~~\n\nIn your sbatch script, you'll want to add `directives to provide context <https://slurm.schedmd.com/sbatch.html>`__ for your job to SLURM.\n\n.. code-block:: bash\n\n  #!/bin/bash\n  #SBATCH --job-name=my-workload\n\nYou'll need to tell SLURM to allocate nodes specifically for Ray. Ray will then find and manage all resources on each node.\n\n.. code-block:: bash\n\n  ### Modify this according to your Ray workload.\n  #SBATCH --nodes=4\n  #SBATCH --exclusive\n\nImportant: To ensure that each Ray worker runtime will run on a separate node, set ``tasks-per-node``.\n\n.. code-block:: bash\n\n  #SBATCH --tasks-per-node=1\n\nSince we've set `tasks-per-node = 1`, this will be used to guarantee that each Ray worker runtime will obtain the\nproper resources. In this example, we ask for at least 5 CPUs and 5 GB of memory per node.\n\n.. code-block:: bash\n\n  ### Modify this according to your Ray workload.\n  #SBATCH --cpus-per-task=5\n  #SBATCH --mem-per-cpu=1GB\n  ### Similarly, you can also specify the number of GPUs per node.\n  ### Modify this according to your Ray workload. Sometimes this\n  ### should be 'gres' instead.\n  #SBATCH --gpus-per-task=1\n\n\nYou can also add other optional flags to your sbatch directives.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "70a61247-6af1-4f88-afa2-b7a5f97df653": {"__data__": {"id_": "70a61247-6af1-4f88-afa2-b7a5f97df653", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "2515ff6607a656573ebb22a9c4a96d4e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst"}, "hash": "697b5bfa715a15819f57834f209e7a0ad9c9db672969b5ca1549d6408757beb9"}, "2": {"node_id": "a761e05b-7ad9-4db3-924a-452fce5a0444", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "ef14700397f13a4948595066981c8152"}, "hash": "99c8e45b35e138fdbcc1f8b7a7708d1f597e96bc0186fab9d3c90a91798bfb47"}, "3": {"node_id": "827773b2-ac63-4039-a3b1-dba6e5647db4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "2ae79783e211bf207d769912d8d3c009"}, "hash": "a43fed72ab3322c878827f9cb081c7c09fa9f0c18eb7e7a371f37da6e2afc265"}}, "hash": "99e9111810379f68d59679d7cdd3ba92ba769581acbccf627a35a1a17e6c4bd3", "text": "Loading your environment\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nFirst, you'll often want to Load modules or your own conda environment at the beginning of the script.\n\nNote that this is an optional step, but it is often required for enabling the right set of dependencies.\n\n.. code-block:: bash\n\n  # Example: module load pytorch/v1.4.0-gpu\n  # Example: conda activate my-env\n\n  conda activate my-env\n\nObtain the head IP address\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nNext, we'll want to obtain a hostname and a node IP address for the head node. This way, when we start worker nodes, we'll be able to properly connect to the right head node.\n\n.. literalinclude:: /cluster/doc_code/slurm-basic.sh\n   :language: bash\n   :start-after: __doc_head_address_start__\n   :end-before: __doc_head_address_end__\n\n\n\nStarting the Ray head node\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nAfter detecting the head node hostname and head node IP, we'll want to create\na Ray head node runtime. We'll do this by using ``srun`` as a background task\nas a single task/node (recall that ``tasks-per-node=1``).\n\nBelow, you'll see that we explicitly specify the number of CPUs (``num-cpus``)\nand number of GPUs (``num-gpus``) to Ray, as this will prevent Ray from using\nmore resources than allocated. We also need to explicitly\nindicate the ``node-ip-address`` for the Ray head runtime:\n\n.. literalinclude:: /cluster/doc_code/slurm-basic.sh\n   :language: bash\n   :start-after: __doc_head_ray_start__\n   :end-before: __doc_head_ray_end__\n\nBy backgrounding the above srun task, we can proceed to start the Ray worker runtimes.\n\nStarting the Ray worker nodes\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBelow, we do the same thing, but for each worker. Make sure the Ray head and Ray worker processes are not started on the same node.\n\n.. literalinclude:: /cluster/doc_code/slurm-basic.sh\n   :language: bash\n   :start-after: __doc_worker_ray_start__\n   :end-before: __doc_worker_ray_end__\n\nSubmitting your script\n~~~~~~~~~~~~~~~~~~~~~~\n\nFinally, you can invoke your Python script:\n\n.. literalinclude:: /cluster/doc_code/slurm-basic.sh\n   :language: bash\n   :start-after: __doc_script_start__\n\n\n.. note:: The -u argument tells python to print to stdout unbuffered, which is important with how slurm deals with rerouting output.If this argument is not included, you may get strange printing behavior such as printed statements not being logged by slurm until the program has terminated... _slurm-network-ray:\n\nSLURM networking caveats\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nThere are two important networking aspects to keep in mind when working with\nSLURM and Ray:\n\n1.Ports binding.2.IP binding.One common use of a SLURM cluster is to have multiple users running concurrent\njobs on the same infrastructure.This can easily conflict with Ray due to the\nway the head node communicates with its workers.Considering 2 users, if they both schedule a SLURM job using Ray\nat the same time, they are both creating a head node.In the backend, Ray will\nassign some internal ports to a few services.The issue is that as soon as the\nfirst head node is created, it will bind some ports and prevent them to be\nused by another head node.To prevent any conflicts, users have to manually\nspecify non overlapping ranges of ports.The following ports are to be\nadjusted.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "827773b2-ac63-4039-a3b1-dba6e5647db4": {"__data__": {"id_": "827773b2-ac63-4039-a3b1-dba6e5647db4", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "2ae79783e211bf207d769912d8d3c009"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst"}, "hash": "697b5bfa715a15819f57834f209e7a0ad9c9db672969b5ca1549d6408757beb9"}, "2": {"node_id": "70a61247-6af1-4f88-afa2-b7a5f97df653", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "2515ff6607a656573ebb22a9c4a96d4e"}, "hash": "99e9111810379f68d59679d7cdd3ba92ba769581acbccf627a35a1a17e6c4bd3"}, "3": {"node_id": "149d57a8-2463-4686-8536-e6b7c927aa6b", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "3408db8d57268b6d1843e4494ebae303"}, "hash": "05255bca38cca9fccfe9ade1205696d6deeef5457931d0e589786acba4a3ad87"}}, "hash": "a43fed72ab3322c878827f9cb081c7c09fa9f0c18eb7e7a371f37da6e2afc265", "text": "For an explanation on ports, see :ref:`here <ray-ports>`::\n\n    # used for all ports\n    --node-manager-port\n    --object-manager-port\n    --min-worker-port\n    --max-worker-port\n    # used for the head node\n    --port\n    --ray-client-server-port\n    --redis-shard-ports\n\nFor instance, again with 2 users, they would have to adapt the instructions\nseen above to:\n\n.. code-block:: bash\n\n  # user 1\n  # same as above\n  ...\n  srun --nodes=1 --ntasks=1 -w \"$head_node\" \\\n      ray start --head --node-ip-address=\"$head_node_ip\" \\\n          --port=6379 \\\n          --node-manager-port=6700 \\\n          --object-manager-port=6701 \\\n          --ray-client-server-port=10001 \\\n          --redis-shard-ports=6702 \\\n          --min-worker-port=10002 \\\n          --max-worker-port=19999 \\\n          --num-cpus \"${SLURM_CPUS_PER_TASK}\" --num-gpus \"${SLURM_GPUS_PER_TASK}\" --block &\n\n  # user 2\n  # same as above\n  ...\n  srun --nodes=1 --ntasks=1 -w \"$head_node\" \\\n      ray start --head --node-ip-address=\"$head_node_ip\" \\\n          --port=6380 \\\n          --node-manager-port=6800 \\\n          --object-manager-port=6801 \\\n          --ray-client-server-port=20001 \\\n          --redis-shard-ports=6802 \\\n          --min-worker-port=20002 \\\n          --max-worker-port=29999 \\\n          --num-cpus \"${SLURM_CPUS_PER_TASK}\" --num-gpus \"${SLURM_GPUS_PER_TASK}\" --block &\n\nAs for the IP binding, on some cluster architecture the network interfaces\ndo not allow to use external IPs between nodes.Instead, there are internal\nnetwork interfaces (`eth0`, `eth1`, etc.).Currently, it's difficult to\nset an internal IP\n(see the open `issue <https://github.com/ray-project/ray/issues/22732>`_).Python-interface SLURM scripts\n------------------------------\n\n[Contributed by @pengzhenghao] Below, we provide a helper utility (:ref:`slurm-launch.py <slurm-launch>`) to auto-generate SLURM scripts and launch.``slurm-launch.py`` uses an underlying template (:ref:`slurm-template.sh <slurm-template>`) and fills out placeholders given user input.You can feel free to copy both files into your cluster for use.Feel free to also open any PRs for contributions to improve this script!Usage example\n~~~~~~~~~~~~~\n\nIf you want to utilize a multi-node cluster in slurm:\n\n.. code-block:: bash\n\n    python slurm-launch.py --exp-name test --command \"python your_file.py\" --num-nodes 3\n\nIf you want to specify the computing node(s), just use the same node name(s) in the same format of the output of ``sinfo`` command:\n\n.. code-block:: bash\n\n    python slurm-launch.py --exp-name test --command \"python your_file.py\" --num-nodes 3 --node NODE_NAMES", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "149d57a8-2463-4686-8536-e6b7c927aa6b": {"__data__": {"id_": "149d57a8-2463-4686-8536-e6b7c927aa6b", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "3408db8d57268b6d1843e4494ebae303"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst"}, "hash": "697b5bfa715a15819f57834f209e7a0ad9c9db672969b5ca1549d6408757beb9"}, "2": {"node_id": "827773b2-ac63-4039-a3b1-dba6e5647db4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "2ae79783e211bf207d769912d8d3c009"}, "hash": "a43fed72ab3322c878827f9cb081c7c09fa9f0c18eb7e7a371f37da6e2afc265"}}, "hash": "05255bca38cca9fccfe9ade1205696d6deeef5457931d0e589786acba4a3ad87", "text": "There are other options you can use when calling ``python slurm-launch.py``:\n\n* ``--exp-name``: The experiment name. Will generate ``{exp-name}_{date}-{time}.sh`` and  ``{exp-name}_{date}-{time}.log``.\n* ``--command``: The command you wish to run. For example: ``rllib train XXX`` or ``python XXX.py``.\n* ``--num-gpus``: The number of GPUs you wish to use in each computing node. Default: 0.\n* ``--node`` (``-w``): The specific nodes you wish to use, in the same form as the output of ``sinfo``. Nodes are automatically assigned if not specified.\n* ``--num-nodes`` (``-n``): The number of nodes you wish to use. Default: 1.\n* ``--partition`` (``-p``): The partition you wish to use. Default: \"\", will use user's default partition.\n* ``--load-env``: The command to setup your environment. For example: ``module load cuda/10.1``. Default: \"\".\n\nNote that the :ref:`slurm-template.sh <slurm-template>` is compatible with both IPV4 and IPV6 ip address of the computing nodes.\n\nImplementation\n~~~~~~~~~~~~~~\n\nConcretely, the (:ref:`slurm-launch.py <slurm-launch>`) does the following things:\n\n1. It automatically writes your requirements, e.g. number of CPUs, GPUs per node, the number of nodes and so on, to a sbatch script name ``{exp-name}_{date}-{time}.sh``. Your command (``--command``) to launch your own job is also written into the sbatch script.\n2. Then it will submit the sbatch script to slurm manager via a new process.\n3. Finally, the python process will terminate itself and leaves a log file named ``{exp-name}_{date}-{time}.log`` to record the progress of your submitted command. At the mean time, the ray cluster and your job is running in the slurm cluster.\n\n\nExamples and templates\n----------------------\n\nHere are some community-contributed templates for using SLURM with Ray:\n\n- `Ray sbatch submission scripts`_ used at `NERSC <https://www.nersc.gov/>`_, a US national lab.\n- `YASPI`_ (yet another slurm python interface) by @albanie. The goal of yaspi is to provide an interface to submitting slurm jobs, thereby obviating the joys of sbatch files. It does so through recipes - these are collections of templates and rules for generating sbatch scripts. Supports job submissions for Ray.\n\n- `Convenient python interface`_ to launch ray cluster and submit task by @pengzhenghao\n\n.. _`Ray sbatch submission scripts`: https://github.com/NERSC/slurm-ray-cluster\n\n.. _`YASPI`: https://github.com/albanie/yaspi\n\n.. _`Convenient python interface`: https://github.com/pengzhenghao/use-ray-with-slurm", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "63063991-ae68-481a-a5d3-875f036bed7a": {"__data__": {"id_": "63063991-ae68-481a-a5d3-875f036bed7a", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst", "text_hash": "ad075e99813d414b2050a3b45ebacb9e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06c765ed6430b814ee82b1268745d228e972b263", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst"}, "hash": "d36843a61ef1285f07facf7aa657acb5434a2b33f77657344081c2bc71aabca8"}, "3": {"node_id": "587ae477-7964-4fff-8767-a4d07796ebbc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst", "text_hash": "e84787f1b8e8e23d6a68ca9f76ffd5df"}, "hash": "134a47ea48029f5d5027fa4cc233c1086f0febddfd1ce1fc9c77cf33af94d3a3"}}, "hash": "309fc591e6a6a30bda8a926b4e1b05e20d53f9229b741cd9869f2122d37bc1d2", "text": ".. _ray-Spark-deploy:\n\nDeploying on Spark Standalone cluster\n=====================================\n\nThis document describes a couple high-level steps to run Ray clusters on `Spark Standalone cluster <https://spark.apache.org/docs/latest/spark-standalone.html>`_.Running a basic example\n-----------------------\n\nThis is a spark application example code that starts Ray cluster on spark,\nand then execute ray application code, then shut down initiated ray cluster.1) Create a python file that contains a spark application code,\nAssuming the python file name is 'ray-on-spark-example1.py'... code-block:: python\n\n    from pyspark.sql import SparkSession\n    from ray.util.spark import setup_ray_cluster, shutdown_ray_cluster, MAX_NUM_WORKER_NODES\n    if __name__ == \"__main__\":\n        spark = SparkSession \\\n            .builder \\\n            .appName(\"Ray on spark example 1\") \\\n            .config(\"spark.task.cpus\", \"4\") \\\n            .getOrCreate()\n\n        # Set up a ray cluster on this spark application, it creates a background\n        # spark job that each spark task launches one ray worker node.# ray head node is launched in spark application driver side.# Resources (CPU / GPU / memory) allocated to each ray worker node is equal\n        # to resources allocated to the corresponding spark task.setup_ray_cluster(num_worker_nodes=MAX_NUM_WORKER_NODES)\n\n        # You can any ray application code here, the ray application will be executed\n        # on the ray cluster setup above.# You don't need to set address for `ray.init`,\n        # it will connect to the cluster created above automatically.ray.init()\n        ...\n\n        # Terminate ray cluster explicitly.# If you don't call it, when spark application is terminated, the ray cluster\n        # will also be terminated.shutdown_ray_cluster()\n\n2) Submit the spark application above to spark standalone cluster... code-block:: bash\n\n    #!/bin/bash\n    spark-submit \\\n      --master spark://{spark_master_IP}:{spark_master_port} \\\n      path/to/ray-on-spark-example1.py\n\nCreating a long running ray cluster on spark cluster\n----------------------------------------------------\n\nThis is a spark application example code that starts a long running Ray cluster on spark.The created ray cluster can be accessed by remote python processes.1) Create a python file that contains a spark application code,\nAssuming the python file name is 'long-running-ray-cluster-on-spark.py'.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "587ae477-7964-4fff-8767-a4d07796ebbc": {"__data__": {"id_": "587ae477-7964-4fff-8767-a4d07796ebbc", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst", "text_hash": "e84787f1b8e8e23d6a68ca9f76ffd5df"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "06c765ed6430b814ee82b1268745d228e972b263", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst"}, "hash": "d36843a61ef1285f07facf7aa657acb5434a2b33f77657344081c2bc71aabca8"}, "2": {"node_id": "63063991-ae68-481a-a5d3-875f036bed7a", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst", "text_hash": "ad075e99813d414b2050a3b45ebacb9e"}, "hash": "309fc591e6a6a30bda8a926b4e1b05e20d53f9229b741cd9869f2122d37bc1d2"}}, "hash": "134a47ea48029f5d5027fa4cc233c1086f0febddfd1ce1fc9c77cf33af94d3a3", "text": ".. code-block:: python\n\n    from pyspark.sql import SparkSession\n    import time\n    from ray.util.spark import setup_ray_cluster, MAX_NUM_WORKER_NODES\n\n    if __name__ == \"__main__\":\n        spark = SparkSession \\\n            .builder \\\n            .appName(\"long running ray cluster on spark\") \\\n            .config(\"spark.task.cpus\", \"4\") \\\n            .getOrCreate()\n\n        cluster_address = setup_ray_cluster(\n            num_worker_nodes=MAX_NUM_WORKER_NODES\n        )\n        print(\"Ray cluster is set up, you can connect to this ray cluster \"\n              f\"via address ray://{cluster_address}\")\n\n        # Sleep forever until the spark application being terminated,\n        # at that time, the ray cluster will also be terminated.while True:\n            time.sleep(10)\n\n2) Submit the spark application above to spark standalone cluster... code-block:: bash\n\n    #!/bin/bash\n    spark-submit \\\n      --master spark://{spark_master_IP}:{spark_master_port} \\\n      path/to/long-running-ray-cluster-on-spark.py\n\nRay on Spark APIs\n-----------------\n\n.. autofunction:: ray.util.spark.setup_ray_cluster\n\n.. autofunction:: ray.util.spark.shutdown_ray_cluster", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f5acc69a-e6c7-4873-a78a-0402d8e658ac": {"__data__": {"id_": "f5acc69a-e6c7-4873-a78a-0402d8e658ac", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "8c0873a1797053c94955050448deb2dd"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "462527dce44ab0f7c53c75be6c03af20b2586f7f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst"}, "hash": "08c60bbe1f382bd31e604ac904ac58eefaf3ac2b661dea5443a8745f63065c73"}, "3": {"node_id": "4b71400e-85bc-4a36-836f-419954390f96", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "52569f0a1f65039c84dde03b44dbdc1d"}, "hash": "daef6a6c7256201142220643c46889344a5d8ef6cc797ca2281bd8096c1b1ff4"}}, "hash": "a3b996c14dc4c919231cdac12f5b59d3c8d0b87dccb36ce91606ff305441e040", "text": ".. _ray-yarn-deploy:\n\nDeploying on YARN\n=================\n\n.. warning::\n\n  Running Ray on YARN is still a work in progress.If you have a\n  suggestion for how to improve this documentation or want to request\n  a missing feature, please feel free to create a pull request or get in touch\n  using one of the channels in the `Questions or Issues?`_ section below.This document assumes that you have access to a YARN cluster and will walk\nyou through using `Skein`_ to deploy a YARN job that starts a Ray cluster and\nruns an example script on it.Skein uses a declarative specification (either written as a yaml file or using the Python API) and allows users to launch jobs and scale applications without the need to write Java code.You will first need to install Skein: ``pip install skein``.The Skein ``yaml`` file and example Ray program used here are provided in the\n`Ray repository`_ to get you started.Refer to the provided ``yaml``\nfiles to be sure that you maintain important configuration options for Ray to\nfunction properly... _`Ray repository`: https://github.com/ray-project/ray/tree/master/doc/yarn\n\nSkein Configuration\n-------------------\n\nA Ray job is configured to run as two `Skein services`:\n\n1.The ``ray-head`` service that starts the Ray head node and then runs the\n   application.2.The ``ray-worker`` service that starts worker nodes that join the Ray cluster.You can change the number of instances in this configuration or at runtime\n   using ``skein container scale`` to scale the cluster up/down.The specification for each service consists of necessary files and commands that will be run to start the service... code-block:: yaml\n\n    services:\n        ray-head:\n            # There should only be one instance of the head node per cluster.instances: 1\n            resources:\n                # The resources for the worker node.vcores: 1\n                memory: 2048\n            files:\n                ...\n            script:\n                ...\n        ray-worker:\n            # Number of ray worker nodes to start initially.# This can be scaled using 'skein container scale'.instances: 3\n            resources:\n                # The resources for the worker node.vcores: 1\n                memory: 2048\n            files:\n                ...\n            script:\n                ...\n\nPackaging Dependencies\n----------------------\n\nUse the ``files`` option to specify files that will be copied into the YARN container for the application to use.See `the Skein file distribution page <https://jcrist.github.io/skein/distributing-files.html>`_ for more information... code-block:: yaml\n\n    services:\n        ray-head:\n            # There should only be one instance of the head node per cluster.instances: 1\n            resources:\n                # The resources for the head node.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "4b71400e-85bc-4a36-836f-419954390f96": {"__data__": {"id_": "4b71400e-85bc-4a36-836f-419954390f96", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "52569f0a1f65039c84dde03b44dbdc1d"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "462527dce44ab0f7c53c75be6c03af20b2586f7f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst"}, "hash": "08c60bbe1f382bd31e604ac904ac58eefaf3ac2b661dea5443a8745f63065c73"}, "2": {"node_id": "f5acc69a-e6c7-4873-a78a-0402d8e658ac", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "8c0873a1797053c94955050448deb2dd"}, "hash": "a3b996c14dc4c919231cdac12f5b59d3c8d0b87dccb36ce91606ff305441e040"}, "3": {"node_id": "c51747c4-b3c4-484f-ad12-fd39932f45fc", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "6aabd8d8fdb07121a18d60a9d658a4bd"}, "hash": "0826468a411a23e7680b5a5815da27e6786df137d130f4a3320ca1a8cb26e40c"}}, "hash": "daef6a6c7256201142220643c46889344a5d8ef6cc797ca2281bd8096c1b1ff4", "text": "vcores: 1\n                memory: 2048\n            files:\n                # ray/doc/yarn/example.py\n                example.py: example.py\n            #     # A packaged python environment using `conda-pack`.Note that Skein\n            #     # doesn't require any specific way of distributing files, but this\n            #     # is a good one for python projects.This is optional.#     # See https://jcrist.github.io/skein/distributing-files.html\n            #     environment: environment.tar.gz\n\nRay Setup in YARN\n-----------------\n\nBelow is a walkthrough of the bash commands used to start the ``ray-head`` and ``ray-worker`` services.Note that this configuration will launch a new Ray cluster for each application, not reuse the same cluster.Head node commands\n~~~~~~~~~~~~~~~~~~\n\nStart by activating a pre-existing environment for dependency management... code-block:: bash\n\n    source environment/bin/activate\n\nRegister the Ray head address needed by the workers in the Skein key-value store... code-block:: bash\n\n    skein kv put --key=RAY_HEAD_ADDRESS --value=$(hostname -i) current\n\nStart all the processes needed on the ray head node.By default, we set object store memory\nand heap memory to roughly 200 MB.This is conservative and should be set according to application needs... code-block:: bash\n\n    ray start --head --port=6379 --object-store-memory=200000000 --memory 200000000 --num-cpus=1\n\nExecute the user script containing the Ray program... code-block:: bash\n\n    python example.py\n\nClean up all started processes even if the application fails or is killed... code-block:: bash\n\n    ray stop\n    skein application shutdown current\n\nPutting things together, we have:\n\n.. literalinclude:: /cluster/doc_code/yarn/ray-skein.yaml\n   :language: yaml\n   :start-after: # Head service\n   :end-before: # Worker service", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "c51747c4-b3c4-484f-ad12-fd39932f45fc": {"__data__": {"id_": "c51747c4-b3c4-484f-ad12-fd39932f45fc", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "6aabd8d8fdb07121a18d60a9d658a4bd"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "462527dce44ab0f7c53c75be6c03af20b2586f7f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst"}, "hash": "08c60bbe1f382bd31e604ac904ac58eefaf3ac2b661dea5443a8745f63065c73"}, "2": {"node_id": "4b71400e-85bc-4a36-836f-419954390f96", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "52569f0a1f65039c84dde03b44dbdc1d"}, "hash": "daef6a6c7256201142220643c46889344a5d8ef6cc797ca2281bd8096c1b1ff4"}}, "hash": "0826468a411a23e7680b5a5815da27e6786df137d130f4a3320ca1a8cb26e40c", "text": "Worker node commands\n~~~~~~~~~~~~~~~~~~~~\n\nFetch the address of the head node from the Skein key-value store.\n\n.. code-block:: bash\n\n    RAY_HEAD_ADDRESS=$(skein kv get current --key=RAY_HEAD_ADDRESS)\n\nStart all of the processes needed on a ray worker node, blocking until killed by Skein/YARN via SIGTERM. After receiving SIGTERM, all started processes should also die (ray stop).\n\n.. code-block:: bash\n\n    ray start --object-store-memory=200000000 --memory 200000000 --num-cpus=1 --address=$RAY_HEAD_ADDRESS:6379 --block; ray stop\n\nPutting things together, we have:\n\n.. literalinclude:: /cluster/doc_code/yarn/ray-skein.yaml\n   :language: yaml\n   :start-after: # Worker service\n\nRunning a Job\n-------------\n\nWithin your Ray script, use the following to connect to the started Ray cluster:\n\n.. literalinclude:: /cluster/doc_code/yarn/example.py\n    :language: python\n    :start-after: if __name__ == \"__main__\"\n\nYou can use the following command to launch the application as specified by the Skein YAML file.\n\n.. code-block:: bash\n\n    skein application submit [TEST.YAML]\n\nOnce it has been submitted, you can see the job running on the YARN dashboard.\n\n.. image:: /cluster/images/yarn-job.png\n\nCleaning Up\n-----------\n\nTo clean up a running job, use the following (using the application ID):\n\n.. code-block:: bash\n\n    skein application shutdown $appid\n\nQuestions or Issues?\n--------------------\n\n.. include:: /_includes/_help.rst\n\n.. _`Skein`: https://jcrist.github.io/skein/", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "059ed3f6-68cb-4d27-8088-93f406198823": {"__data__": {"id_": "059ed3f6-68cb-4d27-8088-93f406198823", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/configuring-autoscaling.rst", "file_name": "configuring-autoscaling.rst", "text_hash": "92b1fa7ad9c0ac32024b46ac641d8b60"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "7560178692df45d7752a866d364dd9404a5d712c", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/configuring-autoscaling.rst", "file_name": "configuring-autoscaling.rst"}, "hash": "53eb697b23e47da5f67092d9579bcb9c4da936f6d061a68f2994b812cfde4799"}}, "hash": "3f2933239e7ad539baa5c86d3384d8493410e2f9cfb1018de817ede98c73602d", "text": ".. _vms-autoscaling:\n\nConfiguring Autoscaling\n=======================\n\nThis guide explains how to configure the Ray autoscaler using the Ray cluster launcher.\nThe Ray autoscaler is a Ray cluster process that automatically scales a cluster up and down based on resource demand.\nThe autoscaler does this by adjusting the number of nodes in the cluster based on the resources required by tasks, actors or placement groups.\n\nNote that the autoscaler only considers logical resource requests for scaling (i.e., those specified in ``@ray.remote`` and displayed in `ray status`), not physical machine utilization. If a user tries to launch an actor, task, or placement group but there are insufficient resources, the request will be queued. The autoscaler adds nodes to satisfy resource demands in this queue.\nThe autoscaler also removes nodes after they become idle for some time.\nA node is considered idle if it has no active tasks, actors, or objects.\n\n.. tip::\n  **When to use Autoscaling?**\n\n  Autoscaling can reduce workload costs, but adds node launch overheads and can be tricky to configure.\n  We recommend starting with non-autoscaling clusters if you're new to Ray.\n\nCluster Config Parameters\n-------------------------\n\nThe following options are available in your cluster config file.\nIt is recommended that you set these before launching your cluster, but you can also modify them at run-time by updating the cluster config.\n\n`max_workers[default_value=2, min_value=0]`: The max number of cluster worker nodes to launch. Note that this does not include the head node.\n\n`min_workers[default_value=0, min_value=0]`: The min number of cluster worker nodes to launch, regardless of utilization. Note that this does not include the head node. This number must be less than the ``max_workers``.\n\n.. note::\n\n  If `max_workers` is modified at runtime, the autoscaler will immediately remove nodes until this constraint\n  is satisfied. This may disrupt running workloads.\n\nIf you are using more than one node type, you can also set min and max workers for each individual type:\n\n`available_node_types.<node_type_name>.max_workers[default_value=cluster max_workers, min_value=0]`: The maximum number of worker nodes of a given type to launch. This number must be less than or equal to the `max_workers` for the cluster.\n\n\n`available_node_types.<node_type_name>.min_workers[default_value=0, min_value=0]`: The minimum number of worker nodes of a given type to launch, regardless of utilization. The sum of `min_workers` across all node types must be less than or equal to the `max_workers` for the cluster.\n\nUpscaling and downscaling speed\n^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIf needed, you can also control the rate at which nodes should be added to or removed from the cluster. For applications with many short-lived tasks, you may wish to adjust the upscaling and downscaling speed to be more conservative.\n\n`upscaling_speed[default_value=1.0, min_value=1.0]`: The number of nodes allowed to be pending as a multiple of the current number of nodes. The higher the value, the more aggressive upscaling will be. For example, if this is set to 1.0, the cluster can grow in size by at most 100% at any time, so if the cluster currently has 20 nodes, at most 20 pending\nlaunches are allowed. The minimum number of pending launches is 5 regardless of this setting.\n\n`idle_timeout_minutes[default_value=5, min_value=0]`: The number of minutes that need to pass before an idle worker node is removed by the\nautoscaler. The smaller the value, the more aggressive downscaling will be. Worker nodes are considered idle when they hold no active tasks, actors, or referenced objects (either in-memory or spilled to disk). This parameter does not affect the head node.\n\nProgrammatic Scaling\n--------------------\n\nFor more information on programmatic access to the autoscaler, see the :ref:`Programmatic Cluster Scaling Guide <ref-autoscaler-sdk>`.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "377e787c-e08e-4b04-b823-7f34d1bef1bb": {"__data__": {"id_": "377e787c-e08e-4b04-b823-7f34d1bef1bb", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/index.md", "file_name": "index.md", "text_hash": "d0f929741541e0b7f796dd115c3efd7c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "01bbf04d082742ec92f875507d1b3b5da760013f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/index.md", "file_name": "index.md"}, "hash": "37f04211b5adcd8f6ceb9b62a3faa254c844864d10a1d42b448488f714caeced"}}, "hash": "c69f837d4d3a3dc6ceff59f9b03623459198727b849e37da7327c02ed0d94bfb", "text": "(vm-cluster-guides)=\n\n# User Guides\n\n:::{note}\nTo learn the basics of Ray on Cloud VMs, we recommend taking a look\nat the {ref}`introductory guide <vm-cluster-quick-start>` first.\n:::\n\nIn these guides, we go into further depth on several topics related to\ndeployments of Ray on cloud VMs or on-premises.\n* {ref}`launching-vm-clusters`\n* {ref}`vms-large-cluster`\n* {ref}`vms-autoscaling`\n* {ref}`ref-cluster-setup`\n* {ref}`cluster-FAQ`", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a9c498dc-346f-437c-ab6c-263fca673dda": {"__data__": {"id_": "a9c498dc-346f-437c-ab6c-263fca673dda", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst", "text_hash": "8329e7710c54d566d97d4b8d9d4f7967"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "278673bf77343f1d9a2c751b29b8254563c2b7fe", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst"}, "hash": "42f1ea68144a21d6adecf55c4ba6b6d65228f64552443c29466c970a5c38273d"}, "3": {"node_id": "1fbf3014-9503-4000-a502-6561115cbcd5", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst", "text_hash": "7d6532708a60f9bb94a4813ed3adc01e"}, "hash": "da112eba68369bd83674c3b6e4da9110656ea937b9b7762a1c9b4b23e9bb6f78"}}, "hash": "9e873ea3065f0b4c71957171726aa78c7465b2d414058c55affc1227ff539b5a", "text": ".. _vms-large-cluster:\n\nBest practices for deploying large clusters\n-------------------------------------------\n\nThis section aims to document best practices for deploying Ray clusters at\nlarge scale.Networking configuration\n^^^^^^^^^^^^^^^^^^^^^^^^\n\nEnd users should only need to directly interact with the head node of the\ncluster.In particular, there are 2 services which should be exposed to users:\n\n1.The dashboard\n2.The Ray client server\n\n.. note::\n\n  While users only need 2 ports to connect to a cluster, the nodes within a\n  cluster require a much wider range of ports to communicate.See :ref:`Ray port configuration <Ray-ports>` for a comprehensive list.Applications (such as :ref:`Ray Serve <Rayserve>`) may also require\n  additional ports to work properly.System configuration\n^^^^^^^^^^^^^^^^^^^^\n\nThere are a few system level configurations that should be set when using Ray\nat a large scale.* Make sure ``ulimit -n`` is set to at least 65535.Ray opens many direct\n  connections between worker processes to avoid bottlenecks, so it can quickly\n  use a large number of file descriptors.* Make sure ``/dev/shm`` is sufficiently large.Most ML/RL applications rely\n  heavily on the plasma store.By default, Ray will try to use ``/dev/shm`` for\n  the object store, but if it is not large enough (i.e.``--object-store-memory``\n  > size of ``/dev/shm``), Ray will write the plasma store to disk instead, which\n  may cause significant performance problems.* Use NVMe SSDs (or other high performance storage) if possible.If\n  :ref:`object spilling <object-spilling>` is enabled Ray will spill objects to\n  disk if necessary.This is most commonly needed for data processing\n  workloads... _vms-large-cluster-configure-head-node:\n\nConfiguring the head node\n^^^^^^^^^^^^^^^^^^^^^^^^^\n\nIn addition to the above changes, when deploying a large cluster, Ray's\narchitecture means that the head node has extra stress due to\nadditional system processes running on it like GCS.* A good starting hardware specification for the head node is 8 CPUs and 32 GB memory.The actual hardware specification depends on the workload and the size of the cluster.Metrics that are useful for deciding the hardware specification are\n  CPU usage, memory usage, and network bandwidth usage.* Make sure the head node has sufficient bandwidth.The most heavily stressed\n  resource on the head node is outbound bandwidth.For large clusters (see the\n  scalability envelope), we recommend using machines networking characteristics\n  at least as good as an r5dn.16xlarge on AWS EC2.* Set ``resources: {\"CPU\": 0}`` on the head node.(For Ray clusters deployed using KubeRay,\n  set ``rayStartParams: {\"num-cpus\": \"0\"}``.See the :ref:`configuration guide for KubeRay clusters <kuberay-num-cpus>`.)Due to the heavy networking load (and the GCS and dashboard processes), we\n  recommend setting the quantity of logical CPU resources to 0 on the head node\n  to avoid scheduling additional tasks on it.Configuring the autoscaler\n^^^^^^^^^^^^^^^^^^^^^^^^^^\n\nFor large, long running clusters, there are a few parameters that can be tuned.* Ensure your quotas for node types are set correctly.* For long running clusters, set the ``AUTOSCALER_MAX_NUM_FAILURES`` environment\n  variable to a large number (or ``inf``) to avoid unexpected autoscaler\n  crashes.The variable can be set by prepending \\ ``export AUTOSCALER_MAX_NUM_FAILURES=inf;``\n  to the head node's Ray start command.(Note: you may want a separate mechanism to detect if the autoscaler\n  errors too often).* For large clusters, consider tuning ``upscaling_speed`` for faster\n  autoscaling.Picking nodes\n^^^^^^^^^^^^^\n\nHere are some tips for how to set your ``available_node_types`` for a cluster,\nusing AWS instance types as a concrete example.General recommendations with AWS instance types:\n\n**When to use GPUs**\n\n* If you\u2019re using some RL/ML framework\n* You\u2019re doing something with tensorflow/pytorch/jax (some framework that can\n  leverage GPUs well)\n\n**What type of GPU?", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1fbf3014-9503-4000-a502-6561115cbcd5": {"__data__": {"id_": "1fbf3014-9503-4000-a502-6561115cbcd5", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst", "text_hash": "7d6532708a60f9bb94a4813ed3adc01e"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "278673bf77343f1d9a2c751b29b8254563c2b7fe", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst"}, "hash": "42f1ea68144a21d6adecf55c4ba6b6d65228f64552443c29466c970a5c38273d"}, "2": {"node_id": "a9c498dc-346f-437c-ab6c-263fca673dda", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst", "text_hash": "8329e7710c54d566d97d4b8d9d4f7967"}, "hash": "9e873ea3065f0b4c71957171726aa78c7465b2d414058c55affc1227ff539b5a"}}, "hash": "da112eba68369bd83674c3b6e4da9110656ea937b9b7762a1c9b4b23e9bb6f78", "text": "**\n\n* The latest gen GPU is almost always the best bang for your buck (p3 > p2, g4\n  > g3), for most well designed applications the performance outweighs the\n  price.(The instance price may be higher, but you use the instance for less\n  time.)* You may want to consider using older instances if you\u2019re doing dev work and\n  won\u2019t actually fully utilize the GPUs though.* If you\u2019re doing training (ML or RL), you should use a P instance.If you\u2019re\n  doing inference, you should use a G instance.The difference is\n  processing:VRAM ratio (training requires more memory).**What type of CPU?**\n\n* Again stick to the latest generation, they\u2019re typically cheaper and faster.* When in doubt use M instances, they have typically have the highest\n  availability.* If you know your application is memory intensive (memory utilization is full,\n  but cpu is not), go with an R instance\n* If you know your application is CPU intensive go with a C instance\n* If you have a big cluster, make the head node an instance with an n (r5dn or\n  c5n)\n\n**How many CPUs/GPUs?**\n\n* Focus on your CPU:GPU ratio first and look at the utilization (Ray dashboard\n  should help with this).If your CPU utilization is low add GPUs, or vice\n  versa.* The exact ratio will be very dependent on your workload.* Once you find a good ratio, you should be able to scale up and and keep the\n  same ratio.* You can\u2019t infinitely scale forever.Eventually, as you add more machines your\n  performance improvements will become sub-linear/not worth it.There may not\n  be a good one-size fits all strategy at this point... note::\n\n   If you're using RLlib, check out :ref:`the RLlib scaling guide\n   <rllib-scaling-guide>` for RLlib specific recommendations.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a8d475c5-a922-4cc5-8e60-79c8f5139427": {"__data__": {"id_": "a8d475c5-a922-4cc5-8e60-79c8f5139427", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "1c857d98d433399629ec66e50712acb3"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "3": {"node_id": "0a98d235-cbbf-4f4c-804e-70d935123949", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "ebceb5f2402dbdcab21feb9963ae1eb0"}, "hash": "6a2e281d456700531261b7bf001cd49069af13437fb39d20fc7ddd0a769a0f98"}}, "hash": "7ee210580617207a88ca271a9bc96445a31e7f78be099a4077b755df4109b509", "text": "# Launching Ray Clusters on AWS\n\nThis guide details the steps needed to start a Ray cluster on AWS.\n\nTo start an AWS Ray cluster, you should use the Ray cluster launcher with the AWS Python SDK.\n\n## Install Ray cluster launcher\n\nThe Ray cluster launcher is part of the `ray` CLI. Use the CLI to start, stop and attach to a running ray cluster using commands such as  `ray up`, `ray down` and `ray attach`. You can use pip to install the ray CLI with cluster launcher support. Follow [the Ray installation documentation](installation) for more detailed instructions.\n\n```bash\n# install ray\npip install -U ray[default]\n```\n\n## Install and Configure AWS Python SDK (Boto3)\n\nNext, install AWS SDK using `pip install -U boto3` and configure your AWS credentials following [the AWS guide](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/credentials.html).\n\n```bash\n# install AWS Python SDK (boto3)\npip install -U boto3\n\n# setup AWS credentials using environment variables\nexport AWS_ACCESS_KEY_ID=foo\nexport AWS_SECRET_ACCESS_KEY=bar\nexport AWS_SESSION_TOKEN=baz\n\n# alternatively, you can setup AWS credentials using ~/.aws/credentials file\necho \"[default]\naws_access_key_id=foo\naws_secret_access_key=bar\naws_session_token=baz\" >> ~/.aws/credentials\n```\n\n## Start Ray with the Ray cluster launcher\n\nOnce Boto3 is configured to manage resources in your AWS account, you should be ready to launch your cluster using the cluster launcher. The provided [cluster config file](https://github.com/ray-project/ray/tree/master/python/ray/autoscaler/aws/example-full.yaml) will create a small cluster with an m5.large head node (on-demand) configured to autoscale to up to two m5.large [spot-instance](https://aws.amazon.com/ec2/spot/) workers.\n\nTest that it works by running the following commands from your local machine:\n\n```bash\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/aws/example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote shell on the head node.\nray attach example-full.yaml\n\n# Try running a Ray program.\npython -c 'import ray; ray.init()'\nexit\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\nCongrats, you have started a Ray cluster on AWS!\n\n\nIf you want to learn more about the Ray cluster launcher, see this blog post for a [step by step guide](https://medium.com/distributed-computing-with-ray/a-step-by-step-guide-to-scaling-your-first-python-application-in-the-cloud-8761fe331ef1).\n\n\n## AWS Configurations\n\n(aws-cluster-efs)=\n\n### Using Amazon EFS\n\nTo utilize Amazon EFS in the Ray cluster, you will need to install some additional utilities and mount the EFS in `setup_commands`.Note that these instructions only work if you are using the Ray cluster launcher on AWS.```yaml\n# Note You need to replace the {{FileSystemId}} with your own EFS ID before using the config.# You may also need to modify the SecurityGroupIds for the head and worker nodes in the config file.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "0a98d235-cbbf-4f4c-804e-70d935123949": {"__data__": {"id_": "0a98d235-cbbf-4f4c-804e-70d935123949", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "ebceb5f2402dbdcab21feb9963ae1eb0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "2": {"node_id": "a8d475c5-a922-4cc5-8e60-79c8f5139427", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "1c857d98d433399629ec66e50712acb3"}, "hash": "7ee210580617207a88ca271a9bc96445a31e7f78be099a4077b755df4109b509"}, "3": {"node_id": "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "84316098357612e97da2d4328e9dbc67"}, "hash": "6a28e3d688e0877c6653e01120114a925f68540cee9841089564fb66dacdcfc8"}}, "hash": "6a2e281d456700531261b7bf001cd49069af13437fb39d20fc7ddd0a769a0f98", "text": "setup_commands:\n    - sudo kill -9 `sudo lsof /var/lib/dpkg/lock-frontend | awk '{print $2}' | tail -n 1`;\n        sudo pkill -9 apt-get;\n        sudo pkill -9 dpkg;\n        sudo dpkg --configure -a;\n        sudo apt-get -y install binutils;\n        cd $HOME;\n        git clone https://github.com/aws/efs-utils;\n        cd $HOME/efs-utils;\n        ./build-deb.sh;\n        sudo apt-get -y install ./build/amazon-efs-utils*deb;\n        cd $HOME;\n        mkdir efs;\n        sudo mount -t efs {{FileSystemId}}:/ efs;\n        sudo chmod 777 efs;\n```\n\n### Configuring IAM Role and EC2 Instance Profile\n\nBy default, Ray nodes in a Ray AWS cluster have full EC2 and S3 permissions (i.e.`arn:aws:iam::aws:policy/AmazonEC2FullAccess` and `arn:aws:iam::aws:policy/AmazonS3FullAccess`).This is a good default for trying out Ray clusters but you may want to change the permissions Ray nodes have for various reasons (e.g.to reduce the permissions for security reasons).You can do so by providing a custom `IamInstanceProfile` to the related `node_config`:\n\n```yaml\navailable_node_types:\n  ray.worker.default:\n    node_config:\n      ...\n      IamInstanceProfile:\n        Arn: arn:aws:iam::YOUR_AWS_ACCOUNT:YOUR_INSTANCE_PROFILE\n```\n\nPlease refer to this [discussion](https://github.com/ray-project/ray/issues/9327) for more details on configuring IAM role and EC2 instance profile.(aws-cluster-s3)=\n\n### Accessing S3\n\nIn various scenarios, worker nodes may need write access to an S3 bucket, e.g., Ray Tune has an option to write checkpoints to S3 instead of syncing them directly back to the driver.If you see errors like \u201cUnable to locate credentials\u201d, make sure that the correct `IamInstanceProfile` is configured for worker nodes in your cluster config file.This may look like:\n\n```yaml\navailable_node_types:\n  ray.worker.default:\n    node_config:\n      ...\n      IamInstanceProfile:\n        Arn: arn:aws:iam::YOUR_AWS_ACCOUNT:YOUR_INSTANCE_PROFILE\n```\n\nYou can verify if the set up is correct by SSHing into a worker node and running\n\n```bash\naws configure list\n```\n\nYou should see something like\n\n```bash\n      Name                    Value             Type    Location\n      ----                    -----             ----    --------\n   profile                <not set>             None    None\naccess_key     ****************XXXX         iam-role\nsecret_key     ****************YYYY         iam-role\n    region                <not set>             None    None\n```\n\nPlease refer to this [discussion](https://github.com/ray-project/ray/issues/9327) for more details on accessing S3.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d": {"__data__": {"id_": "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "84316098357612e97da2d4328e9dbc67"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "2": {"node_id": "0a98d235-cbbf-4f4c-804e-70d935123949", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "ebceb5f2402dbdcab21feb9963ae1eb0"}, "hash": "6a2e281d456700531261b7bf001cd49069af13437fb39d20fc7ddd0a769a0f98"}, "3": {"node_id": "8a43bdf4-9ca0-473b-b450-523dd1f02cce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "1b650d3c9fc273cfad234bddfc517334"}, "hash": "05ff6965bc58aaeb3bf7221ba4181aa51240e731a84394d4cf75ddba61b7055f"}}, "hash": "6a28e3d688e0877c6653e01120114a925f68540cee9841089564fb66dacdcfc8", "text": "## Monitor Ray using Amazon CloudWatch\n\n```{eval-rst}\nAmazon CloudWatch is a monitoring and observability service that provides data and actionable insights to monitor your applications, respond to system-wide performance changes, and optimize resource utilization.CloudWatch integration with Ray requires an AMI (or Docker image) with the Unified CloudWatch Agent pre-installed.AMIs with the Unified CloudWatch Agent pre-installed are provided by the Amazon Ray Team, and are currently available in the us-east-1, us-east-2, us-west-1, and us-west-2 regions.Please direct any questions, comments, or issues to the `Amazon Ray Team <https://github.com/amzn/amazon-ray/issues/new/choose>`_.The table below lists AMIs with the Unified CloudWatch Agent pre-installed in each region, and you can also find AMIs at `amazon-ray README <https://github.com/amzn/amazon-ray>`_... list-table:: All available unified CloudWatch agent images\n\n    * - Base AMI\n      - AMI ID\n      - Region\n      - Unified CloudWatch Agent Version\n    * - AWS Deep Learning AMI (Ubuntu 18.04, 64-bit)\n      - ami-069f2811478f86c20\n      - us-east-1\n      - v1.247348.0b251302\n    * - AWS Deep Learning AMI (Ubuntu 18.04, 64-bit)\n      - ami-058cc0932940c2b8b\n      - us-east-2\n      - v1.247348.0b251302\n    * - AWS Deep Learning AMI (Ubuntu 18.04, 64-bit)\n      - ami-044f95c9ef12883ef\n      - us-west-1\n      - v1.247348.0b251302\n    * - AWS Deep Learning AMI (Ubuntu 18.04, 64-bit)\n      - ami-0d88d9cbe28fac870\n      - us-west-2\n      - v1.247348.0b251302\n\n.. note::\n\n    Using Amazon CloudWatch will incur charges, please refer to `CloudWatch pricing <https://aws.amazon.com/cloudwatch/pricing/>`_ for details.Getting started\n---------------\n\n1.Create a minimal cluster config YAML named ``cloudwatch-basic.yaml`` with the following contents:\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: yaml\n\n    provider:\n        type: aws\n        region: us-west-2\n        availability_zone: us-west-2a\n        # Start by defining a `cloudwatch` section to enable CloudWatch integration with your Ray cluster.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a43bdf4-9ca0-473b-b450-523dd1f02cce": {"__data__": {"id_": "8a43bdf4-9ca0-473b-b450-523dd1f02cce", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "1b650d3c9fc273cfad234bddfc517334"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "2": {"node_id": "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "84316098357612e97da2d4328e9dbc67"}, "hash": "6a28e3d688e0877c6653e01120114a925f68540cee9841089564fb66dacdcfc8"}, "3": {"node_id": "bceca43a-0a80-4673-8e63-ab6e08fb66ea", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "42874d8b5823b21a595160b9594dd56f"}, "hash": "3c80faca73e557a3e24eee5094300a0c1a192b4e9d7064d5e23f74ae977521f2"}}, "hash": "05ff6965bc58aaeb3bf7221ba4181aa51240e731a84394d4cf75ddba61b7055f", "text": "cloudwatch:\n            agent:\n                # Path to Unified CloudWatch Agent config file\n                config: \"cloudwatch/example-cloudwatch-agent-config.json\"\n            dashboard:\n                # CloudWatch Dashboard name\n                name: \"example-dashboard-name\"\n                # Path to the CloudWatch Dashboard config file\n                config: \"cloudwatch/example-cloudwatch-dashboard-config.json\"\n\n    auth:\n        ssh_user: ubuntu\n\n    available_node_types:\n        ray.head.default:\n            node_config:\n            InstanceType: c5a.large\n            ImageId: ami-0d88d9cbe28fac870  # Unified CloudWatch agent pre-installed AMI, us-west-2\n            resources: {}\n        ray.worker.default:\n            node_config:\n                InstanceType: c5a.large\n                ImageId: ami-0d88d9cbe28fac870  # Unified CloudWatch agent pre-installed AMI, us-west-2\n                IamInstanceProfile:\n                    Name: ray-autoscaler-cloudwatch-v1\n            resources: {}\n            min_workers: 0\n\n2.Download CloudWatch Agent and Dashboard config.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nFirst, create a ``cloudwatch`` directory in the same directory as ``cloudwatch-basic.yaml``.Then, download the example `CloudWatch Agent <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-agent-config.json>`_ and `CloudWatch Dashboard <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-dashboard-config.json>`_ config files to the ``cloudwatch`` directory... code-block:: console\n\n    $ mkdir cloudwatch\n    $ cd cloudwatch\n    $ wget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-agent-config.json\n    $ wget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-dashboard-config.json\n\n3.Run ``ray up cloudwatch-basic.yaml`` to start your Ray Cluster.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThis will launch your Ray cluster in ``us-west-2`` by default.When launching a cluster for a different region, you'll need to change your cluster config YAML file's ``region`` AND ``ImageId``.See the \"Unified CloudWatch Agent Images\" table above for available AMIs by region.4.Check out your Ray cluster's logs, metrics, and dashboard in the `CloudWatch Console <https://console.aws.amazon.com/cloudwatch/>`_!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bceca43a-0a80-4673-8e63-ab6e08fb66ea": {"__data__": {"id_": "bceca43a-0a80-4673-8e63-ab6e08fb66ea", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "42874d8b5823b21a595160b9594dd56f"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "2": {"node_id": "8a43bdf4-9ca0-473b-b450-523dd1f02cce", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "1b650d3c9fc273cfad234bddfc517334"}, "hash": "05ff6965bc58aaeb3bf7221ba4181aa51240e731a84394d4cf75ddba61b7055f"}, "3": {"node_id": "1b9061fe-12be-4fc3-96cf-f5ffb4da053f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "4cb4ade2c0bc544e4600b0f4dc4a481a"}, "hash": "db379bbd85e6e6cd25a79f719798aac3c215d778aaf7f6725a1e6163a7d8460d"}}, "hash": "3c80faca73e557a3e24eee5094300a0c1a192b4e9d7064d5e23f74ae977521f2", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nA tail can be acquired on all logs written to a CloudWatch log group by ensuring that you have the `AWS CLI V2+ installed <https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html>`_ and then running:\n\n.. code-block:: bash\n\n    aws logs tail $log_group_name --follow\n\nAdvanced Setup\n--------------\n\nRefer to `example-cloudwatch.yaml <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-cloudwatch.yaml>`_ for a complete example.1.Choose an AMI with the Unified CloudWatch Agent pre-installed.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nEnsure that you're launching your Ray EC2 cluster in the same region as the AMI,\nthen specify the ``ImageId`` to use with your cluster's head and worker nodes in your cluster config YAML file.The following CLI command returns the latest available Unified CloudWatch Agent Image for ``us-west-2``:\n\n.. code-block:: bash\n\n    aws ec2 describe-images --region us-west-2 --filters \"Name=owner-id,Values=160082703681\" \"Name=name,Values=*cloudwatch*\" --query 'Images[*].[ImageId,CreationDate]' --output text | sort -k2 -r | head -n1\n\n.. code-block:: yaml\n\n    available_node_types:\n        ray.head.default:\n            node_config:\n            InstanceType: c5a.large\n            ImageId: ami-0d88d9cbe28fac870\n        ray.worker.default:\n            node_config:\n            InstanceType: c5a.large\n            ImageId: ami-0d88d9cbe28fac870\n\nTo build your own AMI with the Unified CloudWatch Agent installed:\n\n1.Follow the `CloudWatch Agent Installation <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-EC2-Instance.html>`_ user guide to install the Unified CloudWatch Agent on an EC2 instance.2.Follow the `EC2 AMI Creation <https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIs.html#creating-an-ami>`_ user guide to create an AMI from this EC2 instance.2.Define your own CloudWatch Agent, Dashboard, and Alarm JSON config files.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can start by using the example `CloudWatch Agent <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-agent-config.json>`_, `CloudWatch Dashboard <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-dashboard-config.json>`_ and `CloudWatch Alarm <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/cloudwatch/example-cloudwatch-alarm-config.json>`_ config files.These example config files include the following features:\n\n**Logs and Metrics**:  Logs written to ``/tmp/ray/session_*/logs/**.out`` will be available in the ``{cluster_name}-ray_logs_out`` log group,\nand logs written to ``/tmp/ray/session_*/logs/**.err`` will be available in the ``{cluster_name}-ray_logs_err`` log group.Log streams are named after the EC2 instance ID that emitted their logs.Extended EC2 metrics including CPU/Disk/Memory usage and process statistics can be found in the ``{cluster_name}-ray-CWAgent`` metric namespace.**Dashboard**: You will have a cluster-level dashboard showing total cluster CPUs and available object store memory.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1b9061fe-12be-4fc3-96cf-f5ffb4da053f": {"__data__": {"id_": "1b9061fe-12be-4fc3-96cf-f5ffb4da053f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "4cb4ade2c0bc544e4600b0f4dc4a481a"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "2": {"node_id": "bceca43a-0a80-4673-8e63-ab6e08fb66ea", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "42874d8b5823b21a595160b9594dd56f"}, "hash": "3c80faca73e557a3e24eee5094300a0c1a192b4e9d7064d5e23f74ae977521f2"}, "3": {"node_id": "b7031aee-71f2-463d-9b3c-6abc04f76c70", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "e0c0dbccd2595c0e565c46981b0835f1"}, "hash": "71b96fca1c6bb6d5261349cbc9a9cda293c7b85040b5243376d6bf0df0623bc8"}}, "hash": "db379bbd85e6e6cd25a79f719798aac3c215d778aaf7f6725a1e6163a7d8460d", "text": "Process counts, disk usage, memory usage, and CPU utilization will be displayed as both cluster-level sums and single-node maximums/averages.**Alarms**: Node-level alarms tracking prolonged high memory, disk, and CPU usage are configured.Alarm actions are NOT set,\nand must be manually provided in your alarm config file.For more advanced options, see the `Agent <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html>`_, `Dashboard <https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/CloudWatch-Dashboard-Body-Structure.html>`_ and `Alarm <https://docs.aws.amazon.com/AmazonCloudWatch/latest/APIReference/API_PutMetricAlarm.html>`_ config user guides.CloudWatch Agent, Dashboard, and Alarm JSON config files support the following variables:\n\n``{instance_id}``: Replaced with each EC2 instance ID in your Ray cluster.``{region}``: Replaced with your Ray cluster's region.``{cluster_name}``: Replaced with your Ray cluster name.See CloudWatch Agent `Configuration File Details <https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Agent-Configuration-File-Details.html>`_ for additional variables supported natively by the Unified CloudWatch Agent... note::\n    Remember to replace the ``AlarmActions`` placeholder in your CloudWatch Alarm config file!.. code-block:: json\n\n     \"AlarmActions\":[\n         \"TODO: Add alarm actions!See https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/AlarmThatSendsEmail.html\"\n      ]\n\n3.Reference your CloudWatch JSON config files in your cluster config YAML.~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nSpecify the file path to your CloudWatch JSON config files relative to the working directory that you will run ``ray up`` from:\n\n.. code-block:: yaml\n\n     provider:\n        cloudwatch:\n            agent:\n                config: \"cloudwatch/example-cloudwatch-agent-config.json\"", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b7031aee-71f2-463d-9b3c-6abc04f76c70": {"__data__": {"id_": "b7031aee-71f2-463d-9b3c-6abc04f76c70", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "e0c0dbccd2595c0e565c46981b0835f1"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md"}, "hash": "41395ea5f6a7ad6158e4a6cb575331ceded14384557292f4325a973ce0b6cb83"}, "2": {"node_id": "1b9061fe-12be-4fc3-96cf-f5ffb4da053f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "4cb4ade2c0bc544e4600b0f4dc4a481a"}, "hash": "db379bbd85e6e6cd25a79f719798aac3c215d778aaf7f6725a1e6163a7d8460d"}}, "hash": "71b96fca1c6bb6d5261349cbc9a9cda293c7b85040b5243376d6bf0df0623bc8", "text": "4. Set your IAM Role and EC2 Instance Profile.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy default the ``ray-autoscaler-cloudwatch-v1`` IAM role and EC2 instance profile is created at Ray cluster launch time.\nThis role contains all additional permissions required to integrate CloudWatch with Ray, namely the ``CloudWatchAgentAdminPolicy``, ``AmazonSSMManagedInstanceCore``, ``ssm:SendCommand``, ``ssm:ListCommandInvocations``, and ``iam:PassRole`` managed policies.\n\nEnsure that all worker nodes are configured to use the ``ray-autoscaler-cloudwatch-v1`` EC2 instance profile in your cluster config YAML:\n\n.. code-block:: yaml\n\n    ray.worker.default:\n        node_config:\n            InstanceType: c5a.large\n            IamInstanceProfile:\n                Name: ray-autoscaler-cloudwatch-v1\n\n5. Export Ray system metrics to CloudWatch.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nTo export Ray's Prometheus system metrics to CloudWatch, first ensure that your cluster has the\nRay Dashboard installed, then uncomment the ``head_setup_commands`` section in `example-cloudwatch.yaml file <https://github.com/ray-project/ray/blob/master/python/ray/autoscaler/aws/example-cloudwatch.yaml>`_ file.\nYou can find Ray Prometheus metrics in the ``{cluster_name}-ray-prometheus`` metric namespace.\n\n.. code-block:: yaml\n\n    head_setup_commands:\n  # Make `ray_prometheus_waiter.sh` executable.\n  - >-\n    RAY_INSTALL_DIR=`pip show ray | grep -Po \"(?<=Location:).*\"`\n    && sudo chmod +x $RAY_INSTALL_DIR/ray/autoscaler/aws/cloudwatch/ray_prometheus_waiter.sh\n  # Copy `prometheus.yml` to Unified CloudWatch Agent folder\n  - >-\n    RAY_INSTALL_DIR=`pip show ray | grep -Po \"(?<=Location:).*\"`\n    && sudo cp -f $RAY_INSTALL_DIR/ray/autoscaler/aws/cloudwatch/prometheus.yml /opt/aws/amazon-cloudwatch-agent/etc\n  # First get current cluster name, then let the Unified CloudWatch Agent restart and use `AmazonCloudWatch-ray_agent_config_{cluster_name}` parameter at SSM Parameter Store.\n  - >-\n    nohup sudo sh -c \"`pip show ray | grep -Po \"(?<=Location:).*\"`/ray/autoscaler/aws/cloudwatch/ray_prometheus_waiter.sh\n    `cat ~/ray_bootstrap_config.yaml | jq '.cluster_name'`\n    >> '/opt/aws/amazon-cloudwatch-agent/logs/ray_prometheus_waiter.out' 2>> '/opt/aws/amazon-cloudwatch-agent/logs/ray_prometheus_waiter.err'\" &\n\n6. Update CloudWatch Agent, Dashboard and Alarm config files.\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can apply changes to the CloudWatch Logs, Metrics, Dashboard, and Alarms for your cluster by simply modifying the CloudWatch config files referenced by your Ray cluster config YAML and re-running ``ray up example-cloudwatch.yaml``.\nThe Unified CloudWatch Agent will be automatically restarted on all cluster nodes, and your config changes will be applied.\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d4dea8b6-dc1b-44eb-af42-b042825d58a2": {"__data__": {"id_": "d4dea8b6-dc1b-44eb-af42-b042825d58a2", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md", "text_hash": "5ce9af30e9db1e36dffa13440eea43e7"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "398092b12d52dde908ff05170dff8012d97713c4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md"}, "hash": "19ac70004c600426e482856443c25dc42b5d410385680ba3944957e04686983a"}, "3": {"node_id": "8a045afd-bbe8-487b-afc8-7f139db9e45d", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md", "text_hash": "7aff584bf4c62bd7c2e2152194a52266"}, "hash": "d40a522a99d8b3c74fbeca2d3d048eb3e16be611571c44e7396466fbd96fc1ae"}}, "hash": "64cfe517385fb2467db62d2a501069feb81794228d6332134d8c15eab7defc61", "text": "# Launching Ray Clusters on Azure\n\nThis guide details the steps needed to start a Ray cluster on Azure.\n\nThere are two ways to start an Azure Ray cluster.\n- Launch through Ray cluster launcher.\n- Deploy a cluster using Azure portal.\n\n```{note}\nThe Azure integration is community-maintained. Please reach out to the integration maintainers on Github if\nyou run into any problems: gramhagen, eisber, ijrsvt.\n```\n\n## Using Ray cluster launcher\n\n\n### Install Ray cluster launcher\n\nThe Ray cluster launcher is part of the `ray` CLI. Use the CLI to start, stop and attach to a running ray cluster using commands such as  `ray up`, `ray down` and `ray attach`. You can use pip to install the ray CLI with cluster launcher support. Follow [the Ray installation documentation](installation) for more detailed instructions.\n\n```bash\n# install ray\npip install -U ray[default]\n```\n\n### Install and Configure Azure CLI\n\nNext, install the Azure CLI (`pip install -U azure-cli azure-identity`) and login using `az login`.\n\n```bash\n# Install azure cli.\npip install azure-cli azure-identity\n\n# Login to azure. This will redirect you to your web browser.\naz login\n```\n\n### Start Ray with the Ray cluster launcher\n\n\nThe provided [cluster config file](https://github.com/ray-project/ray/tree/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/azure/example-full.yaml) will create a small cluster with a Standard DS2v3 on-demand head node that is configured to autoscale to up to two Standard DS2v3 [spot-instance](https://docs.microsoft.com/en-us/azure/virtual-machines/windows/spot-vms) worker nodes.\n\nNote that you'll need to fill in your Azure [resource_group](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/azure/example-full.yaml#L42) and [location](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/azure/example-full.yaml#L41) in those templates. You also need set the subscription to use. You can do this from the command line with `az account set -s <subscription_id>` or by filling in the [subscription_id](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/azure/example-full.yaml#L44) in the cluster config file.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8a045afd-bbe8-487b-afc8-7f139db9e45d": {"__data__": {"id_": "8a045afd-bbe8-487b-afc8-7f139db9e45d", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md", "text_hash": "7aff584bf4c62bd7c2e2152194a52266"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "398092b12d52dde908ff05170dff8012d97713c4", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md"}, "hash": "19ac70004c600426e482856443c25dc42b5d410385680ba3944957e04686983a"}, "2": {"node_id": "d4dea8b6-dc1b-44eb-af42-b042825d58a2", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md", "text_hash": "5ce9af30e9db1e36dffa13440eea43e7"}, "hash": "64cfe517385fb2467db62d2a501069feb81794228d6332134d8c15eab7defc61"}}, "hash": "d40a522a99d8b3c74fbeca2d3d048eb3e16be611571c44e7396466fbd96fc1ae", "text": "Test that it works by running the following commands from your local machine:\n\n```bash\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/azure/example-full.yaml\n\n# Update the example-full.yaml to update resource_group, location, and subscription_id.\n# vi example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote screen on the head node.\nray attach example-full.yaml\n# Try running a Ray program.\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\nCongratulations, you have started a Ray cluster on Azure!\n\n## Using Azure portal\n\nAlternatively, you can deploy a cluster using Azure portal directly. Please note that autoscaling is done using Azure VM Scale Sets and not through the Ray autoscaler. This will deploy [Azure Data Science VMs (DSVM)](https://azure.microsoft.com/en-us/services/virtual-machines/data-science-virtual-machines/) for both the head node and the auto-scalable cluster managed by [Azure Virtual Machine Scale Sets](https://azure.microsoft.com/en-us/services/virtual-machine-scale-sets/).\nThe head node conveniently exposes both SSH as well as JupyterLab.\n\n\n\nOnce the template is successfully deployed the deployment Outputs page provides the ssh command to connect and the link to the JupyterHub on the head node (username/password as specified on the template input).\nUse the following code in a Jupyter notebook (using the conda environment specified in the template input, py38_tensorflow by default) to connect to the Ray cluster.\n\n```python\nimport ray; ray.init()\n```\n\nUnder the hood, the [azure-init.sh](https://github.com/ray-project/ray/blob/master/doc/azure/azure-init.sh) script is executed and performs the following actions:\n\n1. Activates one of the conda environments available on DSVM\n2. Installs Ray and any other user-specified dependencies\n3. Sets up a systemd task (``/lib/systemd/system/ray.service``) to start Ray in head or worker mode", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1bc2777c-607f-44aa-8e49-893520d55e91": {"__data__": {"id_": "1bc2777c-607f-44aa-8e49-893520d55e91", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/gcp.md", "file_name": "gcp.md", "text_hash": "2e9046b5a9c3a64a89f154150f93b27c"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "37cf48193f890dda1585315591c9d9582be0ba09", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/gcp.md", "file_name": "gcp.md"}, "hash": "21701db1c37b8105f98bf25f1a2cc833ef96dab00dc7f00703aa98c261e548e1"}}, "hash": "eabb7c59b6f2ca869e7bc763027331ec8c071a9d1dcf9130ded39d1192fea57f", "text": "# Launching Ray Clusters on GCP\n\nThis guide details the steps needed to start a Ray cluster in GCP.\n\nTo start a GCP Ray cluster, you will use the Ray cluster launcher with the Google API client.\n\n## Install Ray cluster launcher\n\nThe Ray cluster launcher is part of the `ray` CLI. Use the CLI to start, stop and attach to a running ray cluster using commands such as `ray up`, `ray down` and `ray attach`. You can use pip to install the ray CLI with cluster launcher support. Follow [the Ray installation documentation](installation) for more detailed instructions.\n\n```bash\n# install ray\npip install -U ray[default]\n```\n\n## Install and Configure Google API Client\n\nIf you have never created a Google APIs Console project, read google Cloud's [Managing Projects page](https://cloud.google.com/resource-manager/docs/creating-managing-projects?visit_id=637952351450670909-433962807&rd=1) and create a project in the [Google API Console](https://console.developers.google.com/).\nNext, install the Google API Client using `pip install -U google-api-python-client`.\n\n```bash\n# Install the Google API Client.\npip install google-api-python-client\n```\n\n## Start Ray with the Ray cluster launcher\n\nOnce the Google API client is configured to manage resources on your GCP account, you should be ready to launch your cluster. The provided [cluster config file](https://github.com/ray-project/ray/tree/master/python/ray/autoscaler/gcp/example-full.yaml) will create a small cluster with an on-demand n1-standard-2 head node and is configured to autoscale to up to two n1-standard-2 [preemptible workers](https://cloud.google.com/preemptible-vms/). Note that you'll need to fill in your GCP [project_id](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/gcp/example-full.yaml#L42) in those templates.\n\nTest that it works by running the following commands from your local machine:\n\n```bash\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/gcp/example-full.yaml\n\n# Edit the example-full.yaml to update project_id.\n# vi example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote screen on the head node.\nray attach example-full.yaml\n\n# Try running a Ray program.\npython -c 'import ray; ray.init()'\nexit\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\nCongrats, you have started a Ray cluster on GCP!\n\n## GCP Configurations\n\n### Running workers with Service Accounts\n\nBy default, only the head node runs with a Service Account (`ray-autoscaler-sa-v1@<project-id>.iam.gserviceaccount.com`). To enable workers to run with this same Service Account (to access Google Cloud Storage, or GCR), add the following configuration to the worker_node configuration:\n\n```yaml\navailable_node_types:\n  ray.worker.default:\n    node_config:\n      ...\n    serviceAccounts:\n    - email: ray-autoscaler-sa-v1@<YOUR_PROJECT_ID>.iam.gserviceaccount.com\n        scopes:\n        - https://www.googleapis.com/auth/cloud-platform\n```", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "855344ff-0f0b-4db6-b9f2-fa79bd7366c0": {"__data__": {"id_": "855344ff-0f0b-4db6-b9f2-fa79bd7366c0", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/index.rst", "file_name": "index.rst", "text_hash": "b0be857625b68ec1af527533159a43c0"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "d84782edd1e2dd22c61e4a2d1edcf723465db8b0", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/index.rst", "file_name": "index.rst"}, "hash": "a7e7b7bae9a71e7676ea199255c3737b56b1ecf8a17edc1c8339ab5876be8512"}}, "hash": "5ab1b0950a6d62be111c009f22755b7c3f57974516a41c7380be34bfe8df0c8d", "text": ".. _launching-vm-clusters:\n\nLaunching Ray Clusters on AWS, GCP, Azure, On-Prem\n==================================================\n\nIn this section, you can find guides for launching Ray clusters in various clouds or on-premises.\n\nTable of Contents\n-----------------\n\n.. toctree::\n    :maxdepth: 2\n\n    aws.md\n    gcp.md\n    azure.md\n    on-premises.md", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e1c43659-e2c1-4058-94b2-31494c457f21": {"__data__": {"id_": "e1c43659-e2c1-4058-94b2-31494c457f21", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md", "text_hash": "657d2d98eb1e78ecc46befd864fc9b40"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fe7fb7d482e1dbd4c39940bda29c88aafcf3af1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md"}, "hash": "955ef8d130d5051934034fc8f99d120d454a2e7b27100a499c3474a9fb7fbb21"}, "3": {"node_id": "2081eda7-15ff-4992-9202-8f0c8c7da30f", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md", "text_hash": "608facaf23647f204a373d0484de26cd"}, "hash": "702554892dfa014b032964aabc9daad8210fb0acb7078213c20ef5f43a4193ea"}}, "hash": "96769d8866409455aebb69f92e4b15c2d871690e67024ca54485965ac9221466", "text": "(on-prem)=\n\n# Launching an On-Premise Cluster\n\nThis document describes how to set up an on-premise Ray cluster, i.e., to run Ray on bare metal machines, or in a private cloud.We provide two ways to start an on-premise cluster.* You can [manually set up](manual-setup-cluster) the Ray cluster by installing the Ray package and starting the Ray processes on each node.* Alternatively, if you know all the nodes in advance and have SSH access to them, you should start the Ray cluster using the [cluster-launcher](manual-cluster-launcher).(manual-setup-cluster)=\n\n## Manually Set up a Ray Cluster\nThis section assumes that you have a list of machines and that the nodes in the cluster share the same network.It also assumes that Ray is installed on each machine.You can use pip to install the ray command line tool with cluster launcher support.Follow the [Ray installation instructions](installation) for more details.```bash\n# install ray\npip install -U \"ray[default]\"\n```\n\n### Start the Head Node\nChoose any node to be the head node and run the following.If the `--port` argument is omitted, Ray will first choose port 6379, and then fall back to a random port if in 6379 is in use.```bash\nray start --head --port=6379\n```\n\nThe command will print out the Ray cluster address, which can be passed to `ray start` on other machines to start the worker nodes (see below).If you receive a ConnectionError, check your firewall settings and network configuration.### Start Worker Nodes\nThen on each of the other nodes, run the following command to connect to the head node you just created.```bash\nray start --address=<head-node-address:port>\n```\nMake sure to replace `head-node-address:port` with the value printed by the command on the head node (it should look something like 123.45.67.89:6379).Note that if your compute nodes are on their own subnetwork with Network Address Translation, the address printed by the head node will not work if connecting from a machine outside that subnetwork.You will need to use a head node address reachable from the remote machine.If the head node has a domain address like compute04.berkeley.edu, you can simply use that in place of an IP address and rely on DNS.Ray auto-detects the resources (e.g., CPU) available on each node, but you can also manually override this by passing custom resources to the `ray start` command.For example, if you wish to specify that a machine has 10 CPUs and 1 GPU available for use by Ray, you can do this with the flags `--num-cpus=10` and `--num-gpus=1`.See the [Configuration page](configuring-ray) for more information.### Troubleshooting\n\nIf you see `Unable to connect to GCS at ...`, this means the head node is inaccessible at the given `--address`.Some possible causes include:\n\n- the head node is not actually running;\n- a different version of Ray is running at the specified address;\n- the specified address is wrong;\n- or there are firewall settings preventing access.If the connection fails, to check whether each port can be reached from a node, you can use a tool such as nmap or nc.```bash\n$ nmap -sV --reason -p $PORT $HEAD_ADDRESS\nNmap scan report for compute04.berkeley.edu (123.456.78.910)\nHost is up, received echo-reply ttl 60 (0.00087s latency).rDNS record for 123.456.78.910: compute04.berkeley.edu\nPORT     STATE SERVICE REASON         VERSION\n6379/tcp open  redis?syn-ack\nService detection performed.Please report any incorrect results at https://nmap.org/submit/ .$ nc -vv -z $HEAD_ADDRESS $PORT\nConnection to compute04.berkeley.edu 6379 port [tcp/*] succeeded!```\n\nIf the node cannot access that port at that IP address, you might see\n\n```bash\n$ nmap -sV --reason -p $PORT $HEAD_ADDRESS\nNmap scan report for compute04.berkeley.edu (123.456.78.910)\nHost is up (0.0011s latency).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "2081eda7-15ff-4992-9202-8f0c8c7da30f": {"__data__": {"id_": "2081eda7-15ff-4992-9202-8f0c8c7da30f", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md", "text_hash": "608facaf23647f204a373d0484de26cd"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "2fe7fb7d482e1dbd4c39940bda29c88aafcf3af1", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md"}, "hash": "955ef8d130d5051934034fc8f99d120d454a2e7b27100a499c3474a9fb7fbb21"}, "2": {"node_id": "e1c43659-e2c1-4058-94b2-31494c457f21", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md", "text_hash": "657d2d98eb1e78ecc46befd864fc9b40"}, "hash": "96769d8866409455aebb69f92e4b15c2d871690e67024ca54485965ac9221466"}}, "hash": "702554892dfa014b032964aabc9daad8210fb0acb7078213c20ef5f43a4193ea", "text": "rDNS record for 123.456.78.910: compute04.berkeley.edu\nPORT     STATE  SERVICE REASON       VERSION\n6379/tcp closed redis   reset ttl 60\nService detection performed.Please report any incorrect results at https://nmap.org/submit/ .$ nc -vv -z $HEAD_ADDRESS $PORT\nnc: connect to compute04.berkeley.edu port 6379 (tcp) failed: Connection refused\n```\n(manual-cluster-launcher)=\n\n## Using Ray cluster launcher\n\nThe Ray cluster launcher is part of the `ray` command line tool.It allows you to start, stop and attach to a running ray cluster using commands such as  `ray up`, `ray down` and `ray attach`.You can use pip to install it, or follow [install ray](installation) for more detailed instructions.```bash\n# install ray\npip install \"ray[default]\"\n```\n\n### Start Ray with the Ray cluster launcher\n\nThe provided [example-full.yaml](https://github.com/ray-project/ray/tree/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/local/example-full.yaml) cluster config file will create a Ray cluster given a list of nodes.Note that you'll need to fill in your [head_ip](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/local/example-full.yaml#L20), a list of [worker_ips](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/local/example-full.yaml#L26), and the [ssh_user](https://github.com/ray-project/ray/blob/eacc763c84d47c9c5b86b26a32fd62c685be84e6/python/ray/autoscaler/local/example-full.yaml#L34) field in those templates\n\n\n\nTest that it works by running the following commands from your local machine:\n\n```bash\n# Download the example-full.yaml\nwget https://raw.githubusercontent.com/ray-project/ray/master/python/ray/autoscaler/local/example-full.yaml\n\n# Update the example-full.yaml to update head_ip, worker_ips, and ssh_user.\n# vi example-full.yaml\n\n# Create or update the cluster. When the command finishes, it will print\n# out the command that can be used to SSH into the cluster head node.\nray up example-full.yaml\n\n# Get a remote screen on the head node.\nray attach example-full.yaml\n# Try running a Ray program.\n\n# Tear down the cluster.\nray down example-full.yaml\n```\n\nCongrats, you have started a local Ray cluster!", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d02eaca7-5279-4f09-9530-450dd3c5d25d": {"__data__": {"id_": "d02eaca7-5279-4f09-9530-450dd3c5d25d", "embedding": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/logging.md", "file_name": "logging.md", "text_hash": "622ba4cb5280564613a4776d87d07732"}, "excluded_embed_metadata_keys": [], "excluded_llm_metadata_keys": [], "relationships": {"1": {"node_id": "712f452dc469ddb8d1d08de74cbc2c7fff25f739", "node_type": null, "metadata": {"file_path": "doc/source/cluster/vms/user-guides/logging.md", "file_name": "logging.md"}, "hash": "1fc629de1ef7e584f8e0135ed691f569e52638b258848ae3904adf715fca3b1a"}}, "hash": "8d60407a47697e47aa65005356a948ee96d604703ebc60002375c362aed1da03", "text": "(vm-logging)=\n# Log Persistence\n\nLogs are useful for troubleshooting Ray applications and Clusters. For example, you may want to access system logs if a node terminates unexpectedly.\n\nRay does not provide a native storage solution for log data. Users need to manage the lifecycle of the logs by themselves. The following sections provide instructions on how to collect logs from Ray Clusters running on VMs.\n\n## Ray log directory\nBy default, Ray writes logs to files in the directory `/tmp/ray/session_*/logs` on each Ray node's file system, including application logs and system logs. Learn more about the {ref}`log directory and log files <logging-directory>` and the {ref}`log rotation configuration <log-rotation>` before you start to collect logs.\n\n\n## Log processing tools\n\nA number of open source log processing tools are available, such as [Vector][Vector], [FluentBit][FluentBit], [Fluentd][Fluentd], [Filebeat][Filebeat], and [Promtail][Promtail].\n\n[Vector]: https://vector.dev/\n[FluentBit]: https://docs.fluentbit.io/manual\n[Filebeat]: https://www.elastic.co/guide/en/beats/filebeat/7.17/index.html\n[Fluentd]: https://docs.fluentd.org/\n[Promtail]: https://grafana.com/docs/loki/latest/clients/promtail/\n\n## Log collection\n\nAfter choosing a log processing tool based on your needs, you may need to perform the following steps:\n\n1. Ingest log files on each node of your Ray Cluster as sources.\n2. Parse and transform the logs. You may want to use {ref}`Ray's structured logging <structured-logging>` to simplify this step.\n3. Ship the transformed logs to log storage or management systems.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"619f0028e26d2ca0229c368ae863b591d1ae0f1e": {"node_ids": ["cf7447bd-c777-490d-a105-15d791d5cb49", "cf7447bd-c777-490d-a105-15d791d5cb49"], "metadata": {"file_path": "doc/source/cluster/cli.rst", "file_name": "cli.rst", "text_hash": "4a09c37c744b7b3c032dbceea812a319"}}, "92f6124876c2ae06d5c5e6f9e5f473e1033485d0": {"node_ids": ["ba92b10a-578b-445b-bb26-b66260508667", "3953411a-b66d-4891-a4e2-f3ff12be661e", "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984", "a2d5ebeb-0b87-4056-861f-eadb30cac546", "a8d93d42-4550-4559-a0c9-e2c5a1bbd9ef", "ba92b10a-578b-445b-bb26-b66260508667", "3953411a-b66d-4891-a4e2-f3ff12be661e", "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984", "a2d5ebeb-0b87-4056-861f-eadb30cac546", "a8d93d42-4550-4559-a0c9-e2c5a1bbd9ef"], "metadata": {"file_path": "doc/source/cluster/configure-manage-dashboard.md", "file_name": "configure-manage-dashboard.md", "text_hash": "ca1d2f3171e8d84e084f59b1d27cbb7a"}}, "a44cc38177f171d99f64433945351c51348a6412": {"node_ids": ["e688aa7d-7a01-4d26-a0f8-53ab714d6ee8", "863aac4d-a07a-451b-8fdc-d12ad2d43134", "e688aa7d-7a01-4d26-a0f8-53ab714d6ee8", "863aac4d-a07a-451b-8fdc-d12ad2d43134"], "metadata": {"file_path": "doc/source/cluster/faq.rst", "file_name": "faq.rst", "text_hash": "e4bf36472cb68ab2e04d6f72710b5b65"}}, "d86c419db93279c5f8e539528563b29a3ef9cad5": {"node_ids": ["ae00c381-47aa-408e-a304-9bc55be4ce1c", "eb7fded7-c10b-47bb-b1db-518895993fee", "ae00c381-47aa-408e-a304-9bc55be4ce1c", "eb7fded7-c10b-47bb-b1db-518895993fee"], "metadata": {"file_path": "doc/source/cluster/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "e246227ff25d5515d65348f76823ce3f"}}, "09b54b0a2a9f4cf455148263384a536fd1c639e0": {"node_ids": ["ca3ee395-69e9-4f14-b779-437199ce23d1", "ca3ee395-69e9-4f14-b779-437199ce23d1"], "metadata": {"file_path": "doc/source/cluster/key-concepts.rst", "file_name": "key-concepts.rst", "text_hash": "6c8bcff56cfd7524c8ee83ce40e56f5e"}}, "0bf5221971f6b8b5df8ae3196a2172ed0d59bcef": {"node_ids": ["1fe0ce81-f9ec-47b0-93fb-c3780961a18e", "1fe0ce81-f9ec-47b0-93fb-c3780961a18e"], "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks.md", "file_name": "benchmarks.md", "text_hash": "26c34ed09dffbd56cd5a93a244fb896c"}}, "d8a469417f56973e8e700003f5a35e61cf96beab": {"node_ids": ["254f4b24-2fae-416c-8335-602e73835fce", "672ee971-7834-49fb-a0a0-128ec84ed8d0", "254f4b24-2fae-416c-8335-602e73835fce", "672ee971-7834-49fb-a0a0-128ec84ed8d0"], "metadata": {"file_path": "doc/source/cluster/kubernetes/benchmarks/memory-scalability-benchmark.md", "file_name": "memory-scalability-benchmark.md", "text_hash": "bd95ae63529c2bbde5ae05c3aeb18559"}}, "b6158a2e1f824622aadd10423a4b92218966f453": {"node_ids": ["a1f4bb2b-9424-4dbc-83e6-d52f25d2c885", "a1f4bb2b-9424-4dbc-83e6-d52f25d2c885"], "metadata": {"file_path": "doc/source/cluster/kubernetes/examples.md", "file_name": "examples.md", "text_hash": "0100bf44d0783b71c5ad7c5b3e744483"}}, "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379": {"node_ids": ["d1292c35-6ed4-4667-84b4-c1a45dba25cf", "25fa0e5c-0ca9-433b-9c62-73307a129946", "754d4905-e97d-4d97-909d-ed531f7b0f32", "d1292c35-6ed4-4667-84b4-c1a45dba25cf", "25fa0e5c-0ca9-433b-9c62-73307a129946", "754d4905-e97d-4d97-909d-ed531f7b0f32"], "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/gpu-training-example.md", "file_name": "gpu-training-example.md", "text_hash": "6881a56b993ab4920b9f746e4c46bf3a"}}, "99ef5905657b4160f73fcb6785446d890372bf00": {"node_ids": ["477198a6-6281-404f-93d5-e34ffd8f67a0", "4deac6ab-2a2f-4869-80b5-1bee877f0bb8", "70782ece-9927-4f94-8b0d-ab19026587fe", "e94c4278-d68b-4377-91fc-feebe5aa1951", "477198a6-6281-404f-93d5-e34ffd8f67a0", "4deac6ab-2a2f-4869-80b5-1bee877f0bb8", "70782ece-9927-4f94-8b0d-ab19026587fe", "e94c4278-d68b-4377-91fc-feebe5aa1951"], "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "a32f6af9b9bd4643fb975f337996d763"}}, "51564e249ad3441ea771c142becb87d99c6862c8": {"node_ids": ["0d013edf-234c-43ec-ad73-186085f999c7", "0d013edf-234c-43ec-ad73-186085f999c7"], "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/mobilenet-rayservice.md", "file_name": "mobilenet-rayservice.md", "text_hash": "000839d36da9759a415420ee38135b0c"}}, "24cd0b5eca2e29f47acda73bddf678471ba71237": {"node_ids": ["724f461c-d500-4bfe-a873-98591710ab4f", "724f461c-d500-4bfe-a873-98591710ab4f"], "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/stable-diffusion-rayservice.md", "file_name": "stable-diffusion-rayservice.md", "text_hash": "d5531d93cd99b346bf02bce1bcf39b03"}}, "740309fdd83face28fb6334740e83223522f5b52": {"node_ids": ["cdbd593a-840e-444a-a656-9c09705d441f", "cdbd593a-840e-444a-a656-9c09705d441f"], "metadata": {"file_path": "doc/source/cluster/kubernetes/examples/text-summarizer-rayservice.md", "file_name": "text-summarizer-rayservice.md", "text_hash": "b1755dc872ac4f80045a8006dae21365"}}, "13a936d2637d0c51a1a223abbd5e988368442ae4": {"node_ids": ["6e5614c6-564e-4916-9e3b-b1a32d00fea0", "6e5614c6-564e-4916-9e3b-b1a32d00fea0"], "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started.md", "file_name": "getting-started.md", "text_hash": "133377325d7cd68ce638e977d569f5d5"}}, "d44a40b06a831809a1a01e668d3def8d2333e2a6": {"node_ids": ["f639521d-65a4-4474-b6f0-c4aff6d62a77", "774a6a5a-3743-4677-ba88-6937a3282653", "e0bbdec8-1489-482d-846f-65c98f3fa75a", "f639521d-65a4-4474-b6f0-c4aff6d62a77", "774a6a5a-3743-4677-ba88-6937a3282653", "e0bbdec8-1489-482d-846f-65c98f3fa75a"], "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/raycluster-quick-start.md", "file_name": "raycluster-quick-start.md", "text_hash": "134fdd75f8224140ba58099b04195ab4"}}, "86171dc5b178459bde22346e99afb9e484eb2b29": {"node_ids": ["438f481b-9c81-45c0-8772-f4779a35edb6", "68517bd2-bcee-4082-83be-4d0df05c58ff", "1c21c17d-8b75-467b-8ca2-52413d50981b", "438f481b-9c81-45c0-8772-f4779a35edb6", "68517bd2-bcee-4082-83be-4d0df05c58ff", "1c21c17d-8b75-467b-8ca2-52413d50981b"], "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayjob-quick-start.md", "file_name": "rayjob-quick-start.md", "text_hash": "01280a0e6637466fd3ed5ec13f54ea10"}}, "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc": {"node_ids": ["563f9c64-b857-46d4-be44-db12cdb17e38", "3ea219e7-30a4-48b8-afa1-3abd52244608", "d4abd0cd-f6dd-46d0-9662-63980fff1654", "7ec5a158-8319-4c7a-8a3a-96ebc3248604", "07012c0b-e785-446f-b163-fd1ad04b1f14", "1cac615e-3e13-4380-a358-d489653e42ae", "563f9c64-b857-46d4-be44-db12cdb17e38", "3ea219e7-30a4-48b8-afa1-3abd52244608", "d4abd0cd-f6dd-46d0-9662-63980fff1654", "7ec5a158-8319-4c7a-8a3a-96ebc3248604", "07012c0b-e785-446f-b163-fd1ad04b1f14", "1cac615e-3e13-4380-a358-d489653e42ae"], "metadata": {"file_path": "doc/source/cluster/kubernetes/getting-started/rayservice-quick-start.md", "file_name": "rayservice-quick-start.md", "text_hash": "99f3b0724bc0c1024e06a1447296baae"}}, "451575bfb68af963c2c68ec33a13fc699f1a5403": {"node_ids": ["480d30ed-654f-43e4-b9ef-f280bd1f24b3", "a1528e65-4c98-4402-a654-ca94bab81724", "480d30ed-654f-43e4-b9ef-f280bd1f24b3", "a1528e65-4c98-4402-a654-ca94bab81724"], "metadata": {"file_path": "doc/source/cluster/kubernetes/index.md", "file_name": "index.md", "text_hash": "cb95c1ad92e812dc492200dfd2fb46b0"}}, "e5eee13cd4ac2d3eb43d00fc342865b54e6e870b": {"node_ids": ["ccc2da2f-5f51-4f94-9bf9-9cb622fc8c2e", "ccc2da2f-5f51-4f94-9bf9-9cb622fc8c2e"], "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem.md", "file_name": "k8s-ecosystem.md", "text_hash": "84ed11f43398f275c450b217ef386adc"}}, "14280205ec65214ae56e2e49e685b225991a4c47": {"node_ids": ["afe46cdf-d27f-48f4-9f57-749cb98fdc7b", "f43b28fa-01ea-4f41-a64d-4331a2461f04", "319c8e5d-8e00-4ffb-baac-eb379d68ef32", "afe46cdf-d27f-48f4-9f57-749cb98fdc7b", "f43b28fa-01ea-4f41-a64d-4331a2461f04", "319c8e5d-8e00-4ffb-baac-eb379d68ef32"], "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/ingress.md", "file_name": "ingress.md", "text_hash": "c00f9f5f86e8353d1229299631608819"}}, "e94ae1e47b404478422d0c5de96ec8b49bc63aa8": {"node_ids": ["9e832df2-03ce-48de-b8e3-624545c04a4e", "d83a5888-ad90-4410-b1dd-4498a4603a7c", "9e832df2-03ce-48de-b8e3-624545c04a4e", "d83a5888-ad90-4410-b1dd-4498a4603a7c"], "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/kubeflow.md", "file_name": "kubeflow.md", "text_hash": "d0e194f1ab53bb066b619315af3bdd56"}}, "e97a523f02e286d2bdfd394c52fe9e03c55a8339": {"node_ids": ["5c7a8dda-c347-4886-8d95-932731e4b6fd", "7acb202b-f342-42a0-93bb-eaf2ba917e52", "d014be4c-9b01-4641-8c64-f95aedcd321b", "5830ebfa-4880-41c3-adc9-08303f4f371a", "0d088cc4-f973-4399-82ec-c747dc16ac21", "385ac200-4b03-41ae-81d5-acdb37ca0449", "30b5bb31-3345-49bd-b5a1-5ed0976f57b2", "5c7a8dda-c347-4886-8d95-932731e4b6fd", "7acb202b-f342-42a0-93bb-eaf2ba917e52", "d014be4c-9b01-4641-8c64-f95aedcd321b", "5830ebfa-4880-41c3-adc9-08303f4f371a", "0d088cc4-f973-4399-82ec-c747dc16ac21", "385ac200-4b03-41ae-81d5-acdb37ca0449", "30b5bb31-3345-49bd-b5a1-5ed0976f57b2"], "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/prometheus-grafana.md", "file_name": "prometheus-grafana.md", "text_hash": "3d10c34b561aeb86a85eacd9958ae32a"}}, "38a22c7537e5c6f4028cd701b950c8a6cb05cb61": {"node_ids": ["bd154635-df9d-4866-bd96-2ad3cd190dad", "bd154635-df9d-4866-bd96-2ad3cd190dad"], "metadata": {"file_path": "doc/source/cluster/kubernetes/k8s-ecosystem/pyspy.md", "file_name": "pyspy.md", "text_hash": "e64a17c789493f9199670c38f061678e"}}, "e43761527e1583a7e63955b3c7f439bf71d3a789": {"node_ids": ["b5beb349-8735-4ad9-bcac-9b8a73f7aba1", "b5beb349-8735-4ad9-bcac-9b8a73f7aba1"], "metadata": {"file_path": "doc/source/cluster/kubernetes/references.md", "file_name": "references.md", "text_hash": "d638cca45bcae35f294803ceb0d50943"}}, "5f0d3d82a4b9bd26d149b36619cce5c7a9b20add": {"node_ids": ["f64a2270-bf42-4a9f-8cb9-722e807d9cab", "f64a2270-bf42-4a9f-8cb9-722e807d9cab"], "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "facb095ea6e3066357cb9f02b125cece"}}, "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e": {"node_ids": ["86381715-01dd-48c2-8163-00481f593b46", "238ecbca-fa3c-4ee7-8b67-ee247e560cc6", "54605cca-3f23-4a66-bb5a-ddfc88fd84e9", "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c", "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a", "0f3235ff-6850-417c-8ff2-4f101ce4a94b", "e47b8ac9-0bff-4fff-842d-c78e9e60a3a1", "86381715-01dd-48c2-8163-00481f593b46", "238ecbca-fa3c-4ee7-8b67-ee247e560cc6", "54605cca-3f23-4a66-bb5a-ddfc88fd84e9", "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c", "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a", "0f3235ff-6850-417c-8ff2-4f101ce4a94b", "e47b8ac9-0bff-4fff-842d-c78e9e60a3a1"], "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/rayservice-troubleshooting.md", "file_name": "rayservice-troubleshooting.md", "text_hash": "945a2d29575ced57a2782b476338edd0"}}, "91fb2685a01f700f1014320fe55a2bec9f38453a": {"node_ids": ["c808a3ee-24c5-4f4d-836b-072d2d25b86b", "94dc960a-7081-48f4-8960-4dfa0b74d134", "c808a3ee-24c5-4f4d-836b-072d2d25b86b", "94dc960a-7081-48f4-8960-4dfa0b74d134"], "metadata": {"file_path": "doc/source/cluster/kubernetes/troubleshooting/troubleshooting.md", "file_name": "troubleshooting.md", "text_hash": "4e493a992e31411a8b58e9b811cf0a82"}}, "db32fd9ae4dbd777a9e04c19455dc94c2da0a76f": {"node_ids": ["acdb9e12-24f2-4e76-9b8b-4f17d57be912", "acdb9e12-24f2-4e76-9b8b-4f17d57be912"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides.md", "file_name": "user-guides.md", "text_hash": "27878cfd97b68513207dd16000610c2c"}}, "90359a47da74d7df69371b361d0d214655b93769": {"node_ids": ["4aa497ba-7b26-4649-9ba7-489e4b0fe4d1", "67da064b-5cd5-4f0a-809a-5569c18b5696", "4aa497ba-7b26-4649-9ba7-489e4b0fe4d1", "67da064b-5cd5-4f0a-809a-5569c18b5696"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/aws-eks-gpu-cluster.md", "file_name": "aws-eks-gpu-cluster.md", "text_hash": "a0c72012a478692955bf3996b391b77d"}}, "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3": {"node_ids": ["501a0fe0-a994-4240-9fc0-d0fb6f9d17b6", "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1", "3b9b406e-9259-4f36-823b-913504fc5c67", "617b32f1-cd1f-4683-a4b7-8d9abb607760", "692db461-266e-44e2-b76b-9c0b1559b214", "501a0fe0-a994-4240-9fc0-d0fb6f9d17b6", "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1", "3b9b406e-9259-4f36-823b-913504fc5c67", "617b32f1-cd1f-4683-a4b7-8d9abb607760", "692db461-266e-44e2-b76b-9c0b1559b214"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/config.md", "file_name": "config.md", "text_hash": "f6b2ac2406bc0d88d52b987c1bc38e16"}}, "a4267bc028bc3831c121c91893412728f50930e6": {"node_ids": ["64144582-8a17-423f-b2d9-aa6320573414", "c4bfc911-a085-4f49-b354-fdc0a7752175", "c29014f0-35e5-4fb8-911b-e29f1a8eac8b", "d6f02e6e-040d-463a-bbfb-dd4aed855935", "64144582-8a17-423f-b2d9-aa6320573414", "c4bfc911-a085-4f49-b354-fdc0a7752175", "c29014f0-35e5-4fb8-911b-e29f1a8eac8b", "d6f02e6e-040d-463a-bbfb-dd4aed855935"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/configuring-autoscaling.md", "file_name": "configuring-autoscaling.md", "text_hash": "09b68e65a812106f53b5b019b658516c"}}, "263ee108412167e79e111b51a63451c798fdb732": {"node_ids": ["591a2049-1865-43c7-bb12-ccb90d45c8c2", "591a2049-1865-43c7-bb12-ccb90d45c8c2"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/experimental.md", "file_name": "experimental.md", "text_hash": "5f595cf4cb6a5402ca34fb022169a6a9"}}, "2f9ad0b8129b0df68f9d674ea508af7f576f0d2c": {"node_ids": ["8058a0e1-2d1c-4567-9785-98bd1f1d1a19", "34ce08a4-fe86-4552-b947-923d00090a59", "8058a0e1-2d1c-4567-9785-98bd1f1d1a19", "34ce08a4-fe86-4552-b947-923d00090a59"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gcp-gke-gpu-cluster.md", "file_name": "gcp-gke-gpu-cluster.md", "text_hash": "a0928a34fc4afac9d52e9583ff329eeb"}}, "4db513a141cd34e9111906a454769b31275a7c1e": {"node_ids": ["ec291bef-99f3-4381-87da-40610cff7594", "cffab070-aa42-46ad-bc53-c2c5e9d6c38b", "4a2a1933-008d-40e1-a278-96001d5fb45d", "1db9ccfe-6d92-4837-9da6-bfbe84914a7a", "ec291bef-99f3-4381-87da-40610cff7594", "cffab070-aa42-46ad-bc53-c2c5e9d6c38b", "4a2a1933-008d-40e1-a278-96001d5fb45d", "1db9ccfe-6d92-4837-9da6-bfbe84914a7a"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/gpu.rst", "file_name": "gpu.rst", "text_hash": "b68cb5906ed9298f6486be94c4b43b79"}}, "caa42e633b4f103a254c3905e06d00c50dfbacc8": {"node_ids": ["07898156-be8e-40c3-8969-1382387f0ceb", "07898156-be8e-40c3-8969-1382387f0ceb"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/k8s-cluster-setup.md", "file_name": "k8s-cluster-setup.md", "text_hash": "694143df57f72cd60b1211cc1df35490"}}, "a3b95688928bf73014d93e625eeccb28f2317c17": {"node_ids": ["f50630dc-0b4d-4243-8093-e8683ada589f", "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e", "dc12baf2-8408-461a-84bd-50f978883e89", "f50630dc-0b4d-4243-8093-e8683ada589f", "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e", "dc12baf2-8408-461a-84bd-50f978883e89"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/logging.md", "file_name": "logging.md", "text_hash": "fec3cbc7e1ca800dccc7678ce9fe992b"}}, "7ee0de0c75182ef1a301db805eb4d3d281a174db": {"node_ids": ["59d0f25c-204d-45b8-b813-554978fd7092", "e5998a6f-2648-4186-bf8a-511347e50d51", "050bd1d8-66f8-4a05-8b48-0ff0e26b74b0", "59d0f25c-204d-45b8-b813-554978fd7092", "e5998a6f-2648-4186-bf8a-511347e50d51", "050bd1d8-66f8-4a05-8b48-0ff0e26b74b0"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-command.md", "file_name": "pod-command.md", "text_hash": "6627a011a260a3796f38f5ab0fcddcd4"}}, "795bde4c244d3a4acb0819ac10094d6da841753b": {"node_ids": ["09901628-a61d-4054-aaa7-040ec89f096d", "c6b92eec-8d55-4f48-9022-192ffa2ef6e9", "09901628-a61d-4054-aaa7-040ec89f096d", "c6b92eec-8d55-4f48-9022-192ffa2ef6e9"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/pod-security.md", "file_name": "pod-security.md", "text_hash": "7e6facc87956f8a3c7f73c88158de97e"}}, "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9": {"node_ids": ["0ce4a921-4009-457c-b8d3-d9e7edfcad99", "7008f0bf-8120-4b70-a6ee-a1d92bc941d0", "c39e4c52-7f1e-43be-9b06-dd89269367e4", "0ce4a921-4009-457c-b8d3-d9e7edfcad99", "7008f0bf-8120-4b70-a6ee-a1d92bc941d0", "c39e4c52-7f1e-43be-9b06-dd89269367e4"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/rayserve-dev-doc.md", "file_name": "rayserve-dev-doc.md", "text_hash": "df1f6557e5d959e1db9aee3b97462cdf"}}, "d5530ecb6a190835995e3492aeedf2ec1afa6fd4": {"node_ids": ["89e34c1b-3fe6-49c5-b2f4-3242c12a02cb", "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f", "db5e5308-fb39-4336-ae2e-0d5416dce7b2", "20deaa3c-55ad-4769-8840-1fb167e4d9ee", "89e34c1b-3fe6-49c5-b2f4-3242c12a02cb", "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f", "db5e5308-fb39-4336-ae2e-0d5416dce7b2", "20deaa3c-55ad-4769-8840-1fb167e4d9ee"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/static-ray-cluster-without-kuberay.md", "file_name": "static-ray-cluster-without-kuberay.md", "text_hash": "ffaf23a1efb2c689be942d7c9a8802eb"}}, "375bc84261ca3cac1e7707fce62d9e734af45c56": {"node_ids": ["48cc0889-d983-45e6-83aa-7fa9e56d497f", "630dd8e4-1238-4b2d-be4e-a7b5924608e4", "002471ba-af14-4042-afdc-d4e701d4fa58", "48cc0889-d983-45e6-83aa-7fa9e56d497f", "630dd8e4-1238-4b2d-be4e-a7b5924608e4", "002471ba-af14-4042-afdc-d4e701d4fa58"], "metadata": {"file_path": "doc/source/cluster/kubernetes/user-guides/tls.md", "file_name": "tls.md", "text_hash": "e9cf83d9e7ea1377f9ab6016848a33e3"}}, "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79": {"node_ids": ["4f1762bb-9149-4166-ae4e-7ba4073ba3a5", "528bcaac-3569-45ba-a00f-7444e32e335e", "483f3f3d-30ab-4acb-a327-1fc4d919fa0c", "db4fe360-861c-4758-bdbf-606a7bb18307", "73536aa5-a2db-4434-bf87-c393d1649dd5", "4f1762bb-9149-4166-ae4e-7ba4073ba3a5", "528bcaac-3569-45ba-a00f-7444e32e335e", "483f3f3d-30ab-4acb-a327-1fc4d919fa0c", "db4fe360-861c-4758-bdbf-606a7bb18307", "73536aa5-a2db-4434-bf87-c393d1649dd5"], "metadata": {"file_path": "doc/source/cluster/metrics.md", "file_name": "metrics.md", "text_hash": "2e5af2d63c01dc0a888b7af1610196e6"}}, "1aef16167d64461a82d43b35a79ed3cbd34015b1": {"node_ids": ["c98c13a9-c3e9-4f80-8f56-03e077d74e7a", "c98c13a9-c3e9-4f80-8f56-03e077d74e7a"], "metadata": {"file_path": "doc/source/cluster/package-overview.rst", "file_name": "package-overview.rst", "text_hash": "8f9ee70000f14e90057d04c52452e66f"}}, "c0f44c732a1139656d5f79045c246bffbdbd1b99": {"node_ids": ["c03654ba-8d53-42e2-b774-41685dc114bd", "c03654ba-8d53-42e2-b774-41685dc114bd"], "metadata": {"file_path": "doc/source/cluster/running-applications/autoscaling/reference.rst", "file_name": "reference.rst", "text_hash": "bc67ce5ff163b6d403b27e3fe1be755f"}}, "0c49800e6448d4a3988965eb78514279036abe09": {"node_ids": ["b94d5f22-bb97-4703-8f46-55eb7d7ae23c", "b94d5f22-bb97-4703-8f46-55eb7d7ae23c"], "metadata": {"file_path": "doc/source/cluster/running-applications/index.md", "file_name": "index.md", "text_hash": "1e0a17ebfac43e718c634e3358bb810b"}}, "c8b3cca8117b420d18b7e9f2151c77227cd7d0c0": {"node_ids": ["56743901-aaf1-48f2-963c-ae5be28418e3", "56743901-aaf1-48f2-963c-ae5be28418e3"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/cli.rst", "file_name": "cli.rst", "text_hash": "925aeb1fb40a73201435f779211df398"}}, "bf3902efa57370e1725f0eecf7e0d43daf52904a": {"node_ids": ["0d1fb481-dd12-40fe-840c-8639e9dd6e0b", "0d1fb481-dd12-40fe-840c-8639e9dd6e0b"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/index.md", "file_name": "index.md", "text_hash": "37be36869f4b158017f70c683fe368b2"}}, "ed2d9709ef8c706dbbc05c18a339eb5e4a10c5d8": {"node_ids": ["3d4a7350-3bc8-45bf-aa7c-d6823fa711cb", "3d4a7350-3bc8-45bf-aa7c-d6823fa711cb"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/jobs-package-ref.rst", "file_name": "jobs-package-ref.rst", "text_hash": "af7df164f14251fad2cc7d489562d3b5"}}, "67742f5dc8d71882663b79a9122a5afc603c4cd0": {"node_ids": ["1be93d3d-31ce-4142-9e97-9da986722059", "776a58af-6583-4fd7-b8ba-7473f2dcd914", "92f1bffb-3509-40b9-9700-36a9b5d55f0b", "96961802-1273-4170-8324-f2a2307c08b2", "f463cba3-8ec0-4629-917d-495d2f4192b7", "1be93d3d-31ce-4142-9e97-9da986722059", "776a58af-6583-4fd7-b8ba-7473f2dcd914", "92f1bffb-3509-40b9-9700-36a9b5d55f0b", "96961802-1273-4170-8324-f2a2307c08b2", "f463cba3-8ec0-4629-917d-495d2f4192b7"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/quickstart.rst", "file_name": "quickstart.rst", "text_hash": "ba5eedc99d6d48f026fdfcfe6b225e99"}}, "a153e682f4476fea19423ec384e7d661ed5e5d26": {"node_ids": ["ae1a0b32-23f8-40e4-a8a5-8126a3924cae", "30ee1aa7-9c84-485c-b94c-a79f2660d2ca", "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0", "4c475f09-a5fe-416f-8e3f-480d05890147", "ae1a0b32-23f8-40e4-a8a5-8126a3924cae", "30ee1aa7-9c84-485c-b94c-a79f2660d2ca", "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0", "4c475f09-a5fe-416f-8e3f-480d05890147"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/ray-client.rst", "file_name": "ray-client.rst", "text_hash": "e8a3776cfd48b9888da832dad839677b"}}, "abc8b5a830cd6e67653f12e8402722f1f74b1090": {"node_ids": ["daa65b7d-3353-47db-a084-680ed2e142e7", "daa65b7d-3353-47db-a084-680ed2e142e7"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/rest.rst", "file_name": "rest.rst", "text_hash": "25eda7ce32fb311aa83f235883cd741f"}}, "5e76f83e8d43d101d58ffbb85c275981c06a588f": {"node_ids": ["fafd028b-0881-4c69-8af9-4617374e8dc7", "b508c27b-7c3c-4e8a-99fe-1e73f2976d33", "a0636629-900d-45e2-89ba-4c24c16bbd06", "74d82ea6-3418-43c2-87cf-29e54a21f3bb", "fafd028b-0881-4c69-8af9-4617374e8dc7", "b508c27b-7c3c-4e8a-99fe-1e73f2976d33", "a0636629-900d-45e2-89ba-4c24c16bbd06", "74d82ea6-3418-43c2-87cf-29e54a21f3bb"], "metadata": {"file_path": "doc/source/cluster/running-applications/job-submission/sdk.rst", "file_name": "sdk.rst", "text_hash": "865eb28fbf5acb2ed02e8c07664e0b37"}}, "a191a1b577180b83bb6ec9a4792268fdcff13885": {"node_ids": ["394969df-f50a-49be-a41b-99e28a98665c", "3b6e4178-818d-4c23-abc7-ef68c066b191", "394969df-f50a-49be-a41b-99e28a98665c", "3b6e4178-818d-4c23-abc7-ef68c066b191"], "metadata": {"file_path": "doc/source/cluster/usage-stats.rst", "file_name": "usage-stats.rst", "text_hash": "ff123026a778b7a032d6b85256fa2ad1"}}, "059be000a548ba333d06422639928f37453b7acd": {"node_ids": ["9eb7f6a6-7852-4efb-8774-70ba5ae920b1", "9eb7f6a6-7852-4efb-8774-70ba5ae920b1"], "metadata": {"file_path": "doc/source/cluster/vms/examples/index.md", "file_name": "index.md", "text_hash": "a587cbaf369a564b483eecd0a79a91fa"}}, "3389ca272d7794cef1f1530d852ce7e99e0a8c9b": {"node_ids": ["6c161615-4bfe-4128-9b9e-ba0feb4d1c9a", "157cdab9-099b-407f-9519-196cd295fafd", "6c161615-4bfe-4128-9b9e-ba0feb4d1c9a", "157cdab9-099b-407f-9519-196cd295fafd"], "metadata": {"file_path": "doc/source/cluster/vms/examples/ml-example.md", "file_name": "ml-example.md", "text_hash": "b44bb5fd0af5a3cf2462fa7ae4ea1e19"}}, "7768f2f672765e5415913c4228b360d4cb4a6dc5": {"node_ids": ["c34c8f2b-79bc-4e78-9f99-432e074451b3", "82dcb8f1-44af-499b-80bb-6906495be97b", "384bbf17-abd1-4393-b76b-2d61100f54e1", "6083ac45-ab63-465d-9991-d2cc4c009f3a", "a4a6cdf4-8a73-4b62-a8c6-20b20506458f", "c34c8f2b-79bc-4e78-9f99-432e074451b3", "82dcb8f1-44af-499b-80bb-6906495be97b", "384bbf17-abd1-4393-b76b-2d61100f54e1", "6083ac45-ab63-465d-9991-d2cc4c009f3a", "a4a6cdf4-8a73-4b62-a8c6-20b20506458f"], "metadata": {"file_path": "doc/source/cluster/vms/getting-started.rst", "file_name": "getting-started.rst", "text_hash": "2b13f2af0921f0ee8ccbb6720ac5ed5a"}}, "79b3017bdc6c936583e006b5d4ebbc6c65c8adcb": {"node_ids": ["6392923d-dbf2-4a53-a55c-f4f14353a2ff", "6392923d-dbf2-4a53-a55c-f4f14353a2ff"], "metadata": {"file_path": "doc/source/cluster/vms/index.md", "file_name": "index.md", "text_hash": "1f362d5cac986f1b087e9fc353b22717"}}, "42f28b2ddc1835b66055b3fcf000b0f239bfa174": {"node_ids": ["8321234e-5b11-4ca6-b7cd-6351ccdedd3c", "8321234e-5b11-4ca6-b7cd-6351ccdedd3c"], "metadata": {"file_path": "doc/source/cluster/vms/references/index.md", "file_name": "index.md", "text_hash": "30e4027926ca7711ef4bbadaf7f06cd4"}}, "ecad81e26158d44b8bc8709e61dbff634c2a0ed3": {"node_ids": ["e4f9a169-2d53-4e4c-a08f-335d79196592", "e795e777-11ba-42a7-a975-9316b7469ddd", "683f5197-0867-4850-8f4e-a2c6456a3400", "897574e7-4c29-4fc1-a44d-1e539cd2bf80", "e4f9a169-2d53-4e4c-a08f-335d79196592", "e795e777-11ba-42a7-a975-9316b7469ddd", "683f5197-0867-4850-8f4e-a2c6456a3400", "897574e7-4c29-4fc1-a44d-1e539cd2bf80"], "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-cli.rst", "file_name": "ray-cluster-cli.rst", "text_hash": "699852c4e782c4911bcf3db826287f5e"}}, "dffe95234140e8a264434ea0db9f54c9a87c5c0f": {"node_ids": ["c4c0eb79-eb7a-4f25-a882-fd5052ab7ae0", "46991261-a958-4d42-826d-fe239c2486dc", "a71514b5-1b95-47f4-855e-beb1f35ee24a", "45310706-fff1-46cf-a38f-9489efa2d14f", "81e5446d-43ec-483e-afbb-98275bbda71e", "319705b4-0fe4-4341-aa1e-0976a4a3262b", "351ff147-587a-45b7-8fe8-355ec7db3253", "71701ef5-3759-4b70-91a3-fc309d1c1543", "57d9be46-033b-4cdf-95cb-b5d4e872d4ce", "3bed3ea9-56b4-4a13-ba76-48839b19cbcc", "10f315cd-09be-4e10-bba5-db96872ae438", "c4efdbc4-6845-4071-91dc-225abe067992", "bb30ffd7-a50d-4b0d-923b-055e224a50e0", "bc0eddd8-2564-4de1-b3b5-2e23e0212663", "d005637e-bd3c-46c8-a141-bddb7f3e0457", "c4c0eb79-eb7a-4f25-a882-fd5052ab7ae0", "46991261-a958-4d42-826d-fe239c2486dc", "a71514b5-1b95-47f4-855e-beb1f35ee24a", "45310706-fff1-46cf-a38f-9489efa2d14f", "81e5446d-43ec-483e-afbb-98275bbda71e", "319705b4-0fe4-4341-aa1e-0976a4a3262b", "351ff147-587a-45b7-8fe8-355ec7db3253", "71701ef5-3759-4b70-91a3-fc309d1c1543", "57d9be46-033b-4cdf-95cb-b5d4e872d4ce", "3bed3ea9-56b4-4a13-ba76-48839b19cbcc", "10f315cd-09be-4e10-bba5-db96872ae438", "c4efdbc4-6845-4071-91dc-225abe067992", "bb30ffd7-a50d-4b0d-923b-055e224a50e0", "bc0eddd8-2564-4de1-b3b5-2e23e0212663", "d005637e-bd3c-46c8-a141-bddb7f3e0457"], "metadata": {"file_path": "doc/source/cluster/vms/references/ray-cluster-configuration.rst", "file_name": "ray-cluster-configuration.rst", "text_hash": "3963e4e4c4c2e53cd687616868f26778"}}, "5f72eedd6db99177e353e656c114bdd778cb25b1": {"node_ids": ["0d89b6c6-276b-452d-94fe-e3f3d08369f6", "0d89b6c6-276b-452d-94fe-e3f3d08369f6"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/index.rst", "file_name": "index.rst", "text_hash": "dcb884fc3dfc85cf80bced085bb6533a"}}, "9d96b1ae7e208a0ba069d1d6f3e12027358f5ec2": {"node_ids": ["e5fdc0ec-92be-471a-97d0-e6b8b9397d8e", "e5fdc0ec-92be-471a-97d0-e6b8b9397d8e"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/lsf.rst", "file_name": "lsf.rst", "text_hash": "996544bf5ad9c4b4e79fe228402a5b89"}}, "13cf6ed3b08d89431287ced9df879d64790b6ece": {"node_ids": ["dd3f68ae-eaa5-4a34-a5b5-c02e495ccdd9", "dd3f68ae-eaa5-4a34-a5b5-c02e495ccdd9"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-basic.rst", "file_name": "slurm-basic.rst", "text_hash": "03db2f6b72b5adf83b0c2754069199e2"}}, "2b3d2f49ed4318d827b14b29988b549413ab253d": {"node_ids": ["243c751f-e27c-4caf-abff-f07b144ccb66", "243c751f-e27c-4caf-abff-f07b144ccb66"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-launch.rst", "file_name": "slurm-launch.rst", "text_hash": "2de1302653ddb6bc3336a904d1f5b032"}}, "d669e36152f5242b38b20986b896012d5056b2f6": {"node_ids": ["1754c6fe-7d8a-4ff4-9d7c-21b6afe03d87", "1754c6fe-7d8a-4ff4-9d7c-21b6afe03d87"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm-template.rst", "file_name": "slurm-template.rst", "text_hash": "587b8e7f57b3f2bb07f59fc8bdcea0e3"}}, "f0c1eb33e012e870eda5d847c0836c0ffec9cfce": {"node_ids": ["a761e05b-7ad9-4db3-924a-452fce5a0444", "70a61247-6af1-4f88-afa2-b7a5f97df653", "827773b2-ac63-4039-a3b1-dba6e5647db4", "149d57a8-2463-4686-8536-e6b7c927aa6b", "a761e05b-7ad9-4db3-924a-452fce5a0444", "70a61247-6af1-4f88-afa2-b7a5f97df653", "827773b2-ac63-4039-a3b1-dba6e5647db4", "149d57a8-2463-4686-8536-e6b7c927aa6b"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/slurm.rst", "file_name": "slurm.rst", "text_hash": "ef14700397f13a4948595066981c8152"}}, "06c765ed6430b814ee82b1268745d228e972b263": {"node_ids": ["63063991-ae68-481a-a5d3-875f036bed7a", "587ae477-7964-4fff-8767-a4d07796ebbc", "63063991-ae68-481a-a5d3-875f036bed7a", "587ae477-7964-4fff-8767-a4d07796ebbc"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/spark.rst", "file_name": "spark.rst", "text_hash": "ad075e99813d414b2050a3b45ebacb9e"}}, "462527dce44ab0f7c53c75be6c03af20b2586f7f": {"node_ids": ["f5acc69a-e6c7-4873-a78a-0402d8e658ac", "4b71400e-85bc-4a36-836f-419954390f96", "c51747c4-b3c4-484f-ad12-fd39932f45fc", "f5acc69a-e6c7-4873-a78a-0402d8e658ac", "4b71400e-85bc-4a36-836f-419954390f96", "c51747c4-b3c4-484f-ad12-fd39932f45fc"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/community/yarn.rst", "file_name": "yarn.rst", "text_hash": "8c0873a1797053c94955050448deb2dd"}}, "7560178692df45d7752a866d364dd9404a5d712c": {"node_ids": ["059ed3f6-68cb-4d27-8088-93f406198823", "059ed3f6-68cb-4d27-8088-93f406198823"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/configuring-autoscaling.rst", "file_name": "configuring-autoscaling.rst", "text_hash": "92b1fa7ad9c0ac32024b46ac641d8b60"}}, "01bbf04d082742ec92f875507d1b3b5da760013f": {"node_ids": ["377e787c-e08e-4b04-b823-7f34d1bef1bb", "377e787c-e08e-4b04-b823-7f34d1bef1bb"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/index.md", "file_name": "index.md", "text_hash": "d0f929741541e0b7f796dd115c3efd7c"}}, "278673bf77343f1d9a2c751b29b8254563c2b7fe": {"node_ids": ["a9c498dc-346f-437c-ab6c-263fca673dda", "1fbf3014-9503-4000-a502-6561115cbcd5", "a9c498dc-346f-437c-ab6c-263fca673dda", "1fbf3014-9503-4000-a502-6561115cbcd5"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/large-cluster-best-practices.rst", "file_name": "large-cluster-best-practices.rst", "text_hash": "8329e7710c54d566d97d4b8d9d4f7967"}}, "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8": {"node_ids": ["a8d475c5-a922-4cc5-8e60-79c8f5139427", "0a98d235-cbbf-4f4c-804e-70d935123949", "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d", "8a43bdf4-9ca0-473b-b450-523dd1f02cce", "bceca43a-0a80-4673-8e63-ab6e08fb66ea", "1b9061fe-12be-4fc3-96cf-f5ffb4da053f", "b7031aee-71f2-463d-9b3c-6abc04f76c70", "a8d475c5-a922-4cc5-8e60-79c8f5139427", "0a98d235-cbbf-4f4c-804e-70d935123949", "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d", "8a43bdf4-9ca0-473b-b450-523dd1f02cce", "bceca43a-0a80-4673-8e63-ab6e08fb66ea", "1b9061fe-12be-4fc3-96cf-f5ffb4da053f", "b7031aee-71f2-463d-9b3c-6abc04f76c70"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/aws.md", "file_name": "aws.md", "text_hash": "1c857d98d433399629ec66e50712acb3"}}, "398092b12d52dde908ff05170dff8012d97713c4": {"node_ids": ["d4dea8b6-dc1b-44eb-af42-b042825d58a2", "8a045afd-bbe8-487b-afc8-7f139db9e45d", "d4dea8b6-dc1b-44eb-af42-b042825d58a2", "8a045afd-bbe8-487b-afc8-7f139db9e45d"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/azure.md", "file_name": "azure.md", "text_hash": "5ce9af30e9db1e36dffa13440eea43e7"}}, "37cf48193f890dda1585315591c9d9582be0ba09": {"node_ids": ["1bc2777c-607f-44aa-8e49-893520d55e91", "1bc2777c-607f-44aa-8e49-893520d55e91"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/gcp.md", "file_name": "gcp.md", "text_hash": "2e9046b5a9c3a64a89f154150f93b27c"}}, "d84782edd1e2dd22c61e4a2d1edcf723465db8b0": {"node_ids": ["855344ff-0f0b-4db6-b9f2-fa79bd7366c0", "855344ff-0f0b-4db6-b9f2-fa79bd7366c0"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/index.rst", "file_name": "index.rst", "text_hash": "b0be857625b68ec1af527533159a43c0"}}, "2fe7fb7d482e1dbd4c39940bda29c88aafcf3af1": {"node_ids": ["e1c43659-e2c1-4058-94b2-31494c457f21", "2081eda7-15ff-4992-9202-8f0c8c7da30f", "e1c43659-e2c1-4058-94b2-31494c457f21", "2081eda7-15ff-4992-9202-8f0c8c7da30f"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/launching-clusters/on-premises.md", "file_name": "on-premises.md", "text_hash": "657d2d98eb1e78ecc46befd864fc9b40"}}, "712f452dc469ddb8d1d08de74cbc2c7fff25f739": {"node_ids": ["d02eaca7-5279-4f09-9530-450dd3c5d25d", "d02eaca7-5279-4f09-9530-450dd3c5d25d"], "metadata": {"file_path": "doc/source/cluster/vms/user-guides/logging.md", "file_name": "logging.md", "text_hash": "622ba4cb5280564613a4776d87d07732"}}}, "docstore/metadata": {"cf7447bd-c777-490d-a105-15d791d5cb49": {"doc_hash": "18c69eb8585841a03320748281e22e39e89deb803d6455a5713d6bde17643203", "ref_doc_id": "619f0028e26d2ca0229c368ae863b591d1ae0f1e"}, "ba92b10a-578b-445b-bb26-b66260508667": {"doc_hash": "4c5fea293a438a2d99f8231dd117e1945ac8106d1ae3904da89534bd627f52e2", "ref_doc_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0"}, "3953411a-b66d-4891-a4e2-f3ff12be661e": {"doc_hash": "2b2049e9e18d261863df04f5b83df3e4d988d8f1fbb3671eb35e0969d63cafbe", "ref_doc_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0"}, "cb38dc60-e82e-46b4-b4c9-6c0b95a5b984": {"doc_hash": "3323e5baa911286f2b03d7c49f6a1376e173aedd1008a8a8554c7f2470731a78", "ref_doc_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0"}, "a2d5ebeb-0b87-4056-861f-eadb30cac546": {"doc_hash": "f605728e3645caefca616b86ce9dd0d8f4a763d402038975b32be7d6253e0ba8", "ref_doc_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0"}, "a8d93d42-4550-4559-a0c9-e2c5a1bbd9ef": {"doc_hash": "b20ceb61f17ee4327b0d761108a561ad9900886376dd984f3365093e97069938", "ref_doc_id": "92f6124876c2ae06d5c5e6f9e5f473e1033485d0"}, "e688aa7d-7a01-4d26-a0f8-53ab714d6ee8": {"doc_hash": "c1daf4f228cb15c8e2493b8dc0fded098a1dc3ebc573fa2e868cb73e9aab9f6a", "ref_doc_id": "a44cc38177f171d99f64433945351c51348a6412"}, "863aac4d-a07a-451b-8fdc-d12ad2d43134": {"doc_hash": "c9f10dba061096ef2f6ba0ac4a75efda5e1a61de2dfcdd9f38f3ab3a580bfb29", "ref_doc_id": "a44cc38177f171d99f64433945351c51348a6412"}, "ae00c381-47aa-408e-a304-9bc55be4ce1c": {"doc_hash": "3706e9260949cc82436e23f9f34c564f92088061a6876ae172ae09ec96b8a653", "ref_doc_id": "d86c419db93279c5f8e539528563b29a3ef9cad5"}, "eb7fded7-c10b-47bb-b1db-518895993fee": {"doc_hash": "217c51bd6e9d89bc613e581c3cc70ccad4a567332426e2ff6f3ae793a16f4207", "ref_doc_id": "d86c419db93279c5f8e539528563b29a3ef9cad5"}, "ca3ee395-69e9-4f14-b779-437199ce23d1": {"doc_hash": "5ef32bb3be31fe0695940c20ffd64e6205d1aeaa474acdbe0242af623a09d89b", "ref_doc_id": "09b54b0a2a9f4cf455148263384a536fd1c639e0"}, "1fe0ce81-f9ec-47b0-93fb-c3780961a18e": {"doc_hash": "d0b4203bddb2478e39f085fbaf574d7a32b14caa626c699b93bc48f1dd1033f8", "ref_doc_id": "0bf5221971f6b8b5df8ae3196a2172ed0d59bcef"}, "254f4b24-2fae-416c-8335-602e73835fce": {"doc_hash": "500718868f473af384e2f7aca857863920b43c2772b6b4319e7dcf5fece187ce", "ref_doc_id": "d8a469417f56973e8e700003f5a35e61cf96beab"}, "672ee971-7834-49fb-a0a0-128ec84ed8d0": {"doc_hash": "61c7f6964e94dff9711d60b0f3b5b9b4d128ad60ba5858f8fb6b863328cfe986", "ref_doc_id": "d8a469417f56973e8e700003f5a35e61cf96beab"}, "a1f4bb2b-9424-4dbc-83e6-d52f25d2c885": {"doc_hash": "81fbe54f1a39336c9a7ec6206c7d4ee6a74e8ec03673a6d2495b6defeb68da3c", "ref_doc_id": "b6158a2e1f824622aadd10423a4b92218966f453"}, "d1292c35-6ed4-4667-84b4-c1a45dba25cf": {"doc_hash": "a6c198631d35737724d061f89adbd00e8a86c3ec66a08ad1e1447d0befcebce6", "ref_doc_id": "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379"}, "25fa0e5c-0ca9-433b-9c62-73307a129946": {"doc_hash": "7e45e460387559851108440b982056d6d61727400a6f700daa113ee7429e1223", "ref_doc_id": "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379"}, "754d4905-e97d-4d97-909d-ed531f7b0f32": {"doc_hash": "aadb9e42e91400b3298e4c810f91b806a6f2becf3b2666334f7d423d8ce3a93d", "ref_doc_id": "47f1ccdc81f7cc4d7707a33a1452e2783cf9f379"}, "477198a6-6281-404f-93d5-e34ffd8f67a0": {"doc_hash": "64bafc7cfae69b41bfa987e9b9414992bf504673c382c59b4376c93910d9a88c", "ref_doc_id": "99ef5905657b4160f73fcb6785446d890372bf00"}, "4deac6ab-2a2f-4869-80b5-1bee877f0bb8": {"doc_hash": "94bd9f77a5cd09c3abe1955a6b0eb69ded3157d0553328e41fb8972de174bc85", "ref_doc_id": "99ef5905657b4160f73fcb6785446d890372bf00"}, "70782ece-9927-4f94-8b0d-ab19026587fe": {"doc_hash": "d6c01d387af497deadf5271b4fe291dc512145f3d7f5990ed3e7ad260fefd41c", "ref_doc_id": "99ef5905657b4160f73fcb6785446d890372bf00"}, "e94c4278-d68b-4377-91fc-feebe5aa1951": {"doc_hash": "e87fd1751153778c85f018579e880726882840c8d43aa78432e9733b98f2abc8", "ref_doc_id": "99ef5905657b4160f73fcb6785446d890372bf00"}, "0d013edf-234c-43ec-ad73-186085f999c7": {"doc_hash": "3a819f815c0ece0644b4857e7b46e3a00da6bd6859a2bc1e33e54d319767230e", "ref_doc_id": "51564e249ad3441ea771c142becb87d99c6862c8"}, "724f461c-d500-4bfe-a873-98591710ab4f": {"doc_hash": "1357b6c351a5627471a8fe856130b86daab1c51c69961f0440f8f13dd4161020", "ref_doc_id": "24cd0b5eca2e29f47acda73bddf678471ba71237"}, "cdbd593a-840e-444a-a656-9c09705d441f": {"doc_hash": "c835e2a3c3a603cc6a5e50eb6f06da3b80da9cd63dadd2a3103e62f54b0d7dc0", "ref_doc_id": "740309fdd83face28fb6334740e83223522f5b52"}, "6e5614c6-564e-4916-9e3b-b1a32d00fea0": {"doc_hash": "c1e1b68bdd108119eac7d2ca560a34a41cb625f7ff15565385444cb8866544ee", "ref_doc_id": "13a936d2637d0c51a1a223abbd5e988368442ae4"}, "f639521d-65a4-4474-b6f0-c4aff6d62a77": {"doc_hash": "8f092c2b231a15c045e0f9451dbede87a0456cd86dc4e4c19d98be8222900861", "ref_doc_id": "d44a40b06a831809a1a01e668d3def8d2333e2a6"}, "774a6a5a-3743-4677-ba88-6937a3282653": {"doc_hash": "76b98eb63489dfad4894b19bf245deabe3019621639f32bbade02d25f6603cef", "ref_doc_id": "d44a40b06a831809a1a01e668d3def8d2333e2a6"}, "e0bbdec8-1489-482d-846f-65c98f3fa75a": {"doc_hash": "2dc364870a3dd60c09095431a17266fe410a980753ec99bac9db8e5efa397051", "ref_doc_id": "d44a40b06a831809a1a01e668d3def8d2333e2a6"}, "438f481b-9c81-45c0-8772-f4779a35edb6": {"doc_hash": "5d2d1d78ab59ea67a2a0062b5ad06a6ae2598d8511a38f251bec67f0eb058c6c", "ref_doc_id": "86171dc5b178459bde22346e99afb9e484eb2b29"}, "68517bd2-bcee-4082-83be-4d0df05c58ff": {"doc_hash": "88aca3c6dc8aa8874de4aec8bf85b635efc268254a9f45b5e1233163bca5c031", "ref_doc_id": "86171dc5b178459bde22346e99afb9e484eb2b29"}, "1c21c17d-8b75-467b-8ca2-52413d50981b": {"doc_hash": "7fbffe288045905f7707a1e38c469c55df10c8fa2961df603e1cd25fe24fd017", "ref_doc_id": "86171dc5b178459bde22346e99afb9e484eb2b29"}, "563f9c64-b857-46d4-be44-db12cdb17e38": {"doc_hash": "67367b947c0c1efd882610d1087c9b5fb7ae489fa7bf5d386bc5d9713d2731e4", "ref_doc_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc"}, "3ea219e7-30a4-48b8-afa1-3abd52244608": {"doc_hash": "ca136f7c9398583583a36c94e810f79b4a23793210050b9a2fd36865f9074ff7", "ref_doc_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc"}, "d4abd0cd-f6dd-46d0-9662-63980fff1654": {"doc_hash": "d02aaa7fbe879305fea966d241e353c860595347e00f67b4cc7d806d0e07482e", "ref_doc_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc"}, "7ec5a158-8319-4c7a-8a3a-96ebc3248604": {"doc_hash": "e662857cbe3813ec4cfd30bc911cea3734e08a669cd15d6de66ddc14a92c6be6", "ref_doc_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc"}, "07012c0b-e785-446f-b163-fd1ad04b1f14": {"doc_hash": "a090e0ded09e11ed434dcaec441c909cfaf54cf180b1cd61e315f63ff81b3feb", "ref_doc_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc"}, "1cac615e-3e13-4380-a358-d489653e42ae": {"doc_hash": "e3cf1b0a7de0241b0aee9b9098d3ebf8c71d0baf4b6103b3fea421cbef331a46", "ref_doc_id": "8bc1cbbfb70edcf53eb3900315eaafe3be1081bc"}, "480d30ed-654f-43e4-b9ef-f280bd1f24b3": {"doc_hash": "4c05edc1bfa6e10160c7112747871b9c37077ee4fa09d133f82259259d758d66", "ref_doc_id": "451575bfb68af963c2c68ec33a13fc699f1a5403"}, "a1528e65-4c98-4402-a654-ca94bab81724": {"doc_hash": "d0e9bbe519a7fa6d808bba348fd0434ba52d53ec52b0e93bbebfdf38294684ea", "ref_doc_id": "451575bfb68af963c2c68ec33a13fc699f1a5403"}, "ccc2da2f-5f51-4f94-9bf9-9cb622fc8c2e": {"doc_hash": "a00d4be1061e6ce84a4a4c748b77a9b293dffee104d486ee9311f3d02d32d4eb", "ref_doc_id": "e5eee13cd4ac2d3eb43d00fc342865b54e6e870b"}, "afe46cdf-d27f-48f4-9f57-749cb98fdc7b": {"doc_hash": "3c6c5a2f4a5b6cb51b7639bcab51f59d9bcebd5f42e83009409ca8b10c6e029f", "ref_doc_id": "14280205ec65214ae56e2e49e685b225991a4c47"}, "f43b28fa-01ea-4f41-a64d-4331a2461f04": {"doc_hash": "a912921d5f0d56d3e51c8edd30b38a59447655e4cca4a64f70938dabb4f36d30", "ref_doc_id": "14280205ec65214ae56e2e49e685b225991a4c47"}, "319c8e5d-8e00-4ffb-baac-eb379d68ef32": {"doc_hash": "c6ed89e8e26300e1e3bcaf6bca3d664354f908a52f26f0b0137f8f96069ddc49", "ref_doc_id": "14280205ec65214ae56e2e49e685b225991a4c47"}, "9e832df2-03ce-48de-b8e3-624545c04a4e": {"doc_hash": "e3f4f506355e5f75162664f14d73bb2030271565ee68cc0fa12966b421640ae6", "ref_doc_id": "e94ae1e47b404478422d0c5de96ec8b49bc63aa8"}, "d83a5888-ad90-4410-b1dd-4498a4603a7c": {"doc_hash": "e8b04d80a690b74a1b75f80ecc71662c5a1d160afb225b8f378cd3938c247111", "ref_doc_id": "e94ae1e47b404478422d0c5de96ec8b49bc63aa8"}, "5c7a8dda-c347-4886-8d95-932731e4b6fd": {"doc_hash": "70f2cf378a9501faca70ec2bf699c2e53858795a5f71c816fbfa4e998e83a3db", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "7acb202b-f342-42a0-93bb-eaf2ba917e52": {"doc_hash": "e6e211db5cfb81ad542e8447d65161d5eba9299648216bd256fbf93bc215724f", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "d014be4c-9b01-4641-8c64-f95aedcd321b": {"doc_hash": "d003bfeb01d6112839a54d7967e8855e8c95fecb2250c36b1e4b3ea7cb3cb502", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "5830ebfa-4880-41c3-adc9-08303f4f371a": {"doc_hash": "43113a09e8a3b91592b5094c244aa734ed2774d990b470f9935332daa82cb33f", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "0d088cc4-f973-4399-82ec-c747dc16ac21": {"doc_hash": "cdb32aaf4ebc078b1bb8f38a7a4b0c7d4873dff91e547f0d8440312678d071c7", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "385ac200-4b03-41ae-81d5-acdb37ca0449": {"doc_hash": "ef3acb7e9a8bc997a7cc3088af250782019e9ec6e2cc0331cb21ac10fcf52f73", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "30b5bb31-3345-49bd-b5a1-5ed0976f57b2": {"doc_hash": "88c637aefddaf3d93aa4607d9d16b5d86030c77b0f307a98f2f7b3ab472cc347", "ref_doc_id": "e97a523f02e286d2bdfd394c52fe9e03c55a8339"}, "bd154635-df9d-4866-bd96-2ad3cd190dad": {"doc_hash": "24ef4a79094dda56aa84fec226cde522c2aeaf773c7525dc37700555c197a01d", "ref_doc_id": "38a22c7537e5c6f4028cd701b950c8a6cb05cb61"}, "b5beb349-8735-4ad9-bcac-9b8a73f7aba1": {"doc_hash": "d7ff9c28b5926fffc5759262cbfd17585e2dfe983760cd8e2dab4a7b810ae256", "ref_doc_id": "e43761527e1583a7e63955b3c7f439bf71d3a789"}, "f64a2270-bf42-4a9f-8cb9-722e807d9cab": {"doc_hash": "e5d7448d4916d390d551b1e6dce8a2704323ac12154affe7cfbb897c18e77c0b", "ref_doc_id": "5f0d3d82a4b9bd26d149b36619cce5c7a9b20add"}, "86381715-01dd-48c2-8163-00481f593b46": {"doc_hash": "35e055ff41ad8526d25af1e4e7a65dcaab2185434c552edb5615555c2ff4176e", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "238ecbca-fa3c-4ee7-8b67-ee247e560cc6": {"doc_hash": "c940f5ad2426ddd34da5e79981a2af36bff0141cb28d42827dad569c7c5dd941", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "54605cca-3f23-4a66-bb5a-ddfc88fd84e9": {"doc_hash": "b3562f88155271c0a368afd0451d4ad4261e7803318dd40ed3e70d061876ca1e", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "c2c27e98-edc6-49a9-ab66-0eb6578f9e4c": {"doc_hash": "01f1ab52bc48636f01c57901bd722349a51d6cfb148e751ba552d06a61e83065", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "e68bde0c-0c7f-421c-bbf4-17fd5f927a8a": {"doc_hash": "efb4d3bca72d3c0cd8a5bf02b1fec954b5e0be02871144d8d4370f0c6c7624e5", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "0f3235ff-6850-417c-8ff2-4f101ce4a94b": {"doc_hash": "c566cf2e1c27bbc7d723576b05bd846ea0111963f57573b7921fff95358495c4", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "e47b8ac9-0bff-4fff-842d-c78e9e60a3a1": {"doc_hash": "deea502e9e0eda9552f9348caa2c94fb0fd14e81d52ca2cf6074c65c0e5d0346", "ref_doc_id": "5cd0c0b415f953f754f9a4c2461b5a120ae91a7e"}, "c808a3ee-24c5-4f4d-836b-072d2d25b86b": {"doc_hash": "abdb3a1406b20d91183b49232ca440bb6a2a9dd37f34dda6d0882cfa2a534131", "ref_doc_id": "91fb2685a01f700f1014320fe55a2bec9f38453a"}, "94dc960a-7081-48f4-8960-4dfa0b74d134": {"doc_hash": "6a6fc9fc30ae81d52dc62f40da29fa752c98ffd196fb859dc1aa7d762bbb3614", "ref_doc_id": "91fb2685a01f700f1014320fe55a2bec9f38453a"}, "acdb9e12-24f2-4e76-9b8b-4f17d57be912": {"doc_hash": "2e9e2a6478815d5ef3bda3960276c072784b72c36ce61f34530775ae2a97875e", "ref_doc_id": "db32fd9ae4dbd777a9e04c19455dc94c2da0a76f"}, "4aa497ba-7b26-4649-9ba7-489e4b0fe4d1": {"doc_hash": "e5a7a3a43b8c537aa220f28adfac4cb4b76a22bcf439f84b119d0ace312524cf", "ref_doc_id": "90359a47da74d7df69371b361d0d214655b93769"}, "67da064b-5cd5-4f0a-809a-5569c18b5696": {"doc_hash": "03b910dc4b0183ec88fd8be893e06865b33ef75aab2be8950a51bb9e3d63293c", "ref_doc_id": "90359a47da74d7df69371b361d0d214655b93769"}, "501a0fe0-a994-4240-9fc0-d0fb6f9d17b6": {"doc_hash": "5f8c92f0121952f79b36dfd015aef2718a38e0b57b0a08adccca552ccb515f8b", "ref_doc_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3"}, "e2fdd98f-3961-458b-b6cc-2f8cbb3759b1": {"doc_hash": "b8e685da4862a05b545916cd59781787506783f1b29980c5de66344a690b4431", "ref_doc_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3"}, "3b9b406e-9259-4f36-823b-913504fc5c67": {"doc_hash": "fb368580f0bee3238dcc75f71d5bfba871813a183143520e6f7a2e97b0a0dd54", "ref_doc_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3"}, "617b32f1-cd1f-4683-a4b7-8d9abb607760": {"doc_hash": "6bb8a9e5490668be2ce91224294160cb1193ba5c62e443cc96393768e4805ffa", "ref_doc_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3"}, "692db461-266e-44e2-b76b-9c0b1559b214": {"doc_hash": "ad3cac5c0a6c25def3884c043b392f0c0629fbb7d19867c80deaeb22f630439e", "ref_doc_id": "8445c7e07d2c6cf0957b3c2f9fdaff60061415d3"}, "64144582-8a17-423f-b2d9-aa6320573414": {"doc_hash": "0817f9b8e87be4a1bbf7ca3b3e11e1c02fbab62379e7f1f88d2b4ccab216a872", "ref_doc_id": "a4267bc028bc3831c121c91893412728f50930e6"}, "c4bfc911-a085-4f49-b354-fdc0a7752175": {"doc_hash": "8ef266aa6667fad83d2f0bfbf50e57f310f6f91809a3c033804ec79a3ec8130b", "ref_doc_id": "a4267bc028bc3831c121c91893412728f50930e6"}, "c29014f0-35e5-4fb8-911b-e29f1a8eac8b": {"doc_hash": "d9f2e3c2c0dd19633e31636a4e6473d9f79f64469b809c2e1f9a8341b011b436", "ref_doc_id": "a4267bc028bc3831c121c91893412728f50930e6"}, "d6f02e6e-040d-463a-bbfb-dd4aed855935": {"doc_hash": "f369055b10c9b0ed616de46827dcfea1d286cb3570b997d1a5b3945969da17f7", "ref_doc_id": "a4267bc028bc3831c121c91893412728f50930e6"}, "591a2049-1865-43c7-bb12-ccb90d45c8c2": {"doc_hash": "997b8dd0dd7b4f4fbd7953fdeeb897814adaefc90126990fe68db2c571f931f2", "ref_doc_id": "263ee108412167e79e111b51a63451c798fdb732"}, "8058a0e1-2d1c-4567-9785-98bd1f1d1a19": {"doc_hash": "5b3441189a852a95d95820ddeb86493426285122778c2358e5e20f25fd9f424a", "ref_doc_id": "2f9ad0b8129b0df68f9d674ea508af7f576f0d2c"}, "34ce08a4-fe86-4552-b947-923d00090a59": {"doc_hash": "03cc36360206da8da1ac5eaf253cebdb3565214ca98f04f2259f25d9509e9a81", "ref_doc_id": "2f9ad0b8129b0df68f9d674ea508af7f576f0d2c"}, "ec291bef-99f3-4381-87da-40610cff7594": {"doc_hash": "43426a882e5da14d90b783350c6f15e5ae469cf3c6921867f8eda7dfe39d4925", "ref_doc_id": "4db513a141cd34e9111906a454769b31275a7c1e"}, "cffab070-aa42-46ad-bc53-c2c5e9d6c38b": {"doc_hash": "b5576ded469cc49cf8454f9acb43a5f0728da07b7e6cbec6a6cadd638dab4f9c", "ref_doc_id": "4db513a141cd34e9111906a454769b31275a7c1e"}, "4a2a1933-008d-40e1-a278-96001d5fb45d": {"doc_hash": "46a6bdabc6d7e03f3740a93419fb95aef1bb0c9400adaa77c47b8e62012fbacd", "ref_doc_id": "4db513a141cd34e9111906a454769b31275a7c1e"}, "1db9ccfe-6d92-4837-9da6-bfbe84914a7a": {"doc_hash": "52f338db19b29c8ca73610cd76420f1884cd48bdffe61b275e59ce37fa2cc69f", "ref_doc_id": "4db513a141cd34e9111906a454769b31275a7c1e"}, "07898156-be8e-40c3-8969-1382387f0ceb": {"doc_hash": "2f2666204860f4516d71f67365512c400e1793651bb62c3408d01c07ab47aa42", "ref_doc_id": "caa42e633b4f103a254c3905e06d00c50dfbacc8"}, "f50630dc-0b4d-4243-8093-e8683ada589f": {"doc_hash": "760debccfd5eac2f42b886655f80130d69244c4ea1ae721eb41ad8d8b46f4eb2", "ref_doc_id": "a3b95688928bf73014d93e625eeccb28f2317c17"}, "ab6ca1bf-4c96-4dc7-91db-55a2bb547f9e": {"doc_hash": "5c3c549588cf71cf04721bef02d5b8167c492513d54408c9f53569f4ff7f628d", "ref_doc_id": "a3b95688928bf73014d93e625eeccb28f2317c17"}, "dc12baf2-8408-461a-84bd-50f978883e89": {"doc_hash": "dd6850145c0d1644795071565e5a33df582789e1af59d4794ade4da564c0e336", "ref_doc_id": "a3b95688928bf73014d93e625eeccb28f2317c17"}, "59d0f25c-204d-45b8-b813-554978fd7092": {"doc_hash": "fc7638f19d1bb18b1171fb461a41fd89f2208770f7bd6986b7aa9245427f43b3", "ref_doc_id": "7ee0de0c75182ef1a301db805eb4d3d281a174db"}, "e5998a6f-2648-4186-bf8a-511347e50d51": {"doc_hash": "bd2280eb815e5025610f5b999fa52fd2097a7a9b1283cf52bffe5fffae429c60", "ref_doc_id": "7ee0de0c75182ef1a301db805eb4d3d281a174db"}, "050bd1d8-66f8-4a05-8b48-0ff0e26b74b0": {"doc_hash": "1e369cf4fba0387facd07471984835b226eb7c4acaba77009afa699babfbfd58", "ref_doc_id": "7ee0de0c75182ef1a301db805eb4d3d281a174db"}, "09901628-a61d-4054-aaa7-040ec89f096d": {"doc_hash": "b03d725b92c795488b68c68f72b5c842974f0623382c95332140218626a26cbd", "ref_doc_id": "795bde4c244d3a4acb0819ac10094d6da841753b"}, "c6b92eec-8d55-4f48-9022-192ffa2ef6e9": {"doc_hash": "ab3b8ece2532d4060b1c86cede1d025168962f743f49c271a43296dede0d1578", "ref_doc_id": "795bde4c244d3a4acb0819ac10094d6da841753b"}, "0ce4a921-4009-457c-b8d3-d9e7edfcad99": {"doc_hash": "34490a414761f4f1b9934667d2ed9552a94a2a367f254c3614ce21678155008a", "ref_doc_id": "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9"}, "7008f0bf-8120-4b70-a6ee-a1d92bc941d0": {"doc_hash": "99aa779468ba9c7a43258af898b294492120dd21c7d162fde1f8b6d163cbc008", "ref_doc_id": "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9"}, "c39e4c52-7f1e-43be-9b06-dd89269367e4": {"doc_hash": "04d3c190fc2a2dc61234d21751df612cf9daee97bc874e3ae796359caf555d1f", "ref_doc_id": "be5c26ff3b41adfd966bfd8bd99e39ca940f28a9"}, "89e34c1b-3fe6-49c5-b2f4-3242c12a02cb": {"doc_hash": "c38027b4c2e23a6e914d05908dd2f7846edb391bd7035f7e4e8f5237753d4fda", "ref_doc_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4"}, "9d83e2c9-e559-4d30-9cf4-dbe4807cf37f": {"doc_hash": "5d29e018188cc15aff65b368ac828fd66c3b7d6117bf41a641dc148d736a9d4a", "ref_doc_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4"}, "db5e5308-fb39-4336-ae2e-0d5416dce7b2": {"doc_hash": "6c4c39163670dbf9c5720bf850523edd45984ff24c1b11175c07d72766b846e2", "ref_doc_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4"}, "20deaa3c-55ad-4769-8840-1fb167e4d9ee": {"doc_hash": "ecc5b6fab63cdf9acd0c712c144d9f6d9a2709ba11dffcc1a14a4bbcfd3ab35b", "ref_doc_id": "d5530ecb6a190835995e3492aeedf2ec1afa6fd4"}, "48cc0889-d983-45e6-83aa-7fa9e56d497f": {"doc_hash": "228f694f04294aaacfb6d6ff9bc1f58594b483fbb242ff1afef6c96e846cd8be", "ref_doc_id": "375bc84261ca3cac1e7707fce62d9e734af45c56"}, "630dd8e4-1238-4b2d-be4e-a7b5924608e4": {"doc_hash": "7b1a75e63b563a7ca12d0bd28a80514dca7e46186a5bab502aec48415f4c3a3a", "ref_doc_id": "375bc84261ca3cac1e7707fce62d9e734af45c56"}, "002471ba-af14-4042-afdc-d4e701d4fa58": {"doc_hash": "0336a6cb1cda85ce3cfea8e9aa8a72a007c3fa9601e59a1b0116e63fe5b67713", "ref_doc_id": "375bc84261ca3cac1e7707fce62d9e734af45c56"}, "4f1762bb-9149-4166-ae4e-7ba4073ba3a5": {"doc_hash": "374f78da863be8e907cd085b5a0a4f37aa0de4ee6537118247902bd7f11187ef", "ref_doc_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79"}, "528bcaac-3569-45ba-a00f-7444e32e335e": {"doc_hash": "14fa95832a03b0839ce7f42ca3ead543e17c4188357105cca262127ca55d6509", "ref_doc_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79"}, "483f3f3d-30ab-4acb-a327-1fc4d919fa0c": {"doc_hash": "8d3a21bbf44c9125f51dfb1b543742163e05f6b23ce7ee961cc8f6d87474b04c", "ref_doc_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79"}, "db4fe360-861c-4758-bdbf-606a7bb18307": {"doc_hash": "0cb7af9127fe8817725712f8ae34a1fd3a31ef83511a1037447d518750106b2d", "ref_doc_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79"}, "73536aa5-a2db-4434-bf87-c393d1649dd5": {"doc_hash": "d937ad4c6b0d28d56a44a96606f2a95b7a097df3ea8224d03ddc3442e495a005", "ref_doc_id": "7e8a4a0aaf6a90cddb1df9445e2e97dc4df76f79"}, "c98c13a9-c3e9-4f80-8f56-03e077d74e7a": {"doc_hash": "d3459fba057b43daf26b6dcf8b0ae26b130b724f64386f76132973480eed5add", "ref_doc_id": "1aef16167d64461a82d43b35a79ed3cbd34015b1"}, "c03654ba-8d53-42e2-b774-41685dc114bd": {"doc_hash": "07225fb58a320b1717dd0221de63d8fe0e1857f3f7541f7dc522c701c1f27881", "ref_doc_id": "c0f44c732a1139656d5f79045c246bffbdbd1b99"}, "b94d5f22-bb97-4703-8f46-55eb7d7ae23c": {"doc_hash": "d524b30b8ef4bd81d906f5b66862d154e1b4d7b5de8c46c0a69737fbfb9c883f", "ref_doc_id": "0c49800e6448d4a3988965eb78514279036abe09"}, "56743901-aaf1-48f2-963c-ae5be28418e3": {"doc_hash": "029395a45fd156cf36c96328ed2b875384338fe7a1eba8bcac17e7041084db19", "ref_doc_id": "c8b3cca8117b420d18b7e9f2151c77227cd7d0c0"}, "0d1fb481-dd12-40fe-840c-8639e9dd6e0b": {"doc_hash": "b04a53f2379564f83074f586aeab0b90c29b2c904f0234d420eca605c1a5d361", "ref_doc_id": "bf3902efa57370e1725f0eecf7e0d43daf52904a"}, "3d4a7350-3bc8-45bf-aa7c-d6823fa711cb": {"doc_hash": "29cd6567a02d745168c453f772defda052d436a9ea6556c38d2963717f1c9c24", "ref_doc_id": "ed2d9709ef8c706dbbc05c18a339eb5e4a10c5d8"}, "1be93d3d-31ce-4142-9e97-9da986722059": {"doc_hash": "466262dfe595d9fc9db6ef43b138799d97f09f46b18470136d191b950bbcf139", "ref_doc_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0"}, "776a58af-6583-4fd7-b8ba-7473f2dcd914": {"doc_hash": "32e538ddcdc74364a5dd89af1582ee3ade2d199a17e6e8e57eb490fa7cf957ff", "ref_doc_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0"}, "92f1bffb-3509-40b9-9700-36a9b5d55f0b": {"doc_hash": "5863901e5e97d965e7dc727dbb977882afeddc68327f67c5ebcd4a6c9cac280a", "ref_doc_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0"}, "96961802-1273-4170-8324-f2a2307c08b2": {"doc_hash": "e9ccec6bddb3116da3677bb17bb2679668c0f9e6ad27c29cabe455a6db9337d5", "ref_doc_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0"}, "f463cba3-8ec0-4629-917d-495d2f4192b7": {"doc_hash": "b9ba660fa1f7ca3cfe23758dabcaae2d6e87b1ab84ac7411a3d7f0bd6c158338", "ref_doc_id": "67742f5dc8d71882663b79a9122a5afc603c4cd0"}, "ae1a0b32-23f8-40e4-a8a5-8126a3924cae": {"doc_hash": "ec81bd2a90a909030596f0275d03792d83b498e8450aba7c042a066008ad8832", "ref_doc_id": "a153e682f4476fea19423ec384e7d661ed5e5d26"}, "30ee1aa7-9c84-485c-b94c-a79f2660d2ca": {"doc_hash": "173e59a62f0ecfba1e629a53ec5327e7dc4d71e243f90ea3750db12e951b4758", "ref_doc_id": "a153e682f4476fea19423ec384e7d661ed5e5d26"}, "8d3180d8-c34c-4a6a-8ead-9fff7f333cd0": {"doc_hash": "31116d896ced8871fe3c874e3cf6a08d8ec62eac1580c7bb4ea483fb572246fc", "ref_doc_id": "a153e682f4476fea19423ec384e7d661ed5e5d26"}, "4c475f09-a5fe-416f-8e3f-480d05890147": {"doc_hash": "3233e256ed6a3913fd31ee1d8087f9ed188a50c0218fe4301d231bd369e7c770", "ref_doc_id": "a153e682f4476fea19423ec384e7d661ed5e5d26"}, "daa65b7d-3353-47db-a084-680ed2e142e7": {"doc_hash": "468bd65934600236a5c8fe5b936124af7ba0da3c4e239bdf054bcb5bc55cb248", "ref_doc_id": "abc8b5a830cd6e67653f12e8402722f1f74b1090"}, "fafd028b-0881-4c69-8af9-4617374e8dc7": {"doc_hash": "ff3caf2693b5c1d35fca555c796e540a0d081cfdb4b4f5a5a869711a1adc526d", "ref_doc_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f"}, "b508c27b-7c3c-4e8a-99fe-1e73f2976d33": {"doc_hash": "b6c15177fc64d62f6df5e3b41cdef000e0dbc632dd59db1108e85d80a48498c4", "ref_doc_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f"}, "a0636629-900d-45e2-89ba-4c24c16bbd06": {"doc_hash": "77dea3380f7397c615d9708cb4e45635d4a27ba8d7a31a5512e81b2787deb429", "ref_doc_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f"}, "74d82ea6-3418-43c2-87cf-29e54a21f3bb": {"doc_hash": "f4be7c3e7ee55a2fbcb8f22fbc581416fa1a90e4154fd9a40923d6f4231c2dca", "ref_doc_id": "5e76f83e8d43d101d58ffbb85c275981c06a588f"}, "394969df-f50a-49be-a41b-99e28a98665c": {"doc_hash": "c7cc6a28ea16b9835e5adb5a0f66646a382f98a3d57507007d995a6ac2a66786", "ref_doc_id": "a191a1b577180b83bb6ec9a4792268fdcff13885"}, "3b6e4178-818d-4c23-abc7-ef68c066b191": {"doc_hash": "4fe72503f63fbf2dbc7b9297e40ad3586560b16b4ab1f686705f455f188bd024", "ref_doc_id": "a191a1b577180b83bb6ec9a4792268fdcff13885"}, "9eb7f6a6-7852-4efb-8774-70ba5ae920b1": {"doc_hash": "79b5ead9684b6ea026d0775f1b0d6faac3cf6a09bbb5d079718bfda585698080", "ref_doc_id": "059be000a548ba333d06422639928f37453b7acd"}, "6c161615-4bfe-4128-9b9e-ba0feb4d1c9a": {"doc_hash": "284ce6a894bc0b9599153fe93eadf344068448cd002536b5f986eed55edb282b", "ref_doc_id": "3389ca272d7794cef1f1530d852ce7e99e0a8c9b"}, "157cdab9-099b-407f-9519-196cd295fafd": {"doc_hash": "5c01b610b85e9d8bd106a650043b4fdd9834c32002a45615654219b173661ac1", "ref_doc_id": "3389ca272d7794cef1f1530d852ce7e99e0a8c9b"}, "c34c8f2b-79bc-4e78-9f99-432e074451b3": {"doc_hash": "c5fe0ec146c2e6bae0632c381f932a6f5dd978525608c04f2994d76033aab88b", "ref_doc_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5"}, "82dcb8f1-44af-499b-80bb-6906495be97b": {"doc_hash": "18eb04eb1cbf91b62f06e4dccea1cb5676af07954713fccb9cc92f89e09daa43", "ref_doc_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5"}, "384bbf17-abd1-4393-b76b-2d61100f54e1": {"doc_hash": "15d8adaf2a1e56d45de1a1fdb567659313c2f9e788ece5e58979efb22b43de23", "ref_doc_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5"}, "6083ac45-ab63-465d-9991-d2cc4c009f3a": {"doc_hash": "6db6724d2771f5cbf05510ced06b0109f64e392e2a4b4c116d0db0c6d1554186", "ref_doc_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5"}, "a4a6cdf4-8a73-4b62-a8c6-20b20506458f": {"doc_hash": "381ae6444e90490cea02fc8daf6f88e6c77cceaa057e50aff1fb05320129fc20", "ref_doc_id": "7768f2f672765e5415913c4228b360d4cb4a6dc5"}, "6392923d-dbf2-4a53-a55c-f4f14353a2ff": {"doc_hash": "c7670b4ee63e05079085696ca0243d0c66e2a20ac4c5c95f40c54e099463fd33", "ref_doc_id": "79b3017bdc6c936583e006b5d4ebbc6c65c8adcb"}, "8321234e-5b11-4ca6-b7cd-6351ccdedd3c": {"doc_hash": "3dd1c4b2a2fadb08047f9ca3d13e44712947ac8f400383c14dc1ed21133be57a", "ref_doc_id": "42f28b2ddc1835b66055b3fcf000b0f239bfa174"}, "e4f9a169-2d53-4e4c-a08f-335d79196592": {"doc_hash": "533ae5ccf4c7f8630b0161c78f2a9d3d8808fdbd49bc39b3952f2444b37db104", "ref_doc_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3"}, "e795e777-11ba-42a7-a975-9316b7469ddd": {"doc_hash": "266578864b00b19dd2f151718ba20acd80a43443e4602577ec1208c3a2919434", "ref_doc_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3"}, "683f5197-0867-4850-8f4e-a2c6456a3400": {"doc_hash": "c4efc2aa05ecba45585f49091b1bad5e62a8d122ad3eaa33c878a2fa91893d48", "ref_doc_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3"}, "897574e7-4c29-4fc1-a44d-1e539cd2bf80": {"doc_hash": "c3e583acc0ae2970c573480f2fbbe97cdfbfd2ae1cb15e4c6fe54b2651f1826c", "ref_doc_id": "ecad81e26158d44b8bc8709e61dbff634c2a0ed3"}, "c4c0eb79-eb7a-4f25-a882-fd5052ab7ae0": {"doc_hash": "c03a17f959f2910c39b9dbbe737db7fa58e1f65b3306e38c74811358323ab9f3", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "46991261-a958-4d42-826d-fe239c2486dc": {"doc_hash": "194f6e641287f6efe0af11f26a893073b10854441e67e1e2af7d601f2c21152a", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "a71514b5-1b95-47f4-855e-beb1f35ee24a": {"doc_hash": "882388a8fd22bff2a17f7329a2352de75fbee5cd16034ee8a2f05d5dbfac8ef7", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "45310706-fff1-46cf-a38f-9489efa2d14f": {"doc_hash": "9fce3c0431f29fcc3cea780317bfe370a10febf456f52f4737dc9d31c3225c33", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "81e5446d-43ec-483e-afbb-98275bbda71e": {"doc_hash": "1bc40e37691303f5ba4538b7bfc528831a7a3e6cf83bee00d0af834b9b2d224b", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "319705b4-0fe4-4341-aa1e-0976a4a3262b": {"doc_hash": "85df16b59d6e52d8561501a9e0f5ff96339577cd5ca97a922daaa780aa393074", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "351ff147-587a-45b7-8fe8-355ec7db3253": {"doc_hash": "7dfebbe0a4e13dc56b79bc72f260477455a428b57ecae7de51549903f1ca3924", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "71701ef5-3759-4b70-91a3-fc309d1c1543": {"doc_hash": "e95f6d2e2031ebfb7b050fbe26555a1b0400258f1dc3e24bb78cd0068e297d74", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "57d9be46-033b-4cdf-95cb-b5d4e872d4ce": {"doc_hash": "0b17b515ab930312fe2da46ace73f59d405dff5b33efbcba46c9e6492d58c0db", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "3bed3ea9-56b4-4a13-ba76-48839b19cbcc": {"doc_hash": "6ddbd643058b8fbffb952102b156e83252b4dc6099f44cc4a4382862d99bd0f5", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "10f315cd-09be-4e10-bba5-db96872ae438": {"doc_hash": "0dd66b55f03857df740ba412de4e4c1d69eba4ec67066f405ac4a8c9925db370", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "c4efdbc4-6845-4071-91dc-225abe067992": {"doc_hash": "795ca270c9e455f6f990ee3455d3bdd8377c89ecb274260473fbec3d65b366a7", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "bb30ffd7-a50d-4b0d-923b-055e224a50e0": {"doc_hash": "539e0a00a5afe983a28586340df4da2890d096de626761fd808cbac93d765f11", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "bc0eddd8-2564-4de1-b3b5-2e23e0212663": {"doc_hash": "2747614fa63b7e833e6d14ad9ee6615d0cbf150c5757e68ee8f25e95e205f69a", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "d005637e-bd3c-46c8-a141-bddb7f3e0457": {"doc_hash": "4d9051c5f853f2c80c70e13ecff5240795b096ef8616feff61c3387502c99051", "ref_doc_id": "dffe95234140e8a264434ea0db9f54c9a87c5c0f"}, "0d89b6c6-276b-452d-94fe-e3f3d08369f6": {"doc_hash": "781e54f756d5785c50c9af8abd3434df0646630386902fa7ec27e3b981919387", "ref_doc_id": "5f72eedd6db99177e353e656c114bdd778cb25b1"}, "e5fdc0ec-92be-471a-97d0-e6b8b9397d8e": {"doc_hash": "f7e5a4728e02e3b2263b9d66748d70612d2e3bed84d4d5aaef72a7f8812066f1", "ref_doc_id": "9d96b1ae7e208a0ba069d1d6f3e12027358f5ec2"}, "dd3f68ae-eaa5-4a34-a5b5-c02e495ccdd9": {"doc_hash": "ea62a6b22c4558b9676cbc887787b5403868d93a237f3894e5168f607f2e766e", "ref_doc_id": "13cf6ed3b08d89431287ced9df879d64790b6ece"}, "243c751f-e27c-4caf-abff-f07b144ccb66": {"doc_hash": "4eb86f38e62d93c46f2d0352a153868a2638214a72b6155c672c57c984d9f814", "ref_doc_id": "2b3d2f49ed4318d827b14b29988b549413ab253d"}, "1754c6fe-7d8a-4ff4-9d7c-21b6afe03d87": {"doc_hash": "ec39522d5311a21cdd52fe449068cb2d4007225d93f55684ad8b41a25a00ea8f", "ref_doc_id": "d669e36152f5242b38b20986b896012d5056b2f6"}, "a761e05b-7ad9-4db3-924a-452fce5a0444": {"doc_hash": "99c8e45b35e138fdbcc1f8b7a7708d1f597e96bc0186fab9d3c90a91798bfb47", "ref_doc_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce"}, "70a61247-6af1-4f88-afa2-b7a5f97df653": {"doc_hash": "99e9111810379f68d59679d7cdd3ba92ba769581acbccf627a35a1a17e6c4bd3", "ref_doc_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce"}, "827773b2-ac63-4039-a3b1-dba6e5647db4": {"doc_hash": "a43fed72ab3322c878827f9cb081c7c09fa9f0c18eb7e7a371f37da6e2afc265", "ref_doc_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce"}, "149d57a8-2463-4686-8536-e6b7c927aa6b": {"doc_hash": "05255bca38cca9fccfe9ade1205696d6deeef5457931d0e589786acba4a3ad87", "ref_doc_id": "f0c1eb33e012e870eda5d847c0836c0ffec9cfce"}, "63063991-ae68-481a-a5d3-875f036bed7a": {"doc_hash": "309fc591e6a6a30bda8a926b4e1b05e20d53f9229b741cd9869f2122d37bc1d2", "ref_doc_id": "06c765ed6430b814ee82b1268745d228e972b263"}, "587ae477-7964-4fff-8767-a4d07796ebbc": {"doc_hash": "134a47ea48029f5d5027fa4cc233c1086f0febddfd1ce1fc9c77cf33af94d3a3", "ref_doc_id": "06c765ed6430b814ee82b1268745d228e972b263"}, "f5acc69a-e6c7-4873-a78a-0402d8e658ac": {"doc_hash": "a3b996c14dc4c919231cdac12f5b59d3c8d0b87dccb36ce91606ff305441e040", "ref_doc_id": "462527dce44ab0f7c53c75be6c03af20b2586f7f"}, "4b71400e-85bc-4a36-836f-419954390f96": {"doc_hash": "daef6a6c7256201142220643c46889344a5d8ef6cc797ca2281bd8096c1b1ff4", "ref_doc_id": "462527dce44ab0f7c53c75be6c03af20b2586f7f"}, "c51747c4-b3c4-484f-ad12-fd39932f45fc": {"doc_hash": "0826468a411a23e7680b5a5815da27e6786df137d130f4a3320ca1a8cb26e40c", "ref_doc_id": "462527dce44ab0f7c53c75be6c03af20b2586f7f"}, "059ed3f6-68cb-4d27-8088-93f406198823": {"doc_hash": "3f2933239e7ad539baa5c86d3384d8493410e2f9cfb1018de817ede98c73602d", "ref_doc_id": "7560178692df45d7752a866d364dd9404a5d712c"}, "377e787c-e08e-4b04-b823-7f34d1bef1bb": {"doc_hash": "c69f837d4d3a3dc6ceff59f9b03623459198727b849e37da7327c02ed0d94bfb", "ref_doc_id": "01bbf04d082742ec92f875507d1b3b5da760013f"}, "a9c498dc-346f-437c-ab6c-263fca673dda": {"doc_hash": "9e873ea3065f0b4c71957171726aa78c7465b2d414058c55affc1227ff539b5a", "ref_doc_id": "278673bf77343f1d9a2c751b29b8254563c2b7fe"}, "1fbf3014-9503-4000-a502-6561115cbcd5": {"doc_hash": "da112eba68369bd83674c3b6e4da9110656ea937b9b7762a1c9b4b23e9bb6f78", "ref_doc_id": "278673bf77343f1d9a2c751b29b8254563c2b7fe"}, "a8d475c5-a922-4cc5-8e60-79c8f5139427": {"doc_hash": "7ee210580617207a88ca271a9bc96445a31e7f78be099a4077b755df4109b509", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "0a98d235-cbbf-4f4c-804e-70d935123949": {"doc_hash": "6a2e281d456700531261b7bf001cd49069af13437fb39d20fc7ddd0a769a0f98", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "7a3a6621-3db8-4f42-b5e0-40a4d1115a1d": {"doc_hash": "6a28e3d688e0877c6653e01120114a925f68540cee9841089564fb66dacdcfc8", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "8a43bdf4-9ca0-473b-b450-523dd1f02cce": {"doc_hash": "05ff6965bc58aaeb3bf7221ba4181aa51240e731a84394d4cf75ddba61b7055f", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "bceca43a-0a80-4673-8e63-ab6e08fb66ea": {"doc_hash": "3c80faca73e557a3e24eee5094300a0c1a192b4e9d7064d5e23f74ae977521f2", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "1b9061fe-12be-4fc3-96cf-f5ffb4da053f": {"doc_hash": "db379bbd85e6e6cd25a79f719798aac3c215d778aaf7f6725a1e6163a7d8460d", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "b7031aee-71f2-463d-9b3c-6abc04f76c70": {"doc_hash": "71b96fca1c6bb6d5261349cbc9a9cda293c7b85040b5243376d6bf0df0623bc8", "ref_doc_id": "3f7a3f7aba081d80a00dae40396759ff9cfaa3b8"}, "d4dea8b6-dc1b-44eb-af42-b042825d58a2": {"doc_hash": "64cfe517385fb2467db62d2a501069feb81794228d6332134d8c15eab7defc61", "ref_doc_id": "398092b12d52dde908ff05170dff8012d97713c4"}, "8a045afd-bbe8-487b-afc8-7f139db9e45d": {"doc_hash": "d40a522a99d8b3c74fbeca2d3d048eb3e16be611571c44e7396466fbd96fc1ae", "ref_doc_id": "398092b12d52dde908ff05170dff8012d97713c4"}, "1bc2777c-607f-44aa-8e49-893520d55e91": {"doc_hash": "eabb7c59b6f2ca869e7bc763027331ec8c071a9d1dcf9130ded39d1192fea57f", "ref_doc_id": "37cf48193f890dda1585315591c9d9582be0ba09"}, "855344ff-0f0b-4db6-b9f2-fa79bd7366c0": {"doc_hash": "5ab1b0950a6d62be111c009f22755b7c3f57974516a41c7380be34bfe8df0c8d", "ref_doc_id": "d84782edd1e2dd22c61e4a2d1edcf723465db8b0"}, "e1c43659-e2c1-4058-94b2-31494c457f21": {"doc_hash": "96769d8866409455aebb69f92e4b15c2d871690e67024ca54485965ac9221466", "ref_doc_id": "2fe7fb7d482e1dbd4c39940bda29c88aafcf3af1"}, "2081eda7-15ff-4992-9202-8f0c8c7da30f": {"doc_hash": "702554892dfa014b032964aabc9daad8210fb0acb7078213c20ef5f43a4193ea", "ref_doc_id": "2fe7fb7d482e1dbd4c39940bda29c88aafcf3af1"}, "d02eaca7-5279-4f09-9530-450dd3c5d25d": {"doc_hash": "8d60407a47697e47aa65005356a948ee96d604703ebc60002375c362aed1da03", "ref_doc_id": "712f452dc469ddb8d1d08de74cbc2c7fff25f739"}}}